{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff8dd956",
   "metadata": {},
   "source": [
    "Notebook to investigate raw FinBERT predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import itertools\n",
    "import datetime as dt\n",
    "import sqlite3\n",
    "\n",
    "# connect to folder for custom functions\n",
    "root = Path.cwd().parent\n",
    "vis_path = root / \"src\" / \"visualizations\" \n",
    "mod_path = root / \"src\" / \"modelling\" \n",
    "sys.path.append(str(vis_path))\n",
    "sys.path.append(str(mod_path))\n",
    "\n",
    "from read_articles import read\n",
    "#from plot_functions import plot_aini_series_subplots, plot_aini_hist_grid_by_years\n",
    "from compute_extrema import compute_aini_extrema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a91e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare paths\n",
    "var_path = root / \"data\" / \"processed\" / \"variables\"\n",
    "art_path = root / \"data\" / \"processed\" / \"articles\"\n",
    "table_path = root / \"reports\" / \"tables\"\n",
    "fig_path = root / \"reports\" / \"figures\"\n",
    "\n",
    "# load aini data\n",
    "aini_custom = pd.read_csv(var_path / \"binary_AINI_variables.csv\")\n",
    "aini_w0 = pd.read_csv(var_path / \"w0_AINI_variables.csv\")\n",
    "aini_w1 = pd.read_csv(var_path / \"w1_AINI_variables.csv\")\n",
    "aini_w2 =  pd.read_csv(var_path / \"w2_AINI_variables.csv\")\n",
    "\n",
    "\n",
    "# load financial data\n",
    "fin = pd.read_csv(root / \"data\" / \"raw\" / \"financial\" / \"full_daily_2023_2025.csv\")\n",
    "fin[\"Date\"] = pd.to_datetime(fin[\"Date\"])\n",
    "fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes for visualizaions\n",
    "normalized_AINI = pd.DataFrame()\n",
    "\n",
    "# ensure sorting\n",
    "aini_w0 = aini_w0.sort_values(\"date\")\n",
    "aini_w1 = aini_w1.sort_values(\"date\")\n",
    "aini_w2 = aini_w2.sort_values(\"date\")\n",
    "aini_custom = aini_custom.sort_values(\"date\")\n",
    "\n",
    "# compute extrema\n",
    "merged, tidy, pivot, extrema = compute_aini_extrema(aini_w0,aini_w1,aini_w2,aini_custom)\n",
    "aini_w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060fb20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer total obs per period\n",
    "\n",
    "# ensure datetime\n",
    "fin[\"date\"] = pd.to_datetime(fin[\"Date\"])\n",
    "\n",
    "# reduce to unique trading dates\n",
    "dates = fin[\"date\"].drop_duplicates()\n",
    "\n",
    "# min-max bounds\n",
    "start_all = pd.Timestamp(\"2023-04-01\")\n",
    "end_all   = pd.Timestamp(\"2025-06-16\")\n",
    "\n",
    "# masks for periods\n",
    "mask_all        = (dates >= start_all) & (dates <= end_all)\n",
    "mask_2023       = (dates >= \"2023-04-01\") & (dates <= \"2023-12-31\")\n",
    "mask_2024       = (dates.dt.year == 2024)\n",
    "mask_2025       = (dates >= \"2025-01-01\") & (dates <= \"2025-06-16\")\n",
    "mask_2023_2024  = (dates >= \"2023-04-01\") & (dates <= \"2024-12-31\")\n",
    "mask_2024_2025  = (dates >= \"2024-01-01\") & (dates <= \"2025-06-16\")\n",
    "\n",
    "# counts of unique dates\n",
    "counts = {\n",
    "    \"All_2023-04-01_to_2025-06-16\": dates.loc[mask_all].nunique(),\n",
    "    \"2023_after_03-31\":             dates.loc[mask_2023].nunique(),\n",
    "    \"2024_full\":                    dates.loc[mask_2024].nunique(),\n",
    "    \"2025_to_06-16\":                dates.loc[mask_2025].nunique(),\n",
    "    \"Span_2023-2024\":               dates.loc[mask_2023_2024].nunique(),\n",
    "    \"Span_2024-2025\":               dates.loc[mask_2024_2025].nunique(),\n",
    "}\n",
    "\n",
    "print(pd.Series(counts, name=\"n_unique_dates\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbb5c70",
   "metadata": {},
   "source": [
    "Explore differences in min, max, mean and std. of AINI variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a9384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_columns(df, exclude=[\"date\"]):\n",
    "    \"\"\"Return mean, std, min, max for each numeric column in df (except exclude).\"\"\"\n",
    "    results = []\n",
    "    for col in df.columns:\n",
    "        if col not in exclude:\n",
    "            series = df[col]\n",
    "            results.append({\n",
    "                \"variable\": col,\n",
    "                \"mean\": series.mean(),\n",
    "                \"std\": series.std(),\n",
    "                \"min\": series.min(),\n",
    "                \"max\": series.max()\n",
    "            })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# usage\n",
    "stats_individual = summarize_columns(merged)\n",
    "print(stats_individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution, ignoring raw counts due to unequal count of articles / day# \n",
    "outpath_hist = fig_path / \"aini_hist_year_panels.png\"\n",
    "\n",
    "plot_aini_hist_grid_by_years(\n",
    "    df = merged,\n",
    "    outpath= outpath_hist\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb23354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "extrema.to_csv(table_path / \"aini_extrema.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddf50dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to LaTeX with booktabs, tabular (single-page), wrapped in adjustbox\n",
    "latex_table = extrema.to_latex(\n",
    "    index=False,\n",
    "    escape=True,\n",
    "    column_format=\"l\" + \"c\" * (len(extrema.columns) - 1),\n",
    "    bold_rows=False\n",
    ")\n",
    "\n",
    "# Add booktabs spacing\n",
    "latex_table = latex_table.replace(\"\\\\toprule\", \"\\\\toprule\\n\\\\addlinespace\")\n",
    "latex_table = latex_table.replace(\"\\\\midrule\", \"\\\\midrule\\n\\\\addlinespace\")\n",
    "latex_table = latex_table.replace(\"\\\\bottomrule\", \"\\\\addlinespace\\n\\\\bottomrule\")\n",
    "\n",
    "# Wrap in table + adjustbox\n",
    "latex_wrapped = (\n",
    "    \"\\\\begin{table}[!htbp]\\n\"\n",
    "    \"\\\\centering\\n\"\n",
    "    \"\\\\begin{adjustbox}{width=\\\\textwidth}\\n\"\n",
    "    + latex_table +\n",
    "    \"\\\\end{adjustbox}\\n\"\n",
    "    \"\\\\caption{AINI extrema}\\n\"\n",
    "    \"\\\\label{tab:aini_extrema}\\n\"\n",
    "    \"\\\\end{table}\\n\"\n",
    ")\n",
    "\n",
    "# Save to file\n",
    "output_path = table_path / \"aini_extrema.tex\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(latex_wrapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06324e2",
   "metadata": {},
   "source": [
    "Calculate weekly extrema (by calendar week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7ce0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive calendar week (ISO year + week number)\n",
    "tidy[\"week\"] = tidy[\"date\"].dt.to_period(\"W\").apply(lambda r: r.start_time)\n",
    "\n",
    "# Count how often each min/max week occurs\n",
    "counts_by_week = (\n",
    "    tidy.groupby([\"type\", \"week\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "        .sort_values([\"type\", \"count\"], ascending=[True, False])\n",
    ")\n",
    "\n",
    "# subset n > 0\n",
    "extrema_weekly = counts_by_week[counts_by_week[\"count\"] > 0]\n",
    "\n",
    "# collect variables for each week\n",
    "week_dict = (\n",
    "    tidy.groupby([\"week\"])[\"variable\"]\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# attach variables to each week\n",
    "extrema_weekly[\"measure\"] = extrema_weekly[\"week\"].map(week_dict)\n",
    "\n",
    "# bring into convenient format\n",
    "extrema_weekly_clean = extrema_weekly.copy()\n",
    "\n",
    "# week start (Monday)\n",
    "week_start = extrema_weekly_clean[\"week\"]\n",
    "# week end (Sunday) = start + 6 days\n",
    "week_end = week_start + pd.Timedelta(days=6)\n",
    "\n",
    "# format as \"dd.mm.yyyy - dd.mm.yyyy\"\n",
    "extrema_weekly_clean[\"week\"] = (\n",
    "    week_start.dt.strftime(\"%d.%m.%Y\") + \" - \" + week_end.dt.strftime(\"%d.%m.%Y\")\n",
    ")\n",
    "\n",
    "extrema_weekly_clean[\"type\"] = extrema_weekly_clean[\"type\"].replace({\"min\": \"minimum\", \"max\": \"maximum\"})\n",
    "extrema_weekly_clean.rename(columns={\"count\": \"n measures\"}, inplace=True)\n",
    "\n",
    "# save\n",
    "extrema_weekly_clean.to_csv(table_path / \"aini_weekly_extrema.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to LaTeX with booktabs, tabular (single-page), wrapped in adjustbox\n",
    "latex_table = extrema_weekly_clean.to_latex(\n",
    "    index=False,\n",
    "    escape=True,\n",
    "    column_format=\"l\" + \"c\" * (len(extrema_weekly_clean.columns) - 1),\n",
    "    bold_rows=False\n",
    ")\n",
    "\n",
    "# Add booktabs spacing\n",
    "latex_table = latex_table.replace(\"\\\\toprule\", \"\\\\toprule\\n\\\\addlinespace\")\n",
    "latex_table = latex_table.replace(\"\\\\midrule\", \"\\\\midrule\\n\\\\addlinespace\")\n",
    "latex_table = latex_table.replace(\"\\\\bottomrule\", \"\\\\addlinespace\\n\\\\bottomrule\")\n",
    "\n",
    "# Wrap in table + adjustbox\n",
    "latex_wrapped = (\n",
    "    \"\\\\begin{table}[!htbp]\\n\"\n",
    "    \"\\\\centering\\n\"\n",
    "    \"\\\\begin{adjustbox}{width=\\\\textwidth}\\n\"\n",
    "    + latex_table +\n",
    "    \"\\\\end{adjustbox}\\n\"\n",
    "    \"\\\\caption{AINI extrema}\\n\"\n",
    "    \"\\\\label{tab:aini_extrema}\\n\"\n",
    "    \"\\\\end{table}\\n\"\n",
    ")\n",
    "\n",
    "# Save to file\n",
    "output_path = table_path / \"aini_extrema_weekly.tex\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(latex_wrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f659810",
   "metadata": {},
   "outputs": [],
   "source": [
    "extrema_weekly_clean.sort_values(\"n measures\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf51d518",
   "metadata": {},
   "source": [
    "load data with AINI predictions & compare corpora with maxima and minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33491ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load & merge w1 data custom finbert data\n",
    "c_df23 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2023_on_binary.csv\")\n",
    "c_df24 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2024_on_binary.csv\")\n",
    "c_df25 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2025_on_binary.csv\")\n",
    "c_df = pd.concat([c_df23,c_df24,c_df25]) \n",
    "\n",
    "# load & merge w0 data\n",
    "w0_df23 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2023_windsize_0.csv\")\n",
    "w0_df24 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2024_windsize_0.csv\")\n",
    "w0_df25 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2025_windsize_0.csv\")\n",
    "w0_df = pd.concat([w0_df23,w0_df24,w0_df25]) \n",
    "\n",
    "# load & merge w1 data\n",
    "w1_df23 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2023_windsize_1.csv\")\n",
    "w1_df24 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2024_windsize_1.csv\")\n",
    "w1_df25 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2025_windsize_1.csv\")\n",
    "w1_df = pd.concat([w1_df23,w1_df24,w1_df25]) \n",
    "\n",
    "# merge on normalized_aini_wo to identify relevant articles\n",
    "w2_df23 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2023_windsize_2.csv\")\n",
    "w2_df24 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2024_windsize_2.csv\")\n",
    "w2_df25 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2025_windsize_2.csv\")\n",
    "w2_df = pd.concat([w2_df23,w2_df24,w2_df25]) \n",
    "\n",
    "# create df list \n",
    "aini_dfs = [c_df,w0_df,w1_df,w2_df]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c605b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify integrity \n",
    "for df in aini_dfs:\n",
    "    print(\n",
    "        f\"\\nFirst entry (date): {df['date'].min()} \"\n",
    "        f\"\\nLast entry (date): {df['date'].max()} \"\n",
    "        f\"\\n# entries: {len(df)} \"\n",
    "        f\"\\n# non-unique article_id: {df['article_id'].duplicated().sum()} \"\n",
    "        f\"\\n# unique article_id: {df['article_id'].nunique()} \" \n",
    "        f\"\\nMin article_id: {df['article_id'].min()} \"\n",
    "        f\"\\nMax article_id: {df['article_id'].max()} \"\n",
    "        f\"\\nColumns: {list(df.columns)}\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a34603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate duplicates in article ids\n",
    "dups = w0_df[w0_df[[\"article_id\", \"title\"]].duplicated(keep=False)].sort_values(\"article_id\")\n",
    "dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995dc208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset for relevant columns\n",
    "rel_col = [\"date\",\"article_id\",\"sentiment_label\",\"hype_score\"]\n",
    "clean_df = []\n",
    "\n",
    "for df in aini_dfs:\n",
    "    df = df[rel_col].copy()\n",
    "    clean_df.append(df)\n",
    "    \n",
    "# unpack supsetted dataframes containing Finbert AINI estimates    \n",
    "c_df_sub,w0_df_sub,w1_df_sub,w2_df_sub = clean_df\n",
    "w2_df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7766e87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify integrity \n",
    "for df in clean_df:\n",
    "    print(\n",
    "        f\"First entry (date): {df['date'].min()} \"\n",
    "        f\"\\nLast entry (date): {df['date'].max()} \"\n",
    "        f\"\\n# entries: {len(df)} \"\n",
    "        f\"\\n# non-unique article_id: {df['article_id'].duplicated().sum()} \"\n",
    "        f\"\\nMin article_id: {df['article_id'].min()} \"\n",
    "        f\"\\nMax article_id: {df['article_id'].max()} \"\n",
    "        f\"\\nColumns: {list(df.columns)}\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d411703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summ(df, name):\n",
    "    n = len(df)\n",
    "    u = df['article_id'].nunique()\n",
    "    dups = df['article_id'].duplicated().sum()\n",
    "    print(f\"{name:>3} rows={n:,}  unique_ids={u:,}  dup_ids={dups:,}\")\n",
    "\n",
    "summ(w0_df_sub, \"w0\")\n",
    "summ(w1_df_sub, \"w1\")\n",
    "summ(w2_df_sub, \"w2\")\n",
    "summ(c_df_sub,  \"c \")\n",
    "\n",
    "ids = {\n",
    "    \"w0\": set(w0_df_sub.article_id),\n",
    "    \"w1\": set(w1_df_sub.article_id),\n",
    "    \"w2\": set(w2_df_sub.article_id),\n",
    "    \"c\" : set(c_df_sub.article_id),\n",
    "}\n",
    "union = set().union(*ids.values())\n",
    "print(\"union unique ids:\", len(union))\n",
    "for k in ids:\n",
    "    print(f\"missing in {k}:\", len(union - ids[k]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7a6486",
   "metadata": {},
   "source": [
    "Combine AINI predictions to investigate minima & maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52882832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# harmonize ids\n",
    "w0 = w0_df_sub.copy()\n",
    "w1 = w1_df_sub.copy()\n",
    "w2 = w2_df_sub.copy()\n",
    "c  = c_df_sub.copy()\n",
    "\n",
    "# bring to datetime, normalize to 00:00:00\n",
    "for df in (w0, w1, w2, c):\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.normalize()\n",
    "\n",
    "# drop dubplicates\n",
    "w0 = w0.sort_values([\"article_id\", \"date\"]).drop_duplicates(\"article_id\", keep=\"last\")\n",
    "w1 = w1.sort_values([\"article_id\", \"date\"]).drop_duplicates(\"article_id\", keep=\"last\")\n",
    "w2 = w2.sort_values([\"article_id\", \"date\"]).drop_duplicates(\"article_id\", keep=\"last\")\n",
    "c  = c .sort_values([\"article_id\", \"date\"]).drop_duplicates(\"article_id\", keep=\"last\")\n",
    "\n",
    "# left join\n",
    "complete_left = w0.copy()\n",
    "complete_left = complete_left.merge(\n",
    "    w1, on=\"article_id\", how=\"left\", suffixes=(\"\", \"_w1\"), validate=\"one_to_one\"\n",
    ")\n",
    "complete_left = complete_left.merge(\n",
    "    w2, on=\"article_id\", how=\"left\", suffixes=(\"\", \"_w2\"), validate=\"one_to_one\"\n",
    ")\n",
    "complete_left = complete_left.merge(\n",
    "    c,  on=\"article_id\", how=\"left\", suffixes=(\"\", \"_c\"),  validate=\"one_to_one\"\n",
    ")\n",
    "\n",
    "# control: outer join\n",
    "complete_outer = w0.merge(\n",
    "    w1, on=\"article_id\", how=\"outer\", suffixes=(\"\", \"_w1\"), validate=\"one_to_one\"\n",
    ")\n",
    "complete_outer = complete_outer.merge(\n",
    "    w2, on=\"article_id\", how=\"outer\", suffixes=(\"\", \"_w2\"), validate=\"one_to_one\"\n",
    ")\n",
    "complete_outer = complete_outer.merge(\n",
    "    c,  on=\"article_id\", how=\"outer\", suffixes=(\"\", \"_c\"),  validate=\"one_to_one\"\n",
    ")\n",
    "\n",
    "\n",
    "# compare joins\n",
    "print(\"[LEFT] n observations:\", len(complete_left))\n",
    "print(\"[OUTER] n observations:\", len(complete_outer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be226a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to bring over from w1_df\n",
    "keep = [\"article_id\", \"title\", \"sub_title\",\"section\", \"cleaned_corpus\", \"date\"]\n",
    "\n",
    "# subset + clean IDs on the right side\n",
    "for_texts = w1_df.loc[:, keep].copy()\n",
    "for_texts[\"article_id\"] = (\n",
    "    for_texts[\"article_id\"]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"[\\u200b\\u200c\\u200d\\ufeff]\", \"\", regex=True)  # zero-width + BOM\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# ensure right side has unique keys for a clean one-to-one merge\n",
    "# (keep the first occurrence; adjust if you prefer 'last')\n",
    "for_texts = for_texts.drop_duplicates(subset=\"article_id\", keep=\"first\")\n",
    "\n",
    "# clean IDs on the left side too (same normalization)\n",
    "complete_left = complete_left.copy()\n",
    "complete_left[\"article_id\"] = (\n",
    "    complete_left[\"article_id\"]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"[\\u200b\\u200c\\u200d\\ufeff]\", \"\", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# merge (left join), bring in *_t suffix to avoid column collisions\n",
    "complete_df = complete_left.merge(\n",
    "    for_texts,\n",
    "    how=\"left\",\n",
    "    on=\"article_id\",\n",
    "    suffixes=(\"\", \"_t\"),\n",
    "    validate=\"one_to_one\",  # will raise if either side still has duplicate keys\n",
    ")\n",
    "\n",
    "print(\"[COMPLETE] n observations:\", len(complete_df))\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac42bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extrema.sort_values(\"n measures\",ascending=False).to_csv(var_path/ \"extrema.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c486a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define minima\n",
    "min_1 = pd.Timestamp(\"2025-02-05\") # n=7\n",
    "min_2a = pd.Timestamp(\"2025-07-15\") # n=2\n",
    "min_2b = pd.Timestamp(\"2025-02-04\") # n=2\n",
    "\n",
    "# define maxima, \n",
    "max_1 = pd.Timestamp(\"2023-04-01\") # n=4\n",
    "max_2a = pd.Timestamp(\"2025-02-13\")  # n=3\n",
    "max_2b = pd.Timestamp(\"2025-09-14\")  # n=3\n",
    "max_2c = pd.Timestamp(\"2025-06-21\")  # n=3\n",
    "\n",
    "# subset original data extrema, minima\n",
    "articles_min1 = complete_df[complete_df[\"date\"] == min_1] \n",
    "articles_min2a = complete_df[complete_df[\"date\"] == min_2a]\n",
    "articles_min2b = complete_df[complete_df[\"date\"] == min_2b]\n",
    "\n",
    "# subset original data extrema, maxmima\n",
    "articles_max1 = complete_df[complete_df[\"date\"] == max_1] \n",
    "articles_max2a = complete_df[complete_df[\"date\"] == max_2a] \n",
    "articles_max2b = complete_df[complete_df[\"date\"] == max_2b] \n",
    "articles_max2c = complete_df[complete_df[\"date\"] == max_2c] \n",
    "\n",
    "# investigate structure\n",
    "articles_min1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8e638",
   "metadata": {},
   "source": [
    "Ensure article count does not bias AINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4c248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure both are datetime \n",
    "merged[\"date\"] = pd.to_datetime(merged[\"date\"]).dt.normalize()\n",
    "n_per_day = (complete_df[[\"article_id\", \"date\"]]\n",
    "             .assign(date=lambda df: pd.to_datetime(df[\"date\"]).dt.normalize())\n",
    "             .groupby(\"date\")\n",
    "             .count()\n",
    "             .rename(columns={\"article_id\": \"n_articles\"}))  \n",
    "\n",
    "# merge\n",
    "aini_article_count = merged.merge(\n",
    "    n_per_day,\n",
    "    on=\"date\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# calculate correlation between n articles & AINI variables\n",
    "for col in aini_article_count.columns:\n",
    "    if col not in [\"date\", \"n_articles\"]:\n",
    "        corr = aini_article_count[\"n_articles\"].corr(aini_article_count[col])\n",
    "        print(f\"{col}: {corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f65b12f",
   "metadata": {},
   "source": [
    "Manually investiagte Minima & Maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d696ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "read(articles_min1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e7529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop noisy estimates\n",
    "merged_clean = merged[articles_min1] # too low samplesize + extrema\n",
    "merged_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48daf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate min 1\n",
    "read(merged[merged[\"date\"] == min_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd978fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate 01.04.2023; max 2 according to normalized_AINI_custom, simple_AINI_custom, EMA_02_custom, EMA_08_custom\n",
    "articles_max1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dad634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop noisy estimates\n",
    "merged_clean = merged[merged[\"date\"] != min_1]\n",
    "\n",
    "# ensure datetime type\n",
    "complete_df[\"date\"] = pd.to_datetime(complete_df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# subsets per year\n",
    "df_2023 = complete_df[complete_df[\"date\"].dt.year == 2023]\n",
    "df_2024 = complete_df[complete_df[\"date\"].dt.year == 2024]\n",
    "df_2025 = complete_df[complete_df[\"date\"].dt.year == 2025]\n",
    "\n",
    "merged_clean\n",
    "merged_clean[\"normalized_AINI_w2\"].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ded2cc",
   "metadata": {},
   "source": [
    "Investigate AINI by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a3b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_aini_series_subplots(merged_clean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
