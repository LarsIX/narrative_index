{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008bb9da",
   "metadata": {},
   "source": [
    "Notebook to compare binary vs. three-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833e5d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ea999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set root and target dir\n",
    "project_root = Path.cwd().parent\n",
    "data_dir = project_root / \"data\" \n",
    "articles_dir = data_dir / \"interim\" \n",
    "\n",
    "# Define columns to keep (in lowercase for consistency)\n",
    "target_columns = [\n",
    "    'article_id', 'title','hype_level'\n",
    "]\n",
    "\n",
    "# List of author-annotated CSV files\n",
    "filenames_author = [\n",
    "    \"articles_WSJ_batch_one_author.csv\",\n",
    "    \"articles_WSJ_batch_two_author.csv\",\n",
    "    \"articles_WSJ_batch_three_author.csv\",\n",
    "    \"articles_WSJ_batch_four_subsample_author.csv\"\n",
    "]\n",
    "\n",
    "# Initialize empty DataFrame for author annotations\n",
    "df_author = pd.DataFrame(columns=target_columns)\n",
    "\n",
    "# Loop through each author file\n",
    "for csv in filenames_author:\n",
    "    path = articles_dir / csv\n",
    "\n",
    "    # Read CSV normally\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Convert all column names to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # Keep only relevant columns\n",
    "    subset = df[target_columns].copy()\n",
    "\n",
    "    # Append to cumulative author DataFrame\n",
    "    df_author = pd.concat([df_author, subset], ignore_index=True)\n",
    "\n",
    "# List of annotator-labeled CSV files\n",
    "filenames_annotator = [\n",
    "    \"articles_WSJ_batch_one_annotator.csv\",\n",
    "    \"articles_WSJ_batch_two_annotator.csv\",\n",
    "    \"articles_WSJ_batch_three_annotator.csv\",\n",
    "    \"articles_WSJ_batch_four_annotator.csv\"\n",
    "]\n",
    "\n",
    "# Initialize empty DataFrame for annotator annotations\n",
    "df_annotator = pd.DataFrame(columns=target_columns)\n",
    "\n",
    "# Loop through each annotator file and handle encoding issues\n",
    "for csv in filenames_annotator:\n",
    "    path = articles_dir / csv\n",
    "\n",
    "    try:\n",
    "        # Attempt UTF-8 encoding\n",
    "        df = pd.read_csv(path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        # Fallback to cp1252 encoding (common Windows encoding)\n",
    "        df = pd.read_csv(path, encoding='cp1252')\n",
    "\n",
    "    # Convert all column names to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # Keep only relevant columns\n",
    "    subset = df[target_columns].copy()\n",
    "\n",
    "    # Append to cumulative annotator DataFrame\n",
    "    df_annotator = pd.concat([df_annotator, subset], ignore_index=True)\n",
    "print(set(df_annotator.hype_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8cc8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_author.merge(df_annotator, on=\"article_id\", suffixes=(\"_auth\", \"_ann\"))\n",
    "\n",
    "# verify that 493 are in the overlapping df\n",
    "print(318 + 1/4 * 700 == len(df_merged))\n",
    "\n",
    "# verify values in hype  columns\n",
    "print(\"Unique values in hype_level_ann:\")\n",
    "print(set(df_merged['hype_level_ann'].dropna()))\n",
    "\n",
    "print(\"\\nUnique values in hype_level_auth:\")\n",
    "print(set(df_merged['hype_level_auth'].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626457d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure consistency\n",
    "df_merged[\"hype_level_ann\"] = df_merged[\"hype_level_ann\"].fillna(0).replace(3,2).astype(int)\n",
    "df_merged[\"hype_level_auth\"] = df_merged[\"hype_level_auth\"].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ad0233",
   "metadata": {},
   "source": [
    "Compare three-class annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca046c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find total divergence\n",
    "n_dif_three = np.sum(df_merged.hype_level_auth != df_merged.hype_level_ann)\n",
    "n_dif_three_rel = n_dif_three / len(df_merged)\n",
    "\n",
    "# show results\n",
    "print(f'There are {n_dif_three} aritcles with different hype-levels, which approximiates {round(n_dif_three_rel,2)*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5652509c",
   "metadata": {},
   "source": [
    "Compare binary annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e2da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform hype scores into binary format\n",
    "df_merged[\"hype_level_ann\"] = df_merged[\"hype_level_ann\"].fillna(0).replace([3, 2], 1).astype(int)\n",
    "df_merged[\"hype_level_auth\"] = df_merged[\"hype_level_auth\"].fillna(0).replace(2, 1).astype(int)\n",
    "\n",
    "# Count divergences between annotator and author\n",
    "n_dif_three_bin = np.sum(df_merged.hype_level_auth != df_merged.hype_level_ann)\n",
    "n_dif_three_rel_bin = n_dif_three_bin / len(df_merged)\n",
    "\n",
    "# Display result as percentage\n",
    "print(f'There are {n_dif_three_bin} articles with different hype levels, which is approximately {round(n_dif_three_rel_bin * 100, 2)}%.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dc9b51",
   "metadata": {},
   "source": [
    "Compare binary annatoation with results of dictionary-based Finbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b8fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load binary predictions for 2024 \n",
    "fin_bin_pred = pd.read_csv(data_dir / \"processed\" / \"variables\" / \"FinBERT_binary_prediction_2024.csv\")\n",
    "\n",
    "#verify load\n",
    "print(fin_bin_pred.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de69b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset for overlaps with annotated df\n",
    "df_merged[\"article_id\"] = df_merged[\"article_id\"].astype(int)\n",
    "fin_bin_pred[\"article_id\"] = fin_bin_pred[\"article_id\"].astype(int)\n",
    "fin_sub = fin_bin_pred[fin_bin_pred[\"article_id\"].isin(df_merged[\"article_id\"])]\n",
    "\n",
    "fin_sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
