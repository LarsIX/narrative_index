{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed3b69bf",
   "metadata": {},
   "source": [
    "Notebook used for initial analysis of the scraped WSJ articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9dfaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sqlite3\n",
    "import re\n",
    "import seaborn as sns   \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# connect to directory\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "annotation_dir = os.path.join(parent_dir,\"src\",\"annotation\")\n",
    "flag_dir = os.path.join(parent_dir,\"src\",\"preprocessing\")\n",
    "\n",
    "if annotation_dir not in sys.path:\n",
    "    sys.path.append(annotation_dir)\n",
    "\n",
    "if flag_dir not in sys.path:\n",
    "    sys.path.append(flag_dir)\n",
    "\n",
    "\n",
    "# import the mentions ai function\n",
    "from simple_ai_filter import flag_ai_mentions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7165ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to subdirectory\n",
    "db_path = os.path.join(parent_dir, \"data\", \"processed\", \"articles\",\"articlesWSJ_clean_2024.db\")\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# explore table names\n",
    "query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "table_names = pd.read_sql_query(query, conn)\n",
    "print(\"Table names in the database:\")\n",
    "print(table_names)\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM article\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8213b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform initial analysis\n",
    "print(\"Number of rows in the DataFrame:\", len(df))\n",
    "print(\"Number of columns in the DataFrame:\", len(df.columns))   \n",
    "print(\"Columns in the DataFrame:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f59abb",
   "metadata": {},
   "source": [
    "Flag articles which contain \"AI, A.I., artificial intelligence, machine learning, deep learning, LLM, GPT, ChatGPT, OpenAI, transformer model or generative AI\" (case-insensitive and uses word boundaries).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d77fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for AI-related articles using importflag_ai_mentions from mentions_ai.py\n",
    "df_labeled = flag_ai_mentions(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c6ce66",
   "metadata": {},
   "source": [
    "Analyze distribution by section and evolution over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9536511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fractioin of AI-related articles\n",
    "print(\"Fraction of AI-related articles:\", df_labeled[\"mentions_ai\"].sum() / len(df_labeled))\n",
    "\n",
    "# sections in the dataset\n",
    "sections = df_labeled[\"section\"].unique()\n",
    "print(\"Sections in the dataset:\", sections)\n",
    "\n",
    "# create list of count for each section\n",
    "section_counts = df_labeled[\"section\"].value_counts()\n",
    "print(\"Counts of articles in each section:\", section_counts)\n",
    "\n",
    "# crea a list of counts for each section by ai_relatedness\n",
    "section_ai_counts = df_labeled.groupby([\"section\", \"mentions_ai\"]).size().unstack(fill_value=0)  \n",
    "print(\"Counts of articles in each section by AI-relatedness:\", section_ai_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00e0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the daily counts by total counts of the given day\n",
    "daily_counts_grouped = df_labeled.groupby([\"date\", \"mentions_ai\"]).size().unstack(fill_value=0).reset_index()\n",
    "daily_counts_grouped[\"frac_ai\"] = daily_counts_grouped[1] / (daily_counts_grouped[0] + daily_counts_grouped[1]) \n",
    "# \n",
    "print(daily_counts_grouped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c068d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v isualize the data using seaborn\n",
    "# Set the style of seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6)) \n",
    "\n",
    "# Create the countplot\n",
    "ax = sns.countplot(data=df_labeled, x=\"section\", hue=\"mentions_ai\", palette=[\"blue\", \"red\"])\n",
    "\n",
    "# Add bar labels\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:\n",
    "        ax.text(\n",
    "            p.get_x() + p.get_width() / 2.,  # x-position\n",
    "            height + 1,                     # y-position \n",
    "            f'n={int(height)}',             # text label\n",
    "            ha=\"center\", va=\"bottom\", fontsize=9\n",
    "        )\n",
    "\n",
    "# Final plot formatting\n",
    "plt.title(\"AI-related Articles by Section in the WSJ (2024)\")\n",
    "plt.xlabel(\"Section\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"AI-related\", loc=\"upper right\", labels=[\"No\", \"Yes\"])\n",
    "plt.tight_layout()\n",
    "fig_path = os.path.join(parent_dir,\"reports\",\"figures\",\"WSJ2024_AIrel_articles_by_section.png\")\n",
    "plt.savefig(fig_path, dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6abfd1",
   "metadata": {},
   "source": [
    "Next, the number of articles per day is analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20dd16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by day and count#\n",
    "df_labeled['date'] = pd.to_datetime(df_labeled['date'])\n",
    "daily_counts = df_labeled.groupby(df_labeled['date'].dt.date).size().reset_index(name='count')\n",
    "\n",
    "# Show all rows in notebook or script\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Filter and display\n",
    "display(daily_counts[daily_counts['count'] < 25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec0d3a3",
   "metadata": {},
   "source": [
    "Investigation is performed over all years (2023-2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aeac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6146da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define root\n",
    "parent_dir = Path(__file__).resolve().parent.parent if \"__file__\" in globals() else Path.cwd().parent\n",
    "\n",
    "# build file paths for all years dynamically\n",
    "data_dir = parent_dir / \"data\" / \"processed\" / \"variables\"\n",
    "files = [\n",
    "    data_dir / f\"FinBERT_AINI_prediction_{year}_windsize_1.csv\"\n",
    "    for year in (2023, 2024, 2025)\n",
    "]\n",
    "\n",
    "# load and concatenate\n",
    "df_total = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\n",
    "\n",
    "print(df_total.shape)\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7570bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore descriptives\n",
    "print(set(df_total.section))\n",
    "\n",
    "# drop prior 01/04/23\n",
    "df_total[\"date\"] = pd.to_datetime(df_total[\"date\"])\n",
    "df_total = df_total[df_total[\"date\"] > pd.to_datetime(\"2023-03-31\")]\n",
    "\n",
    "# count total articles\n",
    "df_total.groupby(df_total[\"date\"].dt.year)[\"date\"].count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4555186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print total number\n",
    "print(df_total.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
