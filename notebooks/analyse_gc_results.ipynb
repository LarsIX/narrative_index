{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda8ae5a",
   "metadata": {},
   "source": [
    "Notebook used to inspect results of Granger Causality analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65204a95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a384acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import math\n",
    "import sys\n",
    "import re\n",
    "from itertools import combinations\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "# Get the project root: notebooks/AI_narrative_index\n",
    "root_dir = Path.cwd().parent\n",
    "\n",
    "# Add needed folders to the Python modules search path\n",
    "sys.path.append(str(root_dir / \"src\" / \"scripts\"))\n",
    "sys.path.append(str(root_dir / \"src\" / \"visualizations\"))\n",
    "sys.path.append(str(root_dir / \"src\" / \"modelling\"))\n",
    "\n",
    "# import custom functions\n",
    "#rom plot_granger_causality import plot_aini_lags_by_year, plot_aini_lags_for_year\n",
    "from plot_functions import plot_n_articles_with_extrema_events, plot_stock_growth\n",
    "from construct_tables import export_regression_table\n",
    "from compute_extrema import compute_aini_extrema\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adb6831",
   "metadata": {},
   "source": [
    "No control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae50519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to variables\n",
    "var_path = root_dir / \"data\" / \"processed\" / \"variables\"\n",
    " \n",
    "# load data (S&P 500 control)\n",
    "gc_c = pd.read_csv(var_path / \"granger_causality_binary.csv\")\n",
    "gc_w0 = pd.read_csv(var_path / \"granger_causality_w0.csv\")\n",
    "gc_w1 = pd.read_csv(var_path / \"granger_causality_w1.csv\")\n",
    "gc_w2 = pd.read_csv(var_path / \"granger_causality_w2.csv\")\n",
    "\n",
    "\n",
    "# create column to indicate version\n",
    "gc_c[\"Model\"] = \"custom\"\n",
    "gc_w0[\"Model\"] = \"w0\"\n",
    "gc_w1[\"Model\"] = \"w1\"\n",
    "gc_w2[\"Model\"] = \"w2\"\n",
    "\n",
    "# merge them together\n",
    "gc_all_results = pd.concat([gc_c, gc_w0, gc_w1, gc_w2], ignore_index=True)\n",
    "gc_all_results[\"joint rej. (α=0.1)\"] = gc_all_results[\"BH_reject_F\"] & gc_all_results[\"BH_reject_F_HC3\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe635b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure no dupls\n",
    "seen = {}\n",
    "new_cols = []\n",
    "\n",
    "for c in gc_all_results.columns:\n",
    "    if c not in seen:\n",
    "        seen[c] = 0\n",
    "        new_cols.append(c)\n",
    "    else:\n",
    "        seen[c] += 1\n",
    "        new_cols.append(f\"{c}.{seen[c]}\")\n",
    "\n",
    "gc_all_results.columns = new_cols\n",
    "\n",
    "gc_all_results.columns\n",
    "\n",
    "# HTML output\n",
    "export_regression_table(\n",
    "    df=gc_all_results,\n",
    "    title=\"Granger-Causality all Results (AINI → Returns)\",\n",
    "    output_filename=\"gc_all_results_sox_cont\",\n",
    "    output_format=\"html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c46433",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = {\n",
    "    \"p_x\": \"Lags\",\n",
    "    \"BH_corr_F_pval\": \"BH empirical p\",\n",
    "    \"BH_corr_F_pval_HC3\": \"BH analytical p\",\n",
    "    \"Year\": \"Period\"\n",
    "}\n",
    "\n",
    "# Add lag-based renames (A2R and R2A)\n",
    "for i in range(1, 4):\n",
    "    rename_map[f\"A2R_beta_ret_{i}\"] = f\"β{i}\"\n",
    "    rename_map[f\"A2R_beta_x_{i}\"] = f\"γ{i}\"\n",
    "    rename_map[f\"R2A_beta_ret_{i}\"] = f\"β{i}\"\n",
    "    rename_map[f\"R2A_beta_x_{i}\"] = f\"γ{i}\"\n",
    "\n",
    "# Apply renaming\n",
    "gc_all_results = gc_all_results.rename(columns=rename_map)\n",
    "gc_all_results\n",
    "\n",
    "# drop non-stationary measures, i.e. windows in 2025; EMA_{0.2} in 2025 for costum\n",
    "gc_all_results_for_report = gc_all_results[(gc_all_results[\"Model\"] == \"custom\") | (gc_all_results[\"Period\"] != \"2025\")]\n",
    "gc_all_results_for_report  = gc_all_results_for_report[~((gc_all_results_for_report[\"AINI_variant\"] == \"EMA_02\") & (gc_all_results_for_report[\"Period\"] == \"2025\"))]\n",
    "\n",
    "print(len(gc_all_results_for_report) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce7f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_all_results_for_report_a2r = gc_all_results_for_report[gc_all_results_for_report[\"Direction\"] == \"AINI_to_RET\"]\n",
    "gc_all_results_for_report_a2r = gc_all_results_for_report_a2r.dropna(axis=1, how='all')\n",
    "\n",
    "# drop non-stationary measures, i.e. windows in 2025; EMA_{0.2} in 2025 for costum\n",
    "gc_all_results_for_report_a2r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab52c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML output\n",
    "export_regression_table(\n",
    "    df=gc_all_results,\n",
    "    title=\"Granger-Causality all Results (AINI → Returns)\",\n",
    "    output_filename=\"gc_sp500_aini_to_ret\",\n",
    "    output_format=\"html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3a0d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rejection rate \n",
    "\n",
    "# Make sure Year and Ticker are strings\n",
    "gc_all_results_for_report_a2r[\"Period\"] = gc_all_results_for_report_a2r[\"Period\"].astype(str)\n",
    "gc_all_results_for_report_a2r[\"Ticker\"] = gc_all_results_for_report_a2r[\"Ticker\"].astype(str)\n",
    "\n",
    "# Total number of models tested\n",
    "total = gc_all_results_for_report_a2r[\"joint rej. (α=0.1)\"].count()\n",
    "\n",
    "# Number of rejections (both bootstrap + HC3 significant)\n",
    "n_reject = gc_all_results_for_report_a2r[\"joint rej. (α=0.1)\"].sum()\n",
    "\n",
    "# Rejection rate\n",
    "rejection_rate = n_reject / total * 100\n",
    "\n",
    "print(f\"Total models: {total}\")\n",
    "print(f\"Both-method rejections: {n_reject}\")\n",
    "print(f\"Rejection rate: {rejection_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8eed49",
   "metadata": {},
   "source": [
    "controlled for n art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e2485",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define path to variables\n",
    "var_path = root_dir / \"data\" / \"processed\" / \"variables\"\n",
    " \n",
    "# load data (S&P 500 control)\n",
    "gc_c = pd.read_csv(var_path / \"granger_causality_log_growth_n_art_binary.csv\")\n",
    "gc_w0 = pd.read_csv(var_path / \"granger_causality_log_growth_n_art_w0.csv\")\n",
    "gc_w1 = pd.read_csv(var_path / \"granger_causality_log_growth_n_art_w1.csv\")\n",
    "gc_w2 = pd.read_csv(var_path / \"granger_causality_log_growth_n_art_w2.csv\")\n",
    "\n",
    "\n",
    "# create column to indicate version\n",
    "gc_c[\"Model\"] = \"custom\"\n",
    "gc_w0[\"Model\"] = \"w0\"\n",
    "gc_w1[\"Model\"] = \"w1\"\n",
    "gc_w2[\"Model\"] = \"w2\"\n",
    "\n",
    "# merge them together\n",
    "gc_all_results_art = pd.concat([gc_c, gc_w0, gc_w1, gc_w2], ignore_index=True)\n",
    "gc_all_results_art[\"joint rej. (α=0.1)\"] = gc_all_results_art[\"BH_reject_F\"] & gc_all_results_art[\"BH_reject_F_HC3\"]\n",
    "gc_all_results_art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55646f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure no dupls\n",
    "seen = {}\n",
    "new_cols = []\n",
    "\n",
    "for c in gc_all_results_art.columns:\n",
    "    if c not in seen:\n",
    "        seen[c] = 0\n",
    "        new_cols.append(c)\n",
    "    else:\n",
    "        seen[c] += 1\n",
    "        new_cols.append(f\"{c}.{seen[c]}\")\n",
    "\n",
    "gc_all_results_art.columns = new_cols\n",
    "\n",
    "# HTML output\n",
    "export_regression_table(\n",
    "    df=gc_all_results_art,\n",
    "    title=\"Granger-Causality all Results (controlled for log growth daily articles)\",\n",
    "    output_filename=\"gc_all_results_article_cont\",\n",
    "    output_format=\"html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f436ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_all_results_art.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520ac9f5",
   "metadata": {},
   "source": [
    "Controlled for sox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95ee34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to variables\n",
    "var_path = root_dir / \"data\" / \"processed\" / \"variables\"\n",
    " \n",
    "# load data (S&P 500 control)\n",
    "gc_c_sox = pd.read_csv(var_path / \"granger_causality_log_growth_sox_binary.csv\")\n",
    "gc_w0_sox = pd.read_csv(var_path / \"granger_causality_log_growth_sox_w0.csv\")\n",
    "gc_w1_sox = pd.read_csv(var_path / \"granger_causality_log_growth_sox_w1.csv\")\n",
    "gc_w2_sox = pd.read_csv(var_path / \"granger_causality_log_growth_sox_w2.csv\")\n",
    "\n",
    "\n",
    "# create column to indicate version\n",
    "gc_c_sox[\"Model\"] = \"custom\"\n",
    "gc_w0_sox[\"Model\"] = \"w0\"\n",
    "gc_w1_sox[\"Model\"] = \"w1\"\n",
    "gc_w2_sox[\"Model\"] = \"w2\"\n",
    "\n",
    "rename_map = {\n",
    "    # bookkeeping\n",
    "    \"p_x\": \"Lags\",\n",
    "    \"Original_F_pval\": \"analytical P\",\n",
    "    \"Empirical_F_pval\": \"empirical P\",\n",
    "    \"BH_corr_F_pval\": \"BH empirical p\",\n",
    "    \"BH_corr_F_pval_HC3\": \"BH analytical p\",\n",
    "    \"Year\": \"Period\",\n",
    "\n",
    "    # lagged return betas (AINI → Returns)\n",
    "    \"A2R_beta_ret_1\": \"β1\",\n",
    "    \"A2R_beta_ret_2\": \"β2\",\n",
    "    \"A2R_beta_ret_3\": \"β3\",\n",
    "\n",
    "    # AINI lag coefficients\n",
    "    \"A2R_beta_x_1\": \"γ1\",\n",
    "    \"A2R_beta_x_2\": \"γ2\",\n",
    "    \"A2R_beta_x_3\": \"γ3\",\n",
    "\n",
    "    # VIX control coefficients (original + .1 versions)\n",
    "    \"β_ctrl_log_growth_closed1\": \"ζ1\",\n",
    "    \"β_ctrl_log_growth_closed2\": \"ζ2\",\n",
    "    \"β_ctrl_log_growth_closed3\": \"ζ3\",\n",
    "    \"β_ctrl_log_growth_closed1.1\": \"ζ1\",\n",
    "    \"β_ctrl_log_growth_closed2.1\": \"ζ2\",\n",
    "    \"β_ctrl_log_growth_closed3.1\": \"ζ3\",\n",
    "\n",
    "    # ---- Reverse direction (Returns → AINI) ----\n",
    "    \"R2A_beta_ret_1\": \"β1\",\n",
    "    \"R2A_beta_ret_2\": \"β2\",\n",
    "    \"R2A_beta_ret_3\": \"β3\",\n",
    "\n",
    "    \"R2A_beta_x_1\": \"γ1\",\n",
    "    \"R2A_beta_x_2\": \"γ2\",\n",
    "    \"R2A_beta_x_3\": \"γ3\",\n",
    "}\n",
    "\n",
    "rename_map = {\n",
    "    \"p_x\": \"Lags\",\n",
    "    \"BH_corr_F_pval\": \"BH empirical p\",\n",
    "    \"BH_corr_F_pval_HC3\": \"BH analytical p\",\n",
    "    \"Year\": \"Period\"\n",
    "}\n",
    "\n",
    "# Add lag-based renames (A2R and R2A)\n",
    "for i in range(1, 4):\n",
    "    rename_map[f\"A2R_beta_ret_{i}\"] = f\"β{i}\"\n",
    "    rename_map[f\"A2R_beta_x_{i}\"] = f\"γ{i}\"\n",
    "    rename_map[f\"R2A_beta_ret_{i}\"] = f\"β{i}\"\n",
    "    rename_map[f\"R2A_beta_x_{i}\"] = f\"γ{i}\"\n",
    "\n",
    "\n",
    "# merge them together\n",
    "gc_all_results_sox = pd.concat([gc_c_sox, gc_w0_sox, gc_w1_sox, gc_w2_sox], ignore_index=True)\n",
    "gc_all_results_sox[\"joint rej. (α=0.1)\"] = gc_all_results_sox[\"BH_reject_F\"] & gc_all_results_sox[\"BH_reject_F_HC3\"]\n",
    "\n",
    "# Apply renaming\n",
    "gc_all_results_sox = gc_all_results_sox.rename(columns=rename_map)\n",
    "gc_all_results_sox\n",
    "gc_all_results_sox.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe0f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure no dupls\n",
    "seen = {}\n",
    "new_cols = []\n",
    "\n",
    "for c in gc_all_results_sox.columns:\n",
    "    if c not in seen:\n",
    "        seen[c] = 0\n",
    "        new_cols.append(c)\n",
    "    else:\n",
    "        seen[c] += 1\n",
    "        new_cols.append(f\"{c}.{seen[c]}\")\n",
    "\n",
    "gc_all_results_sox.columns = new_cols\n",
    "\n",
    "gc_all_results_sox.columns\n",
    "\n",
    "# HTML output\n",
    "export_regression_table(\n",
    "    df=gc_all_results_sox,\n",
    "    title=\"Granger-Causality all Results (controlled for SOX)\",\n",
    "    output_filename=\"gc_all_results_sox_cont\",\n",
    "    output_format=\"html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fada05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply renaming\n",
    "gc_all_results_sox = gc_all_results_sox.rename(columns=rename_map)\n",
    "gc_all_results_sox\n",
    "\n",
    "gc_all_results_for_report_sox = gc_all_results_sox.copy()\n",
    "gc_all_results_for_report_sox\n",
    "\n",
    "# drop non-stationary measures, i.e. windows in 2025; EMA_{0.2} in 2025 for costum\n",
    "gc_all_results_for_report_sox = gc_all_results_for_report_sox[(gc_all_results_for_report_sox[\"Model\"] == \"custom\") | (gc_all_results_for_report_sox[\"Period\"] != \"2025\")]\n",
    "gc_all_results_for_report_sox  = gc_all_results_for_report_sox[~((gc_all_results_for_report_sox[\"AINI_variant\"] == \"EMA_02\") & (gc_all_results_for_report_sox[\"Period\"] == \"2025\"))]\n",
    "\n",
    "print(len(gc_all_results_for_report_sox) / 2)\n",
    "gc_c_sox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14661898",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"AINI_variant\", \"Ticker\", \"Period\", \"Model\"]\n",
    "\n",
    "# ensure string dtype for merge keys\n",
    "for c in cols:\n",
    "    gc_all_results[c] = gc_all_results[c].astype(str)\n",
    "    gc_all_results_sox[c] = gc_all_results_sox[c].astype(str)\n",
    "\n",
    "# ensure subset of relevant columns exists in both DataFrames\n",
    "missing_rows = gc_all_results.merge(\n",
    "    gc_all_results_sox[cols],\n",
    "    on=cols,\n",
    "    how=\"left\",\n",
    "    indicator=True\n",
    ").query('_merge == \"left_only\"') \\\n",
    " .drop(columns=\"_merge\")\n",
    "\n",
    "# display or save\n",
    "print(missing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_all_results_for_report_a2r_sox = gc_all_results_for_report_sox[gc_all_results_for_report_sox[\"Direction\"] == \"AINI_to_RET\"]\n",
    "gc_all_results_for_report_a2r_sox = gc_all_results_for_report_a2r_sox.dropna(axis=1, how='all')\n",
    "gc_all_results_for_report_a2r_sox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c7b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rejection rate \n",
    "\n",
    "# Make sure Year and Ticker are strings\n",
    "gc_all_results_for_report_a2r_sox[\"Period\"] = gc_all_results_for_report_a2r_sox[\"Period\"].astype(str)\n",
    "gc_all_results_for_report_a2r_sox[\"Ticker\"] = gc_all_results_for_report_a2r_sox[\"Ticker\"].astype(str)\n",
    "\n",
    "# Total number of models tested\n",
    "total = gc_all_results_for_report_a2r_sox[\"joint rej. (α=0.1)\"].count()\n",
    "\n",
    "# Number of rejections (both bootstrap + HC3 significant)\n",
    "n_reject = gc_all_results_for_report_a2r_sox[\"joint rej. (α=0.1)\"].sum()\n",
    "\n",
    "# Rejection rate\n",
    "rejection_rate = n_reject / total * 100\n",
    "\n",
    "print(f\"Total models: {total}\")\n",
    "print(f\"Both-method rejections: {n_reject}\")\n",
    "print(f\"Rejection rate: {rejection_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d65cd3d",
   "metadata": {},
   "source": [
    "Controlled for S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4818a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to variables\n",
    "var_path = root_dir / \"data\" / \"processed\" / \"variables\"\n",
    " \n",
    "# load data (S&P 500 control)\n",
    "gc_c = pd.read_csv(var_path / \"granger_causality_log_growth_sp500_binary.csv\")\n",
    "gc_w0 = pd.read_csv(var_path / \"granger_causality_log_growth_sp500_w0.csv\")\n",
    "gc_w1 = pd.read_csv(var_path / \"granger_causality_log_growth_sp500_w1.csv\")\n",
    "gc_w2 = pd.read_csv(var_path / \"granger_causality_log_growth_sp500_w2.csv\")\n",
    "\n",
    "\n",
    "# create column to indicate version\n",
    "gc_c[\"Model\"] = \"custom\"\n",
    "gc_w0[\"Model\"] = \"w0\"\n",
    "gc_w1[\"Model\"] = \"w1\"\n",
    "gc_w2[\"Model\"] = \"w2\"\n",
    "\n",
    "# merge them together\n",
    "gc_all_results = pd.concat([gc_c, gc_w0, gc_w1, gc_w2], ignore_index=True)\n",
    "gc_all_results_sp500 = gc_all_results.copy()\n",
    "gc_all_results_sp500[\"joint rej. (α=0.1)\"] = gc_all_results[\"BH_reject_F\"] & gc_all_results[\"BH_reject_F_HC3\"]\n",
    "gc_all_results_sp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe5705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure no dupls\n",
    "seen = {}\n",
    "new_cols = []\n",
    "\n",
    "for c in gc_all_results_sp500.columns:\n",
    "    if c not in seen:\n",
    "        seen[c] = 0\n",
    "        new_cols.append(c)\n",
    "    else:\n",
    "        seen[c] += 1\n",
    "        new_cols.append(f\"{c}.{seen[c]}\")\n",
    "\n",
    "gc_all_results_sp500.columns = new_cols\n",
    "\n",
    "gc_all_results_sp500.columns\n",
    "\n",
    "# HTML output\n",
    "export_regression_table(\n",
    "    df=gc_all_results_sp500,\n",
    "    title=\"Granger-Causality all Results (controlled for S&P500)\",\n",
    "    output_filename=\"gc_all_results_sp500_cont\",\n",
    "    output_format=\"html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4754a15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = {\n",
    "    \"p_x\": \"Lags\",\n",
    "    \"BH_corr_F_pval\": \"BH empirical p\",\n",
    "    \"BH_corr_F_pval_HC3\": \"BH analytical p\",\n",
    "    \"Year\": \"Period\"\n",
    "}\n",
    "\n",
    "# Add lag-based renames (A2R and R2A)\n",
    "for i in range(1, 4):\n",
    "    rename_map[f\"A2R_beta_ret_{i}\"] = f\"β{i}\"\n",
    "    rename_map[f\"A2R_beta_x_{i}\"] = f\"γ{i}\"\n",
    "    rename_map[f\"R2A_beta_ret_{i}\"] = f\"β{i}\"\n",
    "    rename_map[f\"R2A_beta_x_{i}\"] = f\"γ{i}\"\n",
    "\n",
    "# Apply renaming\n",
    "gc_all_results_sp500 = gc_all_results.rename(columns=rename_map)\n",
    "gc_all_results_sp500\n",
    "\n",
    "# drop non-stationary measures, i.e. windows in 2025; EMA_{0.2} in 2025 for costum\n",
    "gc_all_results_sp500 = gc_all_results_sp500[(gc_all_results_sp500[\"Model\"] == \"custom\") | (gc_all_results_sp500[\"Period\"] != \"2025\")]\n",
    "gc_all_results_sp500  = gc_all_results_sp500[~((gc_all_results_sp500[\"AINI_variant\"] == \"EMA_02\") & (gc_all_results_sp500[\"Period\"] == \"2025\"))]\n",
    "\n",
    "gc_all_results_sp500_for_report = gc_all_results_sp500.copy()\n",
    "\n",
    "print(len(gc_all_results_sp500_for_report) / 2)\n",
    "\n",
    "gc_all_results_sp500_for_report_a2r = gc_all_results_sp500_for_report[gc_all_results_sp500_for_report[\"Direction\"] == \"AINI_to_RET\"]\n",
    "gc_all_results_sp500_for_report_a2r = gc_all_results_sp500_for_report_a2r.dropna(axis=1, how='all')\n",
    "gc_all_results_sp500_for_report_a2r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56beaf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rejection rate \n",
    "\n",
    "# Make sure Year and Ticker are strings\n",
    "gc_all_results_sp500_for_report_a2r[\"Period\"] = gc_all_results_sp500_for_report_a2r[\"Period\"].astype(str)\n",
    "gc_all_results_sp500_for_report_a2r[\"Ticker\"] = gc_all_results_sp500_for_report_a2r[\"Ticker\"].astype(str)\n",
    "\n",
    "# Total number of models tested\n",
    "gc_all_results_sp500_for_report_a2r[\"joint rej. (α=0.1)\"] = gc_all_results_sp500_for_report_a2r[\"BH_reject_F\"] & gc_all_results_sp500_for_report_a2r[\"BH_reject_F_HC3\"]\n",
    "\n",
    "total = gc_all_results_sp500_for_report_a2r[\"joint rej. (α=0.1)\"].count()\n",
    "\n",
    "# Number of rejections (both bootstrap + HC3 significant)\n",
    "n_reject = gc_all_results_sp500_for_report_a2r[\"joint rej. (α=0.1)\"].sum()\n",
    "\n",
    "# Rejection rate\n",
    "rejection_rate = n_reject / total * 100\n",
    "\n",
    "print(f\"Total models: {total}\")\n",
    "print(f\"Both-method rejections: {n_reject}\")\n",
    "print(f\"Rejection rate: {rejection_rate:.2f}%\")\n",
    "gc_all_results_for_report_a2r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a93053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find divergence no control vs S&P 500\n",
    "\n",
    "# Define key columns\n",
    "keys = [\"Ticker\", \"AINI_variant\", \"Period\",\"Lags\"]\n",
    "\n",
    "# Filter both for joint rejections\n",
    "a_sig = gc_all_results_sp500_for_report_a2r.loc[\n",
    "    gc_all_results_sp500_for_report_a2r[\"joint rej. (α=0.1)\"] == True, keys\n",
    "]\n",
    "b_sig = gc_all_results_for_report_a2r.loc[\n",
    "    gc_all_results_for_report_a2r[\"joint rej. (α=0.1)\"] == True, keys\n",
    "]\n",
    "\n",
    "# Convert to sets of unique tuples\n",
    "set_a = set(map(tuple, a_sig.drop_duplicates().to_numpy()))\n",
    "set_b = set(map(tuple, b_sig.drop_duplicates().to_numpy()))\n",
    "\n",
    "# Compute differences\n",
    "only_in_a = set_a - set_b\n",
    "only_in_b = set_b - set_a\n",
    "common = set_a & set_b\n",
    "\n",
    "# Quick stats\n",
    "print(f\"Unique in SP500 (only_in_a): {len(only_in_a)}\")\n",
    "print(f\"Unique in All (only_in_b):   {len(only_in_b)}\")\n",
    "print(f\"Common significant pairs:    {len(common)}\")\n",
    "\n",
    "# Optional DataFrames for inspection\n",
    "only_in_a_df = pd.DataFrame(list(only_in_a), columns=keys)\n",
    "only_in_b_df = pd.DataFrame(list(only_in_b), columns=keys)\n",
    "only_in_a_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8362e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML output\n",
    "export_regression_table(\n",
    "    df=gc_all_results_sp500,\n",
    "    title=\"Granger-Causality all Results (AINI → Returns, controlled for S&P500)\",\n",
    "    output_filename=\"gc_all_results_sp500_cont\",\n",
    "    output_format=\"html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09534a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate significance\n",
    "gc_all_results_sp500[\"joint rej. (α=0.1)\"] = gc_all_results_sp500[\"BH_reject_F\"] & gc_all_results_sp500[\"BH_reject_F_HC3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81de5455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset by direction\n",
    "sp500_aini_to_ret = gc_all_results_sp500[gc_all_results_sp500[\"Direction\"] == \"AINI_to_RET\"]\n",
    "sp500_ret_to_aini = gc_all_results_sp500[gc_all_results_sp500[\"Direction\"] == \"RET_to_AINI\"]\n",
    "\n",
    "# cols to keep\n",
    "keep_a2r = [\n",
    "    \"Model\", \"AINI_variant\",\"adj_r2_u\", \"Ticker\", \"Period\", \"Lags\",\n",
    "    \"β1\", \"β2\", \"β3\",\n",
    "    \"γ1\", \"γ2\", \"γ3\",\n",
    "    \"BH empirical p\", \"BH analytical p\", \"joint rej. (α=0.1)\"\n",
    "]\n",
    "\n",
    "keep_r2a = [\n",
    "    \"Model\", \"AINI_variant\",\"adj_r2_u\", \"Ticker\", \"Period\", \"Lags\",\n",
    "    \"β1\", \"β2\", \"β3\",\n",
    "    \"γ1\", \"γ2\", \"γ3\",\n",
    "    \"BH empirical p\", \"BH analytical p\", \"joint rej. (α=0.1)\"\n",
    "]\n",
    "\n",
    "# subset\n",
    "sp500_aini_to_ret_sub = sp500_aini_to_ret[keep_a2r]\n",
    "sp500_ret_to_aini_sub = sp500_ret_to_aini[keep_r2a]\n",
    "\n",
    "sp500_aini_to_ret.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73eba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_aini_to_ret_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b65a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rejection rate \n",
    "\n",
    "# Make sure Year and Ticker are strings\n",
    "sp500_aini_to_ret_sub[\"Period\"] = sp500_aini_to_ret_sub[\"Period\"].astype(str)\n",
    "sp500_aini_to_ret_sub[\"Ticker\"] = sp500_aini_to_ret_sub[\"Ticker\"].astype(str)\n",
    "\n",
    "# \n",
    "\n",
    "# Total number of models tested\n",
    "total = sp500_aini_to_ret_sub[\"joint rej. (α=0.1)\"].count()\n",
    "\n",
    "# Number of rejections (both bootstrap + HC3 significant)\n",
    "n_reject = sp500_aini_to_ret_sub[\"joint rej. (α=0.1)\"].sum()\n",
    "\n",
    "# Rejection rate\n",
    "rejection_rate = n_reject / total * 100\n",
    "\n",
    "print(f\"Total models: {total}\")\n",
    "print(f\"Both-method rejections: {n_reject}\")\n",
    "print(f\"Rejection rate: {rejection_rate:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3834ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rejection rate \n",
    "\n",
    "# Make sure Year and Ticker are strings\n",
    "sp500_ret_to_aini_sub[\"Period\"] = sp500_ret_to_aini_sub[\"Period\"].astype(str)\n",
    "sp500_ret_to_aini_sub[\"Ticker\"] = sp500_ret_to_aini_sub[\"Ticker\"].astype(str)\n",
    "\n",
    "# Total number of models tested\n",
    "total = sp500_ret_to_aini_sub[\"joint rej. (α=0.1)\"].count()\n",
    "\n",
    "# Number of rejections (both bootstrap + HC3 significant)\n",
    "n_reject = sp500_ret_to_aini_sub[\"joint rej. (α=0.1)\"].sum()\n",
    "\n",
    "# Rejection rate\n",
    "rejection_rate = n_reject / total * 100\n",
    "\n",
    "print(f\"Total models: {total}\")\n",
    "print(f\"Both-method rejections: {n_reject}\")\n",
    "print(f\"Rejection rate: {rejection_rate:.2f}%\")\n",
    "sp500_ret_to_aini_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0915183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset significant results\n",
    "sp500_aini_to_ret_sig = sp500_aini_to_ret_sub[sp500_aini_to_ret_sub[\"joint rej. (α=0.1)\"] == True]\n",
    "sp500_ret_to_aini_sig = sp500_ret_to_aini_sub[sp500_ret_to_aini_sub[\"joint rej. (α=0.1)\"] == True]\n",
    "# drop 0 value cols\n",
    "sp500_ret_to_aini_sig = sp500_ret_to_aini_sig.dropna(axis=1,how=\"all\")\n",
    "\n",
    "# Coerce β columns to numeric \n",
    "sp500_ret_to_aini_sig[[\"β1\",\"β2\",\"β3\"]] = (\n",
    "    sp500_ret_to_aini_sig[[\"β1\",\"β2\",\"β3\"]]\n",
    "    .apply(pd.to_numeric, errors=\"coerce\")\n",
    ")\n",
    "sp500_ret_to_aini_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb309c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sp500_aini_to_ret_sig.copy()\n",
    "df[\"Period\"] = df[\"Period\"].replace({\n",
    "    \"2023_24_25\": \"2023-2025\",\n",
    "    \"2024_25\": \"2024-2025\",\n",
    "    \"2023_24\": \"2023-2024\"\n",
    "}).astype(str)\n",
    "df[\"Ticker\"] = df[\"Ticker\"].astype(str)\n",
    "\n",
    "#  build order \n",
    "def period_key(p: str):\n",
    "    years = list(map(int, re.findall(r\"\\d{4}\", p)))\n",
    "    if len(years) == 1:\n",
    "        years = [years[0], years[0]]\n",
    "    start, end = years[0], years[-1]\n",
    "    return (end, start) \n",
    "\n",
    "periods = sorted(df[\"Period\"].unique(), key=period_key, reverse=True)\n",
    "df[\"Period\"] = pd.Categorical(df[\"Period\"], categories=periods, ordered=True)\n",
    "\n",
    "# plot \n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.2)\n",
    "plt.figure(figsize=(10, 5), dpi=300)\n",
    "\n",
    "ax = sns.countplot(\n",
    "    data=df,\n",
    "    x=\"Ticker\",\n",
    "    hue=\"Period\",\n",
    "    hue_order=periods,                          \n",
    "    palette=sns.color_palette(\"viridis\", n_colors=len(periods)),\n",
    "    edgecolor=\"black\"\n",
    ")\n",
    "\n",
    "ax.set_title(\"Significant results by Ticker and Period\", fontsize=13, pad=10)\n",
    "ax.set_xlabel(\"Ticker\", fontsize=12)\n",
    "ax.set_ylabel(\"Count of Significant Results\", fontsize=12)\n",
    "ax.legend(title=\"Period\", title_fontsize=11, fontsize=10, loc=\"upper right\", frameon=True)\n",
    "\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "sns.despine(trim=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(root_dir / \"reports/figures/sp500_aini_to_ret_sig_counts.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9694900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by model\n",
    "model_group_tickers = (\n",
    "    sp500_aini_to_ret_sig\n",
    "    .groupby([\"Ticker\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"jointly rejected at α=0.1\")\n",
    "    .sort_values(by=\"jointly rejected at α=0.1\",ascending=False)\n",
    ")\n",
    "\n",
    "\n",
    "# print\n",
    "print(model_group_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ff46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by model all\n",
    "model_group_tickers = (\n",
    "    sp500_aini_to_ret\n",
    "    .groupby([\"Model\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"jointly rejected at α=0.1\")\n",
    "    .sort_values(by=\"jointly rejected at α=0.1\",ascending=False)\n",
    ")\n",
    "\n",
    "\n",
    "# print\n",
    "print(model_group_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317078db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by period\n",
    "model_group_period = (\n",
    "    sp500_aini_to_ret_sig\n",
    "    .groupby([\"Period\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"jointly rejected at α=0.1\")\n",
    "    .sort_values(by=\"jointly rejected at α=0.1\",ascending=False)\n",
    ")\n",
    "\n",
    "\n",
    "# print\n",
    "print(model_group_period)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4de03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by period total\n",
    "model_group_period = (\n",
    "    sp500_aini_to_ret\n",
    "    .groupby([\"Period\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"jointly rejected at α=0.1\")\n",
    "    .sort_values(by=\"jointly rejected at α=0.1\",ascending=False)\n",
    ")\n",
    "\n",
    "\n",
    "# print\n",
    "print(model_group_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d9394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by Ticker\n",
    "model_group_period = (\n",
    "    sp500_aini_to_ret_sig\n",
    "    .groupby([\"Ticker\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"jointly rejected at α=0.1\")\n",
    "    .sort_values(by=\"jointly rejected at α=0.1\",ascending=False)\n",
    ")\n",
    "\n",
    "\n",
    "# print\n",
    "print(model_group_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab053ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by Ticker total\n",
    "model_group_period = (\n",
    "    sp500_aini_to_ret\n",
    "    .groupby([\"Ticker\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"jointly rejected at α=0.1\")\n",
    "    .sort_values(by=\"jointly rejected at α=0.1\",ascending=False)\n",
    ")\n",
    "\n",
    "\n",
    "# print\n",
    "sp500_aini_to_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd3720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by period & Ticker\n",
    "model_group_tickers = (\n",
    "    sp500_aini_to_ret_sig\n",
    "    .groupby([\"Model\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"jointly rejected at α=0.1\")\n",
    ")\n",
    "\n",
    "\n",
    "# print\n",
    "print(model_group_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b31a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by variant\n",
    "model_group_measure = (\n",
    "    sp500_aini_to_ret_sig\n",
    "    .groupby([\"AINI_variant\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"n_variants\")\n",
    "    .sort_values(by=\"n_variants\",ascending=False)\n",
    ")\n",
    "\n",
    "print(model_group_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebcd22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find distinctions between models\n",
    "keys = [\"Ticker\", \"Period\"]  \n",
    "models = [\"w0\", \"w1\", \"w2\", \"custom\"]               \n",
    "\n",
    "common_dfs = []\n",
    "left_only_dfs = []\n",
    "right_only_dfs = []\n",
    "\n",
    "for m1, m2 in combinations(models, 2):\n",
    "    df1 = sp500_aini_to_ret_sig.loc[sp500_aini_to_ret_sig[\"Model\"] == m1, keys].drop_duplicates()\n",
    "    df2 = sp500_aini_to_ret_sig.loc[sp500_aini_to_ret_sig[\"Model\"] == m2, keys].drop_duplicates()\n",
    "\n",
    "    # intersection\n",
    "    common = df1.merge(df2, on=keys, how=\"inner\")\n",
    "    if not common.empty:\n",
    "        common = common.assign(Model_pair=f\"{m1}&{m2}\")\n",
    "        common_dfs.append(common)\n",
    "\n",
    "    # only in left / only in right\n",
    "    cmp = df1.merge(df2, on=keys, how=\"outer\", indicator=True)\n",
    "    left_only  = cmp.loc[cmp[\"_merge\"] == \"left_only\",  keys].assign(only=m1)\n",
    "    right_only = cmp.loc[cmp[\"_merge\"] == \"right_only\", keys].assign(only=m2)\n",
    "\n",
    "    if not left_only.empty:\n",
    "        left_only_dfs.append(left_only)\n",
    "    if not right_only.empty:\n",
    "        right_only_dfs.append(right_only)\n",
    "\n",
    "# Concatenate\n",
    "common_all = pd.concat(common_dfs, ignore_index=True) if common_dfs else pd.DataFrame(columns=keys+[\"Model_pair\"])\n",
    "left_only_all = pd.concat(left_only_dfs, ignore_index=True) if left_only_dfs else pd.DataFrame(columns=keys+[\"only\"])\n",
    "right_only_all = pd.concat(right_only_dfs, ignore_index=True) if right_only_dfs else pd.DataFrame(columns=keys+[\"only\"])\n",
    "\n",
    "right_only_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6310985",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"Ticker\", \"Period\", \"Model\"]\n",
    "\n",
    "# Defensive copies (optional but clean)\n",
    "A = sp500_aini_to_ret_sig.copy()\n",
    "B = sp500_ret_to_aini_sig.copy()\n",
    "\n",
    "# Normalize data types\n",
    "for c in keys:\n",
    "    A[c] = A[c].astype(str)\n",
    "    B[c] = B[c].astype(str)\n",
    "\n",
    "# Build sets of unique tuples\n",
    "S_A = set(map(tuple, A[keys].drop_duplicates().to_numpy()))\n",
    "S_B = set(map(tuple, B[keys].drop_duplicates().to_numpy()))\n",
    "\n",
    "# Compute intersection and counts\n",
    "intersection = S_A & S_B\n",
    "n_inter = len(intersection)\n",
    "\n",
    "print(f\"Common observations (unique {keys}): {n_inter}\")\n",
    "print(f\"|A| = {len(S_A)}, |B| = {len(S_B)}, overlap = {n_inter / len(S_A):.2%} of A, {n_inter / len(S_B):.2%} of B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065c1400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard for aini-Ret; ret -> AINI sp500_ret_to_aini_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ddfc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate groups by Model\n",
    "model_group_model = (\n",
    "    sp500_aini_to_ret_sig\n",
    "    .groupby([\"Model\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"n_variants\")\n",
    "    .sort_values(by=\"n_variants\",ascending=False)\n",
    ")\n",
    "print(model_group_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babc2037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beautify\n",
    "sp500_aini_to_ret_sig[\"Period\"] = sp500_aini_to_ret_sig[\"Period\"].replace({\"2023_24\": \"2023-2024\"})\n",
    "sp500_aini_to_ret_sig[\"Period\"] = sp500_aini_to_ret_sig[\"Period\"].replace({\"2024_25\": \"2024-2025\"})\n",
    "sp500_aini_to_ret_sig[\"Period\"] = sp500_aini_to_ret_sig[\"Period\"].replace({\"2023_24_25\": \"2023-2025\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d07e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by betas\n",
    "sp500_aini_to_ret_sig = sp500_aini_to_ret_sig.dropna(axis=1,how=\"all\")\n",
    "\n",
    "sp500_aini_to_ret_sort = sp500_aini_to_ret_sig.assign(abs_β1=lambda x: x[\"β1\"].abs()).sort_values(\"abs_β1\", ascending=False)\n",
    "sp500_aini_to_ret_sort_cut = sp500_aini_to_ret_sort.iloc[0:20]\n",
    "\n",
    "# inspect \n",
    "sp500_aini_to_ret_sort_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9439db15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted latex output\n",
    "export_regression_table(\n",
    "    df=sp500_aini_to_ret_sort,\n",
    "    title=\"Granger-Causality, jointly significant results (AINI $\\\\to$ Returns, controlled for S\\\\&P~500). \\\\textit{Source:} Own.\",\n",
    "    output_filename=\"gc_sp500_aini_to_ret_sort_beta\",  \n",
    "    output_format=\"tex\",\n",
    "    latex_env=\"tabular\",          \n",
    "    include_caption_label=False,    \n",
    "    coef_digits=3,\n",
    "    p_digits=3,\n",
    "    tabcolsep_pt=2.0,\n",
    "    font_size_cmd=\"scriptsize\",   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7801c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_regression_table(\n",
    "    df=sp500_aini_to_ret_sort,\n",
    "    title=\"Granger-Causality, jointly significant results (AINI → Returns, controlled for S&P 500)\",\n",
    "    output_filename=\"gc_sp500_aini_to_ret_sort_beta\",\n",
    "    output_format=\"tex\",\n",
    "    latex_env=\"longtable\",         \n",
    "    include_caption_label=True,    \n",
    "    font_size_cmd=\"scriptsize\",\n",
    "    tabcolsep_pt=2.0,\n",
    "    coef_digits=3,\n",
    "    p_digits=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8713cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beautify opposite direction \n",
    "sp500_ret_to_aini_sig[\"Period\"] = sp500_ret_to_aini_sig[\"Period\"].replace({\"2023_24\": \"2023-2024\"})\n",
    "sp500_ret_to_aini_sig[\"Period\"] = sp500_ret_to_aini_sig[\"Period\"].replace({\"2024_25\": \"2024-2025\"})\n",
    "sp500_ret_to_aini_sig[\"Period\"] = sp500_ret_to_aini_sig[\"Period\"].replace({\"2023_24_25\": \"2023-2025\"})\n",
    "\n",
    "# drop NA\n",
    "sp500_ret_to_aini_sig = sp500_ret_to_aini_sig.dropna(axis=1, how=\"all\")\n",
    "\n",
    "# sort by betas\n",
    "sp500_ret_to_aini_sig_sort = sp500_ret_to_aini_sig.assign(abs_γ1=lambda x: x[\"γ1\"].abs()).sort_values(\"abs_γ1\", ascending=False)\n",
    "sp500_ret_to_aini_sig_sort_cut = sp500_ret_to_aini_sig_sort.iloc[0:20]\n",
    "sp500_ret_to_aini_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca5d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_regression_table(\n",
    "    df=sp500_ret_to_aini_sig_sort_cut,\n",
    "    title=\"Granger-Causality, jointly significant results (Returns -> AINI, controlled for S&P 500), Top 20\",\n",
    "    output_filename=\"gc_sp500_ret_to_aini_sort_beta_cut\",\n",
    "    output_format=\"tex\",\n",
    "    latex_env=\"tabular\",           \n",
    "    include_caption_label=False,   \n",
    "    font_size_cmd=\"scriptsize\",\n",
    "    tabcolsep_pt=2.0\n",
    ")\n",
    "sp500_ret_to_aini_sig_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c428f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_regression_table(\n",
    "    df=sp500_ret_to_aini_sig_sort,\n",
    "    title=\"Granger-Causality, jointly significant results (Returns -> AINI, controlled for S&P 500), Top 20\",\n",
    "    output_filename=\"gc_sp500_ret_to_aini_sort_beta\",\n",
    "    output_format=\"tex\",\n",
    "    latex_env=\"tabular\",           \n",
    "    include_caption_label=False,   \n",
    "    font_size_cmd=\"scriptsize\",\n",
    "    tabcolsep_pt=2.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ee23a",
   "metadata": {},
   "source": [
    "Controlling for VIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699fd856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to variables\n",
    "var_path = root_dir / \"data\" / \"processed\" / \"variables\"\n",
    " \n",
    "# load data (S&P 500 control)\n",
    "gc_c = pd.read_csv(var_path / \"granger_causality_VIX_binary.csv\")\n",
    "gc_w0 = pd.read_csv(var_path / \"granger_causality_VIX_w0.csv\")\n",
    "gc_w1 = pd.read_csv(var_path / \"granger_causality_VIX_w1.csv\")\n",
    "gc_w2 = pd.read_csv(var_path / \"granger_causality_VIX_w2.csv\")\n",
    "\n",
    "\n",
    "# create column to indicate version\n",
    "gc_c[\"Model\"] = \"custom\"\n",
    "gc_w0[\"Model\"] = \"w0\"\n",
    "gc_w1[\"Model\"] = \"w1\"\n",
    "gc_w2[\"Model\"] = \"w2\"\n",
    "\n",
    "# merge them together\n",
    "gc_all_results_VIX = pd.concat([gc_c, gc_w0, gc_w1, gc_w2], ignore_index=True)\n",
    "gc_all_results_VIX[\"joint rej. (α=0.1)\"] = gc_all_results_VIX[\"BH_reject_F\"] & gc_all_results_VIX[\"BH_reject_F_HC3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure no dupls\n",
    "seen = {}\n",
    "new_cols = []\n",
    "\n",
    "for c in gc_all_results_VIX.columns:\n",
    "    if c not in seen:\n",
    "        seen[c] = 0\n",
    "        new_cols.append(c)\n",
    "    else:\n",
    "        seen[c] += 1\n",
    "        new_cols.append(f\"{c}.{seen[c]}\")\n",
    "\n",
    "gc_all_results_VIX.columns = new_cols\n",
    "\n",
    "\n",
    "# HTML output\n",
    "export_regression_table(\n",
    "    df=gc_all_results_VIX,\n",
    "    title=\"Granger-Causality all Results (controlled for VIX)\",\n",
    "    output_filename=\"gc_all_results_vix_cont\",\n",
    "    output_format=\"html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf03dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_all_results_VIX_for_report = gc_all_results_VIX.copy()\n",
    "\n",
    "# drop non-stationary measures, i.e. windows in 2025; EMA_{0.2} in 2025 for costum\n",
    "gc_all_results_VIX_for_report = gc_all_results_VIX_for_report[(gc_all_results_VIX_for_report[\"Model\"] == \"custom\") | (gc_all_results_VIX_for_report[\"Year\"] != \"2025\")]\n",
    "gc_all_results_VIX_for_report  = gc_all_results_VIX_for_report[~((gc_all_results_VIX_for_report[\"AINI_variant\"] == \"EMA_02\") & (gc_all_results_VIX_for_report[\"Year\"] == \"2025\"))]\n",
    "\n",
    "# split by direction\n",
    "vix_aini_to_ret = gc_all_results_VIX_for_report[gc_all_results_VIX_for_report[\"Direction\"] == \"AINI_to_RET\"]\n",
    "vix_ret_to_aini = gc_all_results_VIX_for_report[gc_all_results_VIX_for_report[\"Direction\"] == \"RET_to_AINI\"]\n",
    "\n",
    "# beautify\n",
    "rename_map = {\n",
    "    \"p_x\": \"Lags\",\n",
    "    \"BH_corr_F_pval\": \"BH empirical p\",\n",
    "    \"BH_corr_F_pval_HC3\": \"BH analytical p\",\n",
    "    \"Year\": \"Period\"\n",
    "}\n",
    "\n",
    "# drop NAs\n",
    "vix_ret_to_aini = vix_ret_to_aini.dropna(how=\"all\",axis=1) \n",
    "vix_aini_to_ret = vix_aini_to_ret.dropna(how=\"all\",axis=1) \n",
    "vix_ret_to_aini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d3c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = {\n",
    "    # bookkeeping\n",
    "    \"p_x\": \"Lags\",\n",
    "    \"Original_F_pval\": \"analytical P\",\n",
    "    \"Empirical_F_pval\": \"empirical P\",\n",
    "    \"BH_corr_F_pval\": \"BH empirical p\",\n",
    "    \"BH_corr_F_pval_HC3\": \"BH analytical p\",\n",
    "    \"Year\": \"Period\",\n",
    "\n",
    "    # lagged return betas (AINI → Returns)\n",
    "    \"A2R_beta_ret_1\": \"β1\",\n",
    "    \"A2R_beta_ret_2\": \"β2\",\n",
    "    \"A2R_beta_ret_3\": \"β3\",\n",
    "\n",
    "    # AINI lag coefficients\n",
    "    \"A2R_beta_x_1\": \"γ1\",\n",
    "    \"A2R_beta_x_2\": \"γ2\",\n",
    "    \"A2R_beta_x_3\": \"γ3\",\n",
    "\n",
    "    # VIX control coefficients (original + .1 versions)\n",
    "    \"β_ctrl_log_growth_closed1\": \"ζ1\",\n",
    "    \"β_ctrl_log_growth_closed2\": \"ζ2\",\n",
    "    \"β_ctrl_log_growth_closed3\": \"ζ3\",\n",
    "    \"β_ctrl_log_growth_closed1.1\": \"ζ1\",\n",
    "    \"β_ctrl_log_growth_closed2.1\": \"ζ2\",\n",
    "    \"β_ctrl_log_growth_closed3.1\": \"ζ3\",\n",
    "\n",
    "    # ---- Reverse direction (Returns → AINI) ----\n",
    "    \"R2A_beta_ret_1\": \"β1\",\n",
    "    \"R2A_beta_ret_2\": \"β2\",\n",
    "    \"R2A_beta_ret_3\": \"β3\",\n",
    "\n",
    "    \"R2A_beta_x_1\": \"γ1\",\n",
    "    \"R2A_beta_x_2\": \"γ2\",\n",
    "    \"R2A_beta_x_3\": \"γ3\",\n",
    "\n",
    "    \"R2A_BH_corr_F_pval\": \"BH empirical p\",\n",
    "    \"R2A_BH_corr_F_pval_HC3\": \"BH analytical p\",\n",
    "    \"R2A_Empirical_F_pval\": \"empirical P\",\n",
    "    \"R2A_Original_F_pval\": \"analytical P\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "vix_aini_to_ret = vix_aini_to_ret.rename(columns=rename_map)\n",
    "vix_ret_to_aini= vix_ret_to_aini.rename(columns=rename_map)\n",
    "\n",
    "# sanity check: no duplicate column names\n",
    "dup_mask = pd.Series(vix_aini_to_ret.columns).duplicated(keep=False)\n",
    "if dup_mask.any():\n",
    "    dups = pd.Series(vix_aini_to_ret.columns)[dup_mask].tolist()\n",
    "    raise ValueError(f\"Duplicate column names after rename: {dups}\")\n",
    "\n",
    "\n",
    "# Apply renaming\n",
    "gc_all_results_VIX_for_report= vix_aini_to_ret.rename(columns=rename_map)\n",
    "\n",
    "# inspect\n",
    "vix_aini_to_ret.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f70aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols to keep\n",
    "keep_a2r = [\n",
    "    \"Model\", \"AINI_variant\", \"Ticker\", \"Period\", \"Lags\",\n",
    "    # Coefficients\n",
    "    \"β1\", \"β2\", \"β3\",      # lagged returns\n",
    "    \"γ1\", \"γ2\", \"γ3\",      # AINI lags\n",
    "    \"ζ1\", \"ζ2\", \"ζ3\",      # VIX controls (\n",
    "    # Fit quality\n",
    "    \"r2_u\", \"adj_r2_u\",\n",
    "    # P-values and test results\n",
    "    \"analytical P\",\"empirical P\",\n",
    "    \"BH empirical p\", \"BH analytical p\", \"joint rej. (α=0.1)\",\n",
    "]\n",
    "\n",
    "\n",
    "# Columns to keep for Return → AINI \n",
    "keep_r2a = [\n",
    "    \"Model\", \"AINI_variant\", \"Ticker\", \"Period\", \"Lags\",\n",
    "    \"β1\", \"β2\", \"β3\",\n",
    "    \"γ1\", \"γ2\", \"γ3\",\n",
    "    \"ζ1\", \"ζ2\", \"ζ3\", \n",
    "    \"r2_u\", \"adj_r2_u\",\n",
    "    \"analytical P\",\"empirical P\",\n",
    "    \"BH empirical p\", \"BH analytical p\", \"joint rej. (α=0.1)\",\n",
    "]\n",
    "\n",
    "# Apply safely\n",
    "vix_aini_to_ret = vix_aini_to_ret[[c for c in keep_a2r if c in vix_aini_to_ret.columns]]\n",
    "vix_ret_to_aini = vix_ret_to_aini[[c for c in keep_r2a if c in vix_ret_to_aini.columns]]\n",
    "\n",
    "\n",
    "# subset by col\n",
    "vix_ret_to_aini_sub = vix_ret_to_aini[keep_r2a]\n",
    "vix_aini_to_ret_sub = vix_aini_to_ret[keep_a2r]\n",
    "\n",
    "# subset signficant\n",
    "vix_ret_to_aini_sub = vix_ret_to_aini_sub[vix_ret_to_aini_sub[\"joint rej. (α=0.1)\"] == True ]\n",
    "vix_aini_to_ret_sub = vix_aini_to_ret_sub[vix_aini_to_ret_sub[\"joint rej. (α=0.1)\"] == True ]\n",
    "\n",
    "# drop duplicated cols\n",
    "vix_ret_to_aini_sub = vix_ret_to_aini_sub.dropna(how=\"all\",axis=1)\n",
    "vix_aini_to_ret_sub = vix_aini_to_ret_sub.dropna(how=\"all\",axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc2f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vix_aini_to_ret_sub.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd79f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rejection rate \n",
    "\n",
    "# Make sure Year and Ticker are strings\n",
    "vix_aini_to_ret_sub[\"Period\"] = vix_aini_to_ret_sub[\"Period\"].astype(str)\n",
    "vix_aini_to_ret_sub[\"Ticker\"] = vix_aini_to_ret_sub[\"Ticker\"].astype(str)\n",
    "\n",
    "# Total number of models tested\n",
    "total = vix_ret_to_aini[\"joint rej. (α=0.1)\"].count()\n",
    "\n",
    "# Number of rejections (both bootstrap + HC3 significant)\n",
    "n_reject = vix_ret_to_aini[\"joint rej. (α=0.1)\"].sum()\n",
    "\n",
    "# Rejection rate\n",
    "rejection_rate = n_reject / total * 100\n",
    "\n",
    "print(f\"Total models: {total}\")\n",
    "print(f\"Both-method rejections: {n_reject}\")\n",
    "print(f\"Rejection rate: {rejection_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db3f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset\n",
    "VIX_aini_to_ret_sig = vix_aini_to_ret_sub[vix_aini_to_ret_sub[\"joint rej. (α=0.1)\"] == True ]\n",
    "\n",
    "# beautify periods\n",
    "p_rename_map = {\n",
    "    \"2023_24\" : \"2023-2024\",\n",
    "    \"2023_24_25\" : \"2023-2025\",\n",
    "    \"2024_25\" : \"2024-2025\"\n",
    "}\n",
    "\n",
    "VIX_aini_to_ret_sig[\"Period\"] = VIX_aini_to_ret_sig[\"Period\"].replace(p_rename_map)\n",
    "VIX_aini_to_ret_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a980c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ivnestigate difference to VIX\n",
    "\n",
    "# Define the key columns\n",
    "keys = [\"Period\", \"AINI_variant\", \"Ticker\",\"Lags\",\"Model\"]\n",
    "\n",
    "# Defensive copies (optional but clean)\n",
    "A = VIX_aini_to_ret_sig[keys].drop_duplicates().copy()\n",
    "B = sp500_aini_to_ret_sig[keys].drop_duplicates().copy()\n",
    "\n",
    "# Intersection\n",
    "intersection = pd.merge(A, B, on=keys, how=\"inner\")\n",
    "\n",
    "# Unique to VIX (A)\n",
    "unique_vix = pd.merge(A, B, on=keys, how=\"left\", indicator=True).query('_merge == \"left_only\"').drop(columns=\"_merge\")\n",
    "\n",
    "# Unique to SP500 (B)\n",
    "unique_sp500 = pd.merge(A, B, on=keys, how=\"right\", indicator=True).query('_merge == \"right_only\"').drop(columns=\"_merge\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"Intersection: {len(intersection)}\")\n",
    "print(f\"Unique to VIX: {len(unique_vix)}\")\n",
    "print(f\"Unique to SP500: {len(unique_sp500)}\")\n",
    "\n",
    "# total sp500  104; total VIX \n",
    "\n",
    "# (Optional) show proportions for context\n",
    "print(f\"Overlap rate (of VIX): {len(intersection)/len(A):.2%}\")\n",
    "VIX_aini_to_ret_sig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120c580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725313fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ivnestigate difference to RET-AINI\n",
    "\n",
    "# Define the key columns\n",
    "keys = [\"Period\", \"Ticker\"]\n",
    "\n",
    "# copies \n",
    "A = VIX_aini_to_ret_sig[keys].drop_duplicates().copy()\n",
    "B = vix_ret_to_aini_sub[keys].drop_duplicates().copy()\n",
    "\n",
    "# Intersection\n",
    "intersection = pd.merge(A, B, on=keys, how=\"inner\")\n",
    "\n",
    "# Unique to VIX (A)\n",
    "unique_vix = pd.merge(A, B, on=keys, how=\"left\", indicator=True).query('_merge == \"left_only\"').drop(columns=\"_merge\")\n",
    "\n",
    "# Unique to SP500 (B)\n",
    "unique_ret = pd.merge(A, B, on=keys, how=\"right\", indicator=True).query('_merge == \"right_only\"').drop(columns=\"_merge\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"Intersection: {len(intersection)}\")\n",
    "print(f\"Unique to Ret->AINI: {len(unique_vix)}\")\n",
    "print(f\"Unique to AINI->AINI: {len(unique_ret)}\")\n",
    "\n",
    "# show proportions for context\n",
    "print(f\"Overlap rate (of VIX): {len(intersection)/len(VIX_aini_to_ret_sig):.2%}\")\n",
    "VIX_aini_to_ret_sig.columns\n",
    "intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03c09f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "out_path = root_dir / \"reports\" / \"figures\" / \"distribution_of_gammas.png\"\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 100,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"font.size\": 10,\n",
    "    \"axes.titlesize\": 11,\n",
    "    \"axes.labelsize\": 10,\n",
    "    \"xtick.labelsize\": 9,\n",
    "    \"ytick.labelsize\": 9,\n",
    "    \"legend.fontsize\": 9,\n",
    "})\n",
    "\n",
    "SUBSCRIPT_DIGITS = str.maketrans(\"₀₁₂₃₄₅₆₇₈₉\", \"0123456789\")\n",
    "\n",
    "def norm_g(c):\n",
    "    s = str(c).translate(SUBSCRIPT_DIGITS).replace(\"'\", \"\").strip().lower()\n",
    "    if s.startswith(\"gamma\") or s.startswith(\"g\"):\n",
    "        s = \"γ\" + re.sub(r\"^[a-z]+\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def coerce_numeric_df(df):\n",
    "    cleaned = df.replace(r\"[^0-9eE\\.\\+\\-]\", \"\", regex=True)\n",
    "    return cleaned.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "wanted = {\"γ1\", \"γ2\", \"γ3\"}\n",
    "groups = {\"γ1\": [], \"γ2\": [], \"γ3\": []}\n",
    "for c in VIX_aini_to_ret_sig.columns:\n",
    "    key = norm_g(c)\n",
    "    if key in wanted:\n",
    "        groups[key].append(c)\n",
    "\n",
    "collapsed = pd.DataFrame(index=VIX_aini_to_ret_sig.index)\n",
    "for key, cols in groups.items():\n",
    "    collapsed[key] = coerce_numeric_df(VIX_aini_to_ret_sig[cols]).mean(axis=1) if cols else np.nan\n",
    "\n",
    "all_gamma_vals = collapsed[[\"γ1\",\"γ2\",\"γ3\"]].to_numpy(dtype=float).ravel()\n",
    "all_gamma_vals = all_gamma_vals[~np.isnan(all_gamma_vals)]\n",
    "lim = float(max(abs(all_gamma_vals.min()), abs(all_gamma_vals.max()))) if all_gamma_vals.size else 1.0\n",
    "xlim = (-lim, lim)\n",
    "\n",
    "models = [\"w0\", \"w1\", \"w2\", \"custom\"]\n",
    "bins, ymax = 40, 27\n",
    "fig, axes = plt.subplots(2, 2, figsize=(7.2, 5.6), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "legend_handles, legend_labels = None, None\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    ax = axes[i]\n",
    "    subset = collapsed.loc[VIX_aini_to_ret_sig[\"Model\"] == model, [\"γ1\",\"γ2\",\"γ3\"]].dropna(how=\"all\", axis=1)\n",
    "\n",
    "    if subset.empty:\n",
    "        ax.set_title(f\"Model: {model} (no data)\")\n",
    "        ax.set_xlim(xlim); ax.set_ylim(0, ymax)\n",
    "        ax.grid(alpha=0.25, linestyle=\":\", linewidth=0.8)\n",
    "        continue\n",
    "\n",
    "    plot_ax = subset.plot.hist(\n",
    "        bins=bins, range=xlim, alpha=0.6, ax=ax,\n",
    "        edgecolor=\"black\", legend=True, grid=False,\n",
    "    )\n",
    "\n",
    "    if legend_handles is None:\n",
    "        legend_handles, legend_labels = plot_ax.get_legend_handles_labels()\n",
    "\n",
    "    ax.set_title(f\"Model: {model}\")\n",
    "    ax.set_xlim(xlim); ax.set_ylim(0, ymax)\n",
    "    ax.set_ylabel(\"Counts\")\n",
    "    ax.grid(alpha=0.25, linestyle=\":\", linewidth=0.8)\n",
    "\n",
    "    means = subset.mean().values\n",
    "    labels = [fr\"$\\bar{{\\gamma}}_{{{j}}}$={m:.3f}\" for j, m in enumerate(means, start=1)]\n",
    "    ax.text(\n",
    "        0.98, 0.97, \", \".join(labels),\n",
    "        transform=ax.transAxes, ha=\"right\", va=\"top\", fontsize=8.5,\n",
    "        bbox=dict(facecolor=\"white\", edgecolor=\"none\", alpha=0.7, boxstyle=\"round,pad=0.2\")\n",
    "    )\n",
    "\n",
    "    leg = ax.get_legend()\n",
    "    if leg is not None:\n",
    "        leg.remove()\n",
    "\n",
    "fig.text(0.5, 0.02, \"\", ha=\"center\", fontsize=10)\n",
    "\n",
    "if legend_handles:\n",
    "    fig.legend(\n",
    "        legend_handles, legend_labels, title=\"Coefficient\",\n",
    "        loc=\"lower center\", ncol=3, frameon=False\n",
    "    )\n",
    "\n",
    "plt.tight_layout(rect=[0.04, 0.07, 1, 0.98])\n",
    "fig.savefig(out_path, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "plt.close(fig)\n",
    "print(f\"Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f528993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by model\n",
    "model_group_tickers = (\n",
    "    VIX_aini_to_ret_sig\n",
    "    .groupby([\"Ticker\",\"Period\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"jointly rejected at α=0.1\")\n",
    "    .sort_values(by=\"jointly rejected at α=0.1\",ascending=False)\n",
    ")\n",
    "\n",
    "\n",
    "# print\n",
    "print(model_group_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef319da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by ticker\n",
    "model_group_tickers = (\n",
    "    VIX_aini_to_ret_sig\n",
    "    .groupby([\"Ticker\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"jointly rejected at α=0.1\")\n",
    "    .sort_values(by=\"jointly rejected at α=0.1\",ascending=False)\n",
    ")\n",
    "\n",
    "\n",
    "# print\n",
    "print(model_group_tickers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f2852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by model\n",
    "model_group_tickers = (\n",
    "    sp500_aini_to_ret_sig\n",
    "    .groupby([\"Model\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"jointly rejected at α=0.1\")\n",
    "    .sort_values(by=\"jointly rejected at α=0.1\",ascending=False)\n",
    ")\n",
    "\n",
    "\n",
    "# print\n",
    "print(model_group_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41099ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe7ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from  dataframe\n",
    "df = VIX_aini_to_ret_sig.copy()\n",
    "\n",
    "# Isolate the duplicate γ1 columns and drop those that are all zeros ---\n",
    "gamma1_block = df.loc[:, df.columns == \"γ1\"]  # this returns a DataFrame if there are duplicates\n",
    "gamma1_keep = gamma1_block.loc[:, (gamma1_block != 0).any(axis=0)]\n",
    "\n",
    "# keep the first non-zero per row\n",
    "if gamma1_keep.shape[1] == 0:\n",
    "    raise ValueError(\"No non-zero γ1 column found.\")\n",
    "elif gamma1_keep.shape[1] == 1:\n",
    "    gamma1 = gamma1_keep.iloc[:, 0]\n",
    "else:\n",
    "    # Row-wise pick the first non-zero; fallback to the first column if all zeros (shouldn't happen after filtering)\n",
    "    gamma1 = gamma1_keep.apply(lambda r: next((v for v in r if v != 0), r.iloc[0]), axis=1)\n",
    "\n",
    "# Build a tidy table: one γ1 per (Period, Ticker, Lags, Model) ---\n",
    "needed = df[[\"Period\", \"Ticker\", \"Lags\", \"Model\"]].reset_index(drop=True)\n",
    "gamma1 = gamma1.reset_index(drop=True).rename(\"γ1\")\n",
    "tidy = pd.concat([needed, gamma1], axis=1)\n",
    "\n",
    "# Pivot: rows = (Period, Ticker, Lags), columns = Model, values = γ1 \n",
    "gamma1_matrix = (\n",
    "    tidy.pivot_table(\n",
    "        index=[\"Ticker\",\"Period\",  \"Lags\"],\n",
    "        columns=\"Model\",\n",
    "        values=\"γ1\",\n",
    "        aggfunc=\"first\"\n",
    "    )\n",
    "    .sort_index()\n",
    "    .sort_index(axis=1)\n",
    ")\n",
    "\n",
    "print(gamma1_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9544f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jacquard between models\n",
    "\n",
    "# prepare df for manipulation\n",
    "sig = VIX_aini_to_ret_sig.copy()\n",
    "\n",
    "# Create a unique identifier for comparison\n",
    "key_cols = [\"AINI_variant\", \"Ticker\", \"Period\"]\n",
    "sig[\"combo\"] = sig[key_cols].astype(str).agg(\"_\".join, axis=1)\n",
    "\n",
    "# 4) Build a set of combos per model\n",
    "model_sets = {m: set(g[\"combo\"].unique()) for m, g in sig.groupby(\"Model\")}\n",
    "\n",
    "# In case there are models with zero significant rows, include them with empty sets:\n",
    "all_models = sorted(df[\"Model\"].astype(str).unique())\n",
    "for m in all_models:\n",
    "    model_sets.setdefault(m, set())\n",
    "\n",
    "# 5) Construct the n×n Jaccard matrix with diagonal = 1.0\n",
    "mat = pd.DataFrame(index=all_models, columns=all_models, dtype=float)\n",
    "\n",
    "for i in all_models:\n",
    "    Si = model_sets[i]\n",
    "    for j in all_models:\n",
    "        if i == j:\n",
    "            mat.loc[i, j] = 1.0\n",
    "        else:\n",
    "            Sj = model_sets[j]\n",
    "            union = len(Si | Sj)\n",
    "            inter = len(Si & Sj)\n",
    "            mat.loc[i, j] = (inter / union) if union > 0 else float(\"nan\")\n",
    "\n",
    "# Optional: round for readability\n",
    "jacard_matrix = mat.round(3)\n",
    "\n",
    "print(jacard_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc0d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all-NaN columns\n",
    "VIX_aini_to_ret_sig = VIX_aini_to_ret_sig.dropna(axis=1, how=\"all\")\n",
    "\n",
    "# Collapse duplicate \"γ1\" columns into one Series\n",
    "g1_block = VIX_aini_to_ret_sig.loc[:, VIX_aini_to_ret_sig.columns == \"γ1\"]  # may be multiple cols\n",
    "γ1_series = (\n",
    "    g1_block.bfill(axis=1).iloc[:, 0]    # first non-null across dup γ1 cols\n",
    "    .pipe(pd.to_numeric, errors=\"coerce\") # ensure numeric\n",
    ")\n",
    "\n",
    "# Add clean γ1, compute abs, sort, slice\n",
    "VIX_aini_to_ret_sig_sorted = (\n",
    "    VIX_aini_to_ret_sig\n",
    "    .assign(γ1_clean=γ1_series)\n",
    "    .assign(abs_γ1=lambda d: d[\"γ1_clean\"].abs())\n",
    "    .sort_values(\"abs_γ1\", ascending=False)\n",
    ")\n",
    "\n",
    "VIX_aini_to_ret_sig_sort_cut = VIX_aini_to_ret_sig_sorted.iloc[:20].drop(columns=[\"abs_γ1\"])\n",
    "\n",
    "# Export LaTeX\n",
    "export_regression_table(\n",
    "    df=VIX_aini_to_ret_sig_sort_cut,\n",
    "    title=\"Granger-Causality, jointly significant results (AINI $\\\\to$ Returns, controlled for VIX).Top 20 Sorted by $\\gamma_1$. \\\\textit{Source:} Own.\",\n",
    "    output_filename=\"gc_vix_aini_to_ret_sort_Gamma_cut\",\n",
    "    output_format=\"tex\",\n",
    "    latex_env=\"tabular\",\n",
    "    include_caption_label=True,\n",
    "    coef_digits=3,\n",
    "    p_digits=3,\n",
    "    tabcolsep_pt=2.0,\n",
    "    font_size_cmd=\"scriptsize\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55e867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate sign fllips\n",
    "def gamma_sign_switch_summary(df: pd.DataFrame) -> dict:\n",
    "    g1 = pd.to_numeric(df['γ1'], errors='coerce')\n",
    "    g2 = pd.to_numeric(df['γ2'], errors='coerce')\n",
    "    g3 = pd.to_numeric(df['γ3'], errors='coerce')\n",
    "\n",
    "    total_non_nan_12 = df[['γ1', 'γ2']].dropna().shape[0]\n",
    "    m12 = g1.notna() & g2.notna() & (g1 != 0) & (g2 != 0)\n",
    "    compared12 = int(m12.sum())\n",
    "    switches12 = int((g1[m12] * g2[m12] < 0).sum())\n",
    "    switch_share12 = switches12 / compared12 if compared12 else np.nan\n",
    "    compared_share12 = compared12 / total_non_nan_12 if total_non_nan_12 else np.nan\n",
    "\n",
    "    total_non_nan_23 = df[['γ2', 'γ3']].dropna().shape[0]\n",
    "    m23 = g2.notna() & g3.notna() & (g2 != 0) & (g3 != 0)\n",
    "    compared23 = int(m23.sum())\n",
    "    switches23 = int((g2[m23] * g3[m23] < 0).sum())\n",
    "    switch_share23 = switches23 / compared23 if compared23 else np.nan\n",
    "    compared_share23 = compared23 / total_non_nan_23 if total_non_nan_23 else np.nan\n",
    "\n",
    "    return {\n",
    "        'γ1→γ2_switches': switches12,\n",
    "        'γ1→γ2_compared': compared12,\n",
    "        'γ1→γ2_total_non_nan': total_non_nan_12,\n",
    "        'γ1→γ2_switch_share': switch_share12,\n",
    "        'γ1→γ2_compared_share': compared_share12,\n",
    "        'γ2→γ3_switches': switches23,\n",
    "        'γ2→γ3_compared': compared23,\n",
    "        'γ2→γ3_total_non_nan': total_non_nan_23,\n",
    "        'γ2→γ3_switch_share': switch_share23,\n",
    "        'γ2→γ3_compared_share': compared_share23,\n",
    "    }\n",
    "\n",
    "# Example:\n",
    "result = gamma_sign_switch_summary(VIX_aini_to_ret_sig_sorted)\n",
    "print(result)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7fd8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIX_aini_to_ret_sig_r2 = VIX_aini_to_ret_sig_sorted.copy()\n",
    "\n",
    "mask = VIX_aini_to_ret_sig_r2['BH empirical p'] < VIX_aini_to_ret_sig_r2['BH analytical p']\n",
    "df_higher_empirical = VIX_aini_to_ret_sig_r2[mask]\n",
    "\n",
    "print(f\"Cases where BH empirical p < BH analytical p: {mask.sum()}\")\n",
    "df_higher_empirical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b09f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by R^2\n",
    "VIX_aini_to_ret_sig_r2 = (\n",
    "    VIX_aini_to_ret_sig\n",
    "    .assign(γ1_clean=γ1_series)\n",
    "    .sort_values(\"adj_r2_u\", ascending=False)\n",
    ")\n",
    "\n",
    "VIX_aini_to_ret_sig_r2[[\"Model\",\"AINI_variant\",\"Ticker\",\"Period\",\"adj_r2_u\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8540abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export LaTeX\n",
    "export_regression_table(\n",
    "    df=VIX_aini_to_ret_sig_sorted,\n",
    "    title=\"Granger-Causality, jointly significant results (AINI $\\\\to$ Returns, controlled for VIX).Sorted by absolute magnitude of $\\gamma_1$. Source: Own.\",\n",
    "    output_filename=\"gc_vix_aini_to_ret_sort_Gamma\",\n",
    "    output_format=\"tex\",\n",
    "    latex_env=\"tabular\",\n",
    "    include_caption_label=True,\n",
    "    coef_digits=3,\n",
    "    p_digits=3,\n",
    "    tabcolsep_pt=2.0,\n",
    "    font_size_cmd=\"scriptsize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e4336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vix_ret_to_aini_sub\n",
    "vix_ret_to_aini_sub = vix_ret_to_aini_sub.sort_values(\"adj_r2_u\",ascending=False)\n",
    "vix_ret_to_aini_sub_cut = vix_ret_to_aini_sub.iloc[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc2e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate sign flips for beta\n",
    "def beta_sign_switch_summary(df: pd.DataFrame) -> dict:\n",
    "    b1 = pd.to_numeric(df['β1'], errors='coerce')\n",
    "    b2 = pd.to_numeric(df['β2'], errors='coerce')\n",
    "    b3 = pd.to_numeric(df['β3'], errors='coerce')\n",
    "\n",
    "    total_non_nan_12 = df[['β1', 'β2']].dropna().shape[0]\n",
    "    m12 = b1.notna() & b2.notna() & (b1 != 0) & (b2 != 0)\n",
    "    compared12 = int(m12.sum())\n",
    "    switches12 = int((b1[m12] * b2[m12] < 0).sum())\n",
    "    switch_share12 = switches12 / compared12 if compared12 else np.nan\n",
    "    compared_share12 = compared12 / total_non_nan_12 if total_non_nan_12 else np.nan\n",
    "\n",
    "    total_non_nan_23 = df[['β2', 'β3']].dropna().shape[0]\n",
    "    m23 = b2.notna() & b3.notna() & (b2 != 0) & (b3 != 0)\n",
    "    compared23 = int(m23.sum())\n",
    "    switches23 = int((b2[m23] * b3[m23] < 0).sum())\n",
    "    switch_share23 = switches23 / compared23 if compared23 else np.nan\n",
    "    compared_share23 = compared23 / total_non_nan_23 if total_non_nan_23 else np.nan\n",
    "\n",
    "    return {\n",
    "        'β1→β2_switches': switches12,\n",
    "        'β1→β2_compared': compared12,\n",
    "        'β1→β2_total_non_nan': total_non_nan_12,\n",
    "        'β1→β2_switch_share': switch_share12,\n",
    "        'β1→β2_compared_share': compared_share12,\n",
    "        'β2→β3_switches': switches23,\n",
    "        'β2→β3_compared': compared23,\n",
    "        'β2→β3_total_non_nan': total_non_nan_23,\n",
    "        'β2→β3_switch_share': switch_share23,\n",
    "        'β2→β3_compared_share': compared_share23,\n",
    "    }\n",
    "\n",
    "# Example:\n",
    "result_beta = beta_sign_switch_summary(vix_ret_to_aini_sub)\n",
    "print(result_beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e85839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate by r^2\n",
    "vix_ret_to_aini_sub[[\"Model\",\"AINI_variant\",\"Ticker\",\"Period\",\"adj_r2_u\"]].sort_values(\"adj_r2_u\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b8fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export LaTeX reverse direction\n",
    "export_regression_table(\n",
    "    df=vix_ret_to_aini_sub,\n",
    "    title=\"Granger-Causality, jointly significant results (Returns $\\\\to$ AINI, controlled for VIX).Sorted by absolute magnitude of $\\beta_1$. Source: Own.\",\n",
    "    output_filename=\"gc_vix_return_to_aini\",\n",
    "    output_format=\"tex\",\n",
    "    latex_env=\"tabular\",\n",
    "    include_caption_label=True,\n",
    "    reverse=True,\n",
    "    coef_digits=3,\n",
    "    p_digits=3,\n",
    "    tabcolsep_pt=2.0,\n",
    "    font_size_cmd=\"scriptsize\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c59e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export LaTeX reverse direction short\n",
    "export_regression_table(\n",
    "    df=vix_ret_to_aini_sub_cut,\n",
    "    title=\"Granger-Causality, jointly significant results (Returns $\\\\to$ AINI, controlled for VIX).Sorted by absolute magnitude of $\\beta_1$. Source: Own.\",\n",
    "    output_filename=\"gc_vix_return_to_aini_cut\",\n",
    "    output_format=\"tex\",\n",
    "    latex_env=\"tabular\",\n",
    "    include_caption_label=True,\n",
    "    reverse=True,\n",
    "    coef_digits=3,\n",
    "    p_digits=3,\n",
    "    tabcolsep_pt=2.0,\n",
    "    font_size_cmd=\"scriptsize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee0262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export LaTeX\n",
    "export_regression_table(\n",
    "    df=VIX_aini_to_ret_sig_sorted,\n",
    "    title=\"Granger-Causality, jointly significant results (AINI $\\\\to$ Returns, controlled for VIX).Sorted by absolute magnitude of $\\gamma_1$. Source: Own.\",\n",
    "    output_filename=\"gc_vix_aini_to_ret_sort_Gamma\",\n",
    "    output_format=\"tex\",\n",
    "    latex_env=\"tabular\",\n",
    "    include_caption_label=False,\n",
    "    coef_digits=3,\n",
    "    p_digits=3,\n",
    "    tabcolsep_pt=2.0,\n",
    "    font_size_cmd=\"scriptsize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a53ee0",
   "metadata": {},
   "source": [
    "Investigate assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beautify\n",
    "vix_aini_to_ret_sub[\"Period\"] = vix_aini_to_ret_sub[\"Period\"].replace({\"2023_24\": \"2023-2024\"})\n",
    "vix_aini_to_ret_sub[\"Period\"] = vix_aini_to_ret_sub[\"Period\"].replace({\"2024_25\": \"2024-2025\"})\n",
    "vix_aini_to_ret_sub[\"Period\"] = vix_aini_to_ret_sub[\"Period\"].replace({\"2023_24_25\": \"2023-2025\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cddb7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save merged results\n",
    "gc_all_results_VIX.to_csv(var_path / \"granger_causality_VIX.csv\", index=False)\n",
    "\n",
    "# define table path\n",
    "table_path = root_dir / \"reports\" / \"tables\"\n",
    "\n",
    "# Export as HTML for online appendix\n",
    "gc_all_results.to_html(table_path / \"granger_causality_VIX.html\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d249f01",
   "metadata": {},
   "source": [
    "Controlled for VIX as collider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f458ac",
   "metadata": {},
   "source": [
    "Investigate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218c7586",
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = [\"w0\", \"w1\", \"w2\", \"binary\"]\n",
    "dfs = {}\n",
    "\n",
    "for v in variants:\n",
    "    dfs[v] = pd.read_csv(var_path / f\"vix_causality_{v}_log_growth_closed.csv\")\n",
    "    dfs[v][\"variant\"] = v\n",
    "\n",
    "dfs[\"w0\"][\"BH_corr_F_pval_HC3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5a998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "variants = [\"w0\", \"w1\", \"w2\", \"binary\"]\n",
    "dfs = {}\n",
    "\n",
    "for v in variants:\n",
    "    dfs[v] = pd.read_csv(var_path / f\"vix_causality_{v}_log_growth_closed.csv\")\n",
    "    dfs[v][\"Model\"] = v if v != \"binary\" else \"c\"\n",
    "    dfs[v][\"joint rej. (α=0.1)\"] = (dfs[v][\"BH_corr_F_pval\"] < 0.1) & (dfs[v][\"BH_corr_F_pval_HC3\"] < 0.1)\n",
    "\n",
    "# find rejections\n",
    "rejects = {}\n",
    "for v in variants:\n",
    "    df = dfs[v]\n",
    "    rej = df[(df[\"BH_corr_F_pval\"] < 0.1) & (df[\"BH_corr_F_pval_HC3\"] < 0.1)]          \n",
    "    rejects[v] = rej\n",
    "    print(rej.head())\n",
    "\n",
    "# unpack\n",
    "all_w0 , all_w1, all_w2, all_custom = dfs.values()\n",
    "rej_w0 , rej_w1, rej_w2, rej_custom = rejects.values()\n",
    "\n",
    "all_w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdcd19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find AINI -> VIX\n",
    "aini_rejects = dict()\n",
    "\n",
    "for v in variants:\n",
    "    df = dfs[v]\n",
    "    rej_aini = df[(df[\"BH_corr_F_pval\"] < 0.1) & (df[\"BH_corr_F_pval_HC3\"] < 0.1) & (df[\"Direction\"] == \"AINI_to_VIX\")]          \n",
    "    aini_rejects[v] = rej_aini\n",
    "    print(rej_aini.head())\n",
    "\n",
    "all_w0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee921fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beautify to report\n",
    "rename_map = {\n",
    "    \"p_x\": \"Lags\",\n",
    "    \"BH_corr_F_pval\": \"BH empirical p\",\n",
    "    \"BH_corr_F_pval_HC3\": \"BH analytical p\",\n",
    "    \"Year\": \"Period\",\n",
    "    \"RET_1\" : \"β1\", \n",
    "    \"RET_2\" : \"β2\",\n",
    "    \"RET_3\" : \"β3\",\n",
    "    \"AINI_1\" : \"γ1\",\n",
    "    \"AINI_2\" : \"γ2\",\n",
    "    \"AINI_3\" : \"γ3\",\n",
    "    \"VIX_ar_1\" : \"ζ1\",\n",
    "    \"VIX_ar_2\" : \"ζ2\",\n",
    "    \"VIX_ar_3\" : \"ζ3\"\n",
    "}\n",
    "\n",
    "aini_map = {\n",
    "    \"EMA_02\" : \"EMA^{0.2}\",\n",
    "    \"EMA_08\" : \"EMA^{0.8}\",\n",
    "    \"normalized_AINI\" : \"AINI^{norm}\"\n",
    "}\n",
    "\n",
    "\n",
    "# rename for reporting\n",
    "for name, df in dfs.items():\n",
    "    # rename columns\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # ensure Period is string for robust comparisons\n",
    "    df[\"Period\"] = df[\"Period\"].astype(str)\n",
    "\n",
    "    # pretty-print values for the AINI variant (this changes the actual values)\n",
    "    df[\"AINI_variant\"] = df[\"AINI_variant\"].replace(aini_map)\n",
    "\n",
    "    # build LaTeX-ready index label\n",
    "    df[\"Measure\"] = \"$\" + df[\"AINI_variant\"] + \"_{\" + df[\"Model\"] + \"}$\"\n",
    "\n",
    "    dfs[name] = df\n",
    "\n",
    "\n",
    "# filter non-stationarity measures, i.e. windows in 2025; EMA_{0.2} in 2025 for costum\n",
    "for name, df in dfs.items():\n",
    "\n",
    "    # keep only 'custom' for 2025, others exclude 2025\n",
    "    df = df[(df[\"Model\"] == \"custom\") | (df[\"Period\"] != \"2025\")]\n",
    "    \n",
    "    # exclude EMA_02 in 2025\n",
    "    df = df[~((df[\"AINI_variant\"] == \"EMA_02\") & (df[\"Period\"] == \"2025\"))]\n",
    "    \n",
    "    # reassign back\n",
    "    dfs[name] = df  \n",
    "\n",
    "# group by direction; exrtact dfs with same direction\n",
    "direction_dfs = {}\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    for direction, subdf in df.groupby(\"Direction\"):\n",
    "        direction_dfs[f\"{name}_{direction}\"] = subdf\n",
    "\n",
    "# concat with same direction\n",
    "direction_groups = {}\n",
    "\n",
    "for key, df in direction_dfs.items():\n",
    "    direction = df[\"Direction\"].iloc[0]  # same for all rows in that df\n",
    "\n",
    "    if direction not in direction_groups:\n",
    "        direction_groups[direction] = []\n",
    "    direction_groups[direction].append(df.assign(Model=key.split(\"_\")[0]))\n",
    "\n",
    "# Concatenate all per direction\n",
    "combined_by_direction = {\n",
    "    direction: pd.concat(dfs, ignore_index=True)\n",
    "    for direction, dfs in direction_groups.items()\n",
    "}\n",
    "\n",
    "AINI_to_VIX, Return_to_VIX = combined_by_direction.values()\n",
    "AINI_to_VIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data to test for covariance\n",
    "log_vix = pd.read_csv(var_path / \"log_growth_VIX.csv\")\n",
    "aini_custom = pd.read_csv(var_path / \"binary_AINI_variables.csv\")\n",
    "aini_w0 = pd.read_csv(var_path / \"w0_AINI_variables.csv\")\n",
    "aini_w1 = pd.read_csv(var_path / \"w1_AINI_variables.csv\")\n",
    "aini_w2 =  pd.read_csv(var_path / \"w2_AINI_variables.csv\")\n",
    "\n",
    "merged, tidy, pivot, extrema = compute_aini_extrema(aini_w0,aini_w1,aini_w2,aini_custom,\n",
    "                                                    )\n",
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c0e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Align and merge data \n",
    "aini_measures = [\n",
    "    'EMA_08_w0', 'normalized_AINI_w1', 'EMA_02_w1',\n",
    "    'EMA_08_w1', 'normalized_AINI_w2', 'EMA_02_w2',\n",
    "    'EMA_08_w2', 'normalized_AINI_custom', \n",
    "    'EMA_02_custom', 'EMA_08_custom'\n",
    "]\n",
    "\n",
    "\n",
    "# Ensure datetime\n",
    "log_vix['date'] = pd.to_datetime(log_vix['date'])\n",
    "merged['date'] = pd.to_datetime(merged['date'])\n",
    "\n",
    "# Merge both dataframes by date\n",
    "df = pd.merge(\n",
    "    log_vix[['date', 'log_growth_closed']],\n",
    "    merged[['date'] + aini_measures],\n",
    "    on='date',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Add year column\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "periods = {\n",
    "    '2023': (2023, 2023),\n",
    "    '2024': (2024, 2024),\n",
    "    '2025': (2025, 2025),\n",
    "    '2023–2024': (2023, 2024),\n",
    "    '2024–2025': (2024, 2025),\n",
    "    '2023-2025': (2023, 2025)\n",
    "}\n",
    "\n",
    "# Compute correlations\n",
    "results = []\n",
    "for label, (start, end) in periods.items():\n",
    "    subset = df[(df['year'] >= start) & (df['year'] <= end)]\n",
    "    for var in aini_measures:\n",
    "        valid = subset[['log_growth_closed', var]].dropna()\n",
    "        corr = pearsonr(valid['log_growth_closed'], valid[var])[0] if len(valid) > 1 else None\n",
    "        results.append({'period': label, 'measure': var, 'corr': corr})\n",
    "\n",
    "corr_df = pd.DataFrame(results).pivot(index='measure', columns='period', values='corr')\n",
    "corr_df = corr_df.round(3).sort_index()\n",
    "\n",
    "#  Display \n",
    "print(corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61916dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "aini_measures = [\n",
    "    'EMA_08_w0', 'normalized_AINI_w1', 'EMA_02_w1',\n",
    "    'EMA_08_w1', 'normalized_AINI_w2', 'EMA_02_w2',\n",
    "    'EMA_08_w2', 'normalized_AINI_custom', \n",
    "    'EMA_02_custom', 'EMA_08_custom'\n",
    "]\n",
    "\n",
    "# Ensure datetime\n",
    "log_vix['date'] = pd.to_datetime(log_vix['date'])\n",
    "merged['date'] = pd.to_datetime(merged['date'])\n",
    "\n",
    "# Merge on date\n",
    "df = pd.merge(\n",
    "    log_vix[['date', 'log_growth_closed']],\n",
    "    merged[['date'] + aini_measures],\n",
    "    on='date',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Add year column\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Define periods\n",
    "periods = {\n",
    "    '2023': (2023, 2023),\n",
    "    '2024': (2024, 2024),\n",
    "    '2025': (2025, 2025),\n",
    "    '2023–2024': (2023, 2024),\n",
    "    '2024–2025': (2024, 2025),\n",
    "    '2023–2025': (2023, 2025)\n",
    "}\n",
    "\n",
    "# --- Compute covariances ---\n",
    "results = []\n",
    "for label, (start, end) in periods.items():\n",
    "    subset = df[(df['year'] >= start) & (df['year'] <= end)]\n",
    "    for var in aini_measures:\n",
    "        valid = subset[['log_growth_closed', var]].dropna()\n",
    "        cov = valid['log_growth_closed'].cov(valid[var]) if len(valid) > 1 else None\n",
    "        results.append({'period': label, 'measure': var, 'cov': cov})\n",
    "\n",
    "# --- Build covariance table ---\n",
    "cov_df = pd.DataFrame(results).pivot(index='measure', columns='period', values='cov')\n",
    "cov_df = cov_df.round(6).sort_index()\n",
    "\n",
    "# --- Display ---\n",
    "print(cov_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_stats_by_period(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    #  Ensure proper dtypes & ordering\n",
    "    df = df.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'], utc=False)\n",
    "    df = df.sort_values(['Ticker', 'Date'])\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "\n",
    "    # Define periods\n",
    "    periods = {\n",
    "        '2023': [2023],\n",
    "        '2024': [2024],\n",
    "        '2025': [2025],\n",
    "        '2023–2024': [2023, 2024],\n",
    "        '2023–2025': [2023, 2024, 2025],\n",
    "        '2024–2025': [2024, 2025],\n",
    "    }\n",
    "\n",
    "    out = []\n",
    "    for label, years in periods.items():\n",
    "        sub = df[df['Year'].isin(years)].copy()\n",
    "\n",
    "        # Compute log differences per Ticker\n",
    "        sub['log_diff'] = sub.groupby('Ticker')['Adj Close'].transform(lambda x: np.log(x).diff())\n",
    "\n",
    "        # --- Aggregate stats \n",
    "        agg = (\n",
    "            sub.dropna(subset=['log_diff'])\n",
    "               .groupby('Ticker', as_index=False)['log_diff']\n",
    "               .agg(mean='mean', std='std', n='size')\n",
    "        )\n",
    "        agg.insert(1, 'Period', label)\n",
    "        out.append(agg)\n",
    "\n",
    "    # Combine results\n",
    "    result = pd.concat(out, ignore_index=True)\n",
    "\n",
    "    # Sort output\n",
    "    period_order = list(periods.keys())\n",
    "    result['Period'] = pd.Categorical(result['Period'], categories=period_order, ordered=True)\n",
    "    result = result.sort_values(['Ticker', 'Period']).reset_index(drop=True)\n",
    "    return result\n",
    "\n",
    "\n",
    "#  usage\n",
    "df = pd.read_csv(root_dir / \"data\" / \"raw\" / \"financial\" / \"full_daily_2023_2025.csv\")\n",
    "\n",
    "stats = diff_stats_by_period(df)\n",
    "\n",
    "for row in range(len(stats)):\n",
    "    print(stats.iloc[row].to_dict())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
