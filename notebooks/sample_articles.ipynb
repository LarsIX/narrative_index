{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aaff11e",
   "metadata": {},
   "source": [
    "Notebook used to create samples for manual annotation of the WSJ articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341ea96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from IPython.display import display\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Determine the project root (assumes Wd is /notebooks)\n",
    "current_path = Path().resolve()\n",
    "project_root = current_path.parents[0]  \n",
    "\n",
    "# Define paths for custom functions\n",
    "flagging_path = project_root / \"src\" / \"preprocessing\"\n",
    "cleaning_path = project_root / \"src\" / \"cleaning\"\n",
    "\n",
    "# Add to sys.path if not already present\n",
    "for path in [ cleaning_path, flagging_path]:\n",
    "    path_str = str(path)\n",
    "    if path_str not in sys.path:\n",
    "        sys.path.append(path_str)\n",
    "\n",
    "#  Imports modules\n",
    "from simple_ai_filter import flag_ai_mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c71a4e6",
   "metadata": {},
   "source": [
    "Preparing the First Sample (n=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce955ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Desktop\\Masterarbeit\\AI_narrative_index\\data\\processed\\articles\\articlesWSJ_clean_2024.db\n",
      "Index(['article_id', 'image_src', 'scanned_time', 'title', 'sub_title',\n",
      "       'corpus', 'index_id', 'id', 'date', 'link', 'section',\n",
      "       'cleaned_corpus'],\n",
      "      dtype='object')\n",
      "   article_id image_src         scanned_time  \\\n",
      "0       13068            2025-04-01 09:47:17   \n",
      "1       13069            2025-04-01 09:47:27   \n",
      "2       13070            2025-04-01 09:47:37   \n",
      "3       13071            2025-04-01 09:47:49   \n",
      "4       13072            2025-04-01 09:47:59   \n",
      "\n",
      "                                               title  \\\n",
      "0  Baidu Terminates $3.6B Deal to Buy JOYY’s Chin...   \n",
      "1                The Military’s Phantom ‘Extremists’   \n",
      "2                  Double Dipping in Opioid Lawsuits   \n",
      "3                     Xi Jinping Says Happy New Year   \n",
      "4  Israel Reshuffles Forces, Prepares for Long-Te...   \n",
      "\n",
      "                                           sub_title  \\\n",
      "0  As of the end of December, the closing conditi...   \n",
      "1  An independent study puts to rest another fals...   \n",
      "2  OptumRx seeks to disqualify Motley Rice for a ...   \n",
      "3  China’s leader tries to influence Taiwan’s Jan...   \n",
      "4  Resisting pressure from U.S. to wind down the ...   \n",
      "\n",
      "                                              corpus  index_id  id  \\\n",
      "0  Advertisement\\nBUSINESS\\nTELECOM\\nBaidu Termin...         1   1   \n",
      "1  Advertisement\\nOPINION\\nREVIEW & OUTLOOK\\nFoll...         2   2   \n",
      "2  Advertisement\\nOPINION\\nREVIEW & OUTLOOK\\nFoll...         3   3   \n",
      "3  Advertisement\\nOPINION\\nREVIEW & OUTLOOK\\nFoll...         4   4   \n",
      "4  Israel Reshuffles Forces, Prepares for Long-Te...         5   5   \n",
      "\n",
      "                  date                                               link  \\\n",
      "0  2024-01-01 00:00:00  https://www.wsj.com/business/telecom/baidu-ter...   \n",
      "1  2024-01-01 00:00:00  https://www.wsj.com/opinion/military-extremist...   \n",
      "2  2024-01-01 00:00:00  https://www.wsj.com/opinion/double-dipping-in-...   \n",
      "3  2024-01-01 00:00:00  https://www.wsj.com/opinion/xi-jinping-says-ha...   \n",
      "4  2024-01-01 00:00:00  https://www.wsj.com/world/middle-east/israel-r...   \n",
      "\n",
      "    section                                     cleaned_corpus  \n",
      "0  business  Jan. 1, 644 pm. ET 2 min. As of the end of. De...  \n",
      "1   opinion  REVIEW. OUTLOOK. Jan. 1, 545 pm. ET 834 3 min....  \n",
      "2   opinion  REVIEW. OUTLOOK. OptumRx seeks to disqualify. ...  \n",
      "3   opinion  REVIEW. OUTLOOK. China's leader tries to influ...  \n",
      "4     world  Israel. Reshuffles. Forces,. Prepares for. Lon...  \n",
      "There are False duplicates in the dataframe\n",
      "There are 0 duplicates in the article_id column\n",
      "There are 0 null values in the corpus column\n",
      "There are 0 empty strings in the corpus column\n",
      "There are 14443 unique articles in the dataframe\n",
      "There are 0 duplicates in the article_id column\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the cleaned 2024 database\n",
    "db_path = project_root / \"data\" / \"processed\" / \"articles\" / \"articlesWSJ_clean_2024.db\"\n",
    "print(db_path)\n",
    "# read sql file in dataframe\n",
    "conn = sqlite3.connect(db_path) \n",
    "df = pd.read_sql_query(\"SELECT * FROM article\", conn)\n",
    "\n",
    "# close the connection\n",
    "conn.close() \n",
    "\n",
    "#inspect colums \n",
    "print(df.columns)\n",
    "\n",
    "# insect first 5 rows of the dataframe\n",
    "print(df.head()) \n",
    "\n",
    "# check for duplicates \n",
    "print(f\"There are {df.duplicated().any()} duplicates in the dataframe\")\n",
    "\n",
    "# check for duplicates in article_id\n",
    "print(f\"There are {df['article_id'].duplicated().sum()} duplicates in the article_id column\")\n",
    "\n",
    " # check for null values in corpus\n",
    "print(f\"There are {df['corpus'].isnull().sum()} null values in the corpus column\") \n",
    "\n",
    "# check for empty strings in corpus\n",
    "print(f\"There are {(df['corpus'] == '').sum()} empty strings in the corpus column\")\n",
    "\n",
    "# check number of articles\n",
    "print(f\"There are {df['article_id'].nunique()} unique articles in the dataframe\")\n",
    "\n",
    "# veryfy uniquenes of article_id\n",
    "print(f\"There are {df['article_id'].duplicated().sum()} duplicates in the article_id column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3dee22",
   "metadata": {},
   "source": [
    "Constructing Initial Subsample (n = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac66f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 500 random article from the dataframe to build annotation data set\n",
    "df_sample_large = df.sample(500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bcf4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned sample to a csv file\n",
    "df_path = project_root / \"data\" / \"interim\" / \"articles_WSJ_sub500.csv\"\n",
    "df_sample_large.to_csv(df_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd877e1b",
   "metadata": {},
   "source": [
    "AI-Related Article Filter \n",
    "\n",
    "Uses flag_ai_mentions() to detect AI keywords (AI, A.I., artificial intelligence, machine learning, deep learning, LLM, GPT, ChatGPT, OpenAI, transformer model, generative AI, neural network).  \n",
    "Matching is case-insensitive with word boundaries to avoid false positives.  \n",
    "Ensures each batch contains relevant AI content.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag articles with AI mentions by setting mentioned_ai to 1 if the article contains any of the AI-related keywords, 0 otherwise\n",
    "df_sample_large = flag_ai_mentions(df_sample_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc42669",
   "metadata": {},
   "source": [
    "Sample with seed 42 for annotated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e326b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter  annotated articles\n",
    "ai_articles = df_sample_large[df_sample_large['mentions_ai'] == True]\n",
    "non_ai_articles = df_sample_large[df_sample_large['mentions_ai'] == False]\n",
    "\n",
    "# Randomly select 2 AI-related articles and 1 non-AI article (reproducible with seed)\n",
    "sample_ai = ai_articles.sample(4, random_state=42)\n",
    "sample_non_ai = non_ai_articles.sample(1, random_state=42)\n",
    "\n",
    "# Combine into one DataFrame\n",
    "df_three_articles = pd.concat([sample_ai, sample_non_ai]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#  Display the result\n",
    "df_three_articles[['article_id', 'title', 'corpus', 'mentions_ai']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d64a374",
   "metadata": {},
   "source": [
    "Investigate 5 flagged and an unflagged article to discuss with the annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572abf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter  annotated articles\n",
    "ai_articles = df_sample_large[df_sample_large['mentions_ai'] == True]\n",
    "non_ai_articles = df_sample_large[df_sample_large['mentions_ai'] == False]\n",
    "\n",
    "# Randomly select 3 AI-related articles and 2 non-AI article (reproducible with seed)\n",
    "sample_ai_non_ann = ai_articles.sample(4, random_state=41)\n",
    "sample_non_ai_non_ann = non_ai_articles.sample(1, random_state=41)\n",
    "\n",
    "# Combine into one DataFrame\n",
    "df_non_ann = pd.concat([sample_ai_non_ann, sample_non_ai_non_ann]).reset_index(drop=True).drop(columns=['mentions_ai'])\n",
    "\n",
    "# show columns of the dataframe\n",
    "print(df_non_ann.columns)\n",
    "\n",
    "# Display full text for each article in the corpus\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(df_non_ann[['title', 'corpus']])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f09ec7",
   "metadata": {},
   "source": [
    "Sampling the first batch (n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 50 AI-related articles and 50 non-AI article (reproducible with seed)\n",
    "sample_ai_100 = ai_articles.sample(50, random_state=42)\n",
    "sample_non_ai_100 = non_ai_articles.sample(50, random_state=42)\n",
    "df_non_ann_100 = pd.concat([sample_ai_100, sample_non_ai_100]).reset_index(drop=True).drop(columns=['mentions_ai'])\n",
    "\n",
    "#verify the sample size\n",
    "print(f\"Number of AI-related articles in the sample: {len(sample_ai_100)}\")\n",
    "print(f\"Number of non-AI articles in the sample: {len(sample_non_ai_100)}\")\n",
    "print(f\"Total number of articles in the sample: {len(df_non_ann_100)}\")\n",
    "print(f'columns: {df_non_ann_100.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24b15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv file\n",
    "df_path = project_root / \"data\" / \"interim\" / \"articles_WSJ_batch_one.csv\"\n",
    "df_non_ann_100.to_csv(df_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f3507",
   "metadata": {},
   "source": [
    "Sampling the second batch (n=118)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a63e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory, ensure being in \\notebooks\n",
    "current_path = Path().resolve()\n",
    "\n",
    "# Go up one level to get the project root\n",
    "project_root = current_path.parents[0]\n",
    "\n",
    "# load the large sample from the csv \n",
    "df_path_sub500 = project_root / \"data\" / \"interim\" / \"articles_WSJ_sub500.csv\"\n",
    "df_first_subsample = pd.read_csv(df_path_sub500)\n",
    "\n",
    "# load first batch to exclude it from the large sample\n",
    "df_path_b1 =  project_root / \"data\" / \"interim\" / \"articles_WSJ_batch_one.csv\"\n",
    "first_batch = pd.read_csv(df_path_b1)\n",
    "\n",
    "# verify the loaded files\n",
    "print(f\"Number of articles in the clean df: {len(df_first_subsample)}\")\n",
    "print(f\"columns: {df_first_subsample.columns}\")\n",
    "print(f\"Number of articles in the first batch: {len(first_batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90efcb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert article_id to int64\n",
    "first_batch['article_id'] = first_batch['article_id'].astype('int64')\n",
    "df_first_subsample['article_id'] = df_first_subsample['article_id'].astype('int64')\n",
    "\n",
    "# verify the conversion\n",
    "print(first_batch['article_id'].dtype)\n",
    "print(df_first_subsample['article_id'].dtype)\n",
    "\n",
    "# filter fist_sample for article_id not in df_100_annotated\n",
    "first_sample_not_annotated = df_first_subsample[~df_first_subsample['article_id'].isin(first_batch['article_id'])]\n",
    "\n",
    "# print the number of articles in the filtered dataframe\n",
    "print(f\"Number of articles in the filtered dataframe: {len(first_sample_not_annotated)}\")\n",
    "\n",
    "# print the first 5 rows of the filtered dataframe\n",
    "print(first_sample_not_annotated.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dcff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag articles with AI mentions by setting mentioned_ai to 1 if the article contains any of the AI-related keywords, 0 otherwise\n",
    "first_sample_not_annotated = flag_ai_mentions(first_sample_not_annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d441f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct second sample\n",
    "second_sample = first_sample_not_annotated[first_sample_not_annotated['mentions_ai'] == 1]\n",
    "second_sample = pd.concat([second_sample, first_sample_not_annotated[first_sample_not_annotated['mentions_ai'] == 0].sample(100, random_state=42)])\n",
    "\n",
    "# shuffle the sample, reset the index and drop the old index\n",
    "second_sample = second_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# verify sample size   \n",
    "print(f\"Number of AI-related articles in the sample: {len(second_sample[second_sample['mentions_ai'] == 1])}\")\n",
    "print(f\"Number of non-AI articles in the sample: {len(second_sample[second_sample['mentions_ai'] == 0])}\")\n",
    "\n",
    "# drop the mentions_ai column\n",
    "second_sample = second_sample.drop(columns=['mentions_ai'])\n",
    "\n",
    "# save the second sample to a csv file\n",
    "output_path = project_root / \"data\" / \"interim\" / \"articles_WSJ_batch_two.csv\"\n",
    "second_sample.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84202236",
   "metadata": {},
   "source": [
    "Sampling the Third Batch (n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2eb350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory, ensure being in \\notebooks\n",
    "current_path = Path().resolve()\n",
    "\n",
    "# Go up one level to get the project root\n",
    "project_root = current_path.parents[0]\n",
    "\n",
    "# load the dataset from the cleaned database\n",
    "db_path = project_root / \"data\" / \"interim\" / \"articlesWSJ_clean.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "df = pd.read_sql_query(\"SELECT * FROM article\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf69de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the columns of the full dataset\n",
    "print(f\"Columns in the full dataset: {df.columns}\")\n",
    "\n",
    "# check for NA\n",
    "print(f\"Number of NA values in the full dataset: {df.isna().sum().sum()}\")\n",
    "\n",
    "# check for NA in the article_id column\n",
    "print(f\"Number of NA values in the article_id column: {df['article_id'].isna().sum()}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7379dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory, ensure being in \\notebooks\n",
    "current_path = Path().resolve()\n",
    "\n",
    "# Go up one level to get the project root\n",
    "project_root = current_path.parents[0]\n",
    "\n",
    "# Define full paths to the CSV files in the 'interim' folder\n",
    "interim_path = project_root / \"data\" / \"interim\"\n",
    "batch_1_path = interim_path / \"articles_WSJ_batch_one.csv\"\n",
    "batch_2_path = interim_path / \"articles_WSJ_batch_two.csv\"\n",
    "\n",
    "# Load the CSV files\n",
    "batch_1 = pd.read_csv(batch_1_path)\n",
    "batch_2 = pd.read_csv(batch_2_path)\n",
    "\n",
    "# check the columns of the batch 1 and batch 2\n",
    "print(f\"Columns in the batch 1: {batch_1.columns}\")\n",
    "print(f\"Columns in the batch 2: {batch_2.columns}\")\n",
    "\n",
    "# concatenate the two batches\n",
    "batch_1_2 = pd.concat([batch_1, batch_2], ignore_index=True)   \n",
    "\n",
    "# verify the concatenation\n",
    "print(f\"Number of articles in batch 1 and 2: {len(batch_1_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude the articles that are already annotated\n",
    "df_final = df[~df['article_id'].isin(batch_1_2['article_id'])]\n",
    "\n",
    "# verify the exclusion\n",
    "print(f\"Number of articles in the final dataset: {len(df_final)}\")\n",
    "print(len(df_final) + len(batch_1_2) == len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc8ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag ai-related articles in the final dataset\n",
    "flagged_df = flag_ai_mentions(df_final)\n",
    "\n",
    "# verify the flagging process\n",
    "print(f\"Number of articles in the flagged dataset: {len(flagged_df)}\")\n",
    "print(f\"Number of AI-related articles in the flagged dataset: {flagged_df['mentions_ai'].sum()}\")\n",
    "print(f\"Number of non-AI articles in the flagged dataset: {len(flagged_df) - flagged_df['mentions_ai'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811711fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter  annotated articles\n",
    "ai_articles = flagged_df[flagged_df['mentions_ai'] == True]\n",
    "non_ai_articles = flagged_df[flagged_df['mentions_ai'] == False]\n",
    "\n",
    "# Randomly select 50 AI-related articles and 50 non-AI article (reproducible with seed)\n",
    "sample_ai = ai_articles.sample(50, random_state=42)\n",
    "sample_non_ai = non_ai_articles.sample(50, random_state=42)\n",
    "\n",
    "# Combine into one DataFrame\n",
    "sampled_batch_three = pd.concat([sample_ai, sample_non_ai]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#  Display the result\n",
    "sampled_batch_three[['article_id', 'title', 'corpus', 'mentions_ai']]\n",
    "\n",
    "# print number of articles in the batch three\n",
    "print(f\"Number of articles in the batch three: {len(sampled_batch_three)}\")\n",
    "\n",
    "# verify existence of 50 AI-related articles and 50 non-AI articles\n",
    "print(f\"Number of AI-related articles in the batch three: {len(sampled_batch_three[sampled_batch_three['mentions_ai'] == 1])}\")\n",
    "\n",
    "# drop the mentions_ai column\n",
    "sampled_batch_three = sampled_batch_three.drop(columns=['mentions_ai'])\n",
    "\n",
    "# write to csv file\n",
    "sampled_batch_three.to_csv(interim_path / \"articles_WSJ_batch_three.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d173a65",
   "metadata": {},
   "source": [
    "Sampling the fourth batch (n=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890d2220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory, ensure being in \\notebooks\n",
    "current_path = Path().resolve()\n",
    "\n",
    "# Go up one level to get the project root\n",
    "project_root = current_path.parents[0]\n",
    "\n",
    "# Define full paths to the CSV files in the 'interim' folder\n",
    "interim_path = project_root / \"data\" / \"interim\"\n",
    "\n",
    "# read the batch 1, 2 & 3 from the csv file\n",
    "batch_1 = pd.read_csv(interim_path / \"articles_WSJ_batch_one.csv\")\n",
    "batch_2 = pd.read_csv(interim_path / \"articles_WSJ_batch_two.csv\")\n",
    "batch_3 = pd.read_csv(interim_path / \"articles_WSJ_batch_three.csv\")\n",
    "\n",
    "# find columns that are not in all three batches\n",
    "cols1 = set(batch_1.columns)\n",
    "cols2 = set(batch_2.columns)\n",
    "cols3 = set(batch_3.columns)\n",
    "\n",
    "print(\"In batch_1 but not in batch_2:\", cols1 - cols2)\n",
    "print(\"In batch_2 but not in batch_1:\", cols2 - cols1)\n",
    "\n",
    "print(\"In batch_1 but not in batch_3:\", cols1 - cols3)\n",
    "print(\"In batch_3 but not in batch_1:\", cols3 - cols1)\n",
    "\n",
    "print(\"In batch_2 but not in batch_3:\", cols2 - cols3)\n",
    "print(\"In batch_3 but not in batch_2:\", cols3 - cols2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f88af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the batches\n",
    "batch_1_2_3 = pd.concat([batch_1, batch_2, batch_3], ignore_index=True)   \n",
    "\n",
    "# verify the concatenation\n",
    "print(f'there are 318 articles in the concatenated df:', len(batch_1_2_3) == 318)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f8bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from the cleaned database\n",
    "path = interim_path / \"articlesWSJ_clean.db\"\n",
    "conn = sqlite3.connect(path)\n",
    "df = pd.read_sql_query(\"SELECT * FROM article\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5178882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the columns of the full dataset\n",
    "print(f\"Columns in the full dataset: {df.columns}\")\n",
    "print(f\"Number of articles in the full dataset: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f737510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset for article_id not in batch 1, 2 and 3\n",
    "batch_1_2_3['article_id'] = batch_1_2_3['article_id'].astype('int64')\n",
    "df_final = df[~df['article_id'].isin(batch_1_2_3['article_id'])]\n",
    "\n",
    "# verify the exclusion\n",
    "print(f\"Number of articles in the final dataset: {len(df_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag ai-related articles in the final dataset\n",
    "flagged_df = flag_ai_mentions(df_final)\n",
    "\n",
    "# verify the flagging process\n",
    "print(f\"Number of articles in the flagged dataset: {len(flagged_df)}\")\n",
    "print(f\"Number of AI-related articles in the flagged dataset: {flagged_df['mentions_ai'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 350 AI-related articles and 350 non-AI article (reproducible with seed)\n",
    "sample_ai = flagged_df[flagged_df['mentions_ai'] == True].sample(350, random_state=42)\n",
    "sample_non_ai = flagged_df[flagged_df['mentions_ai'] == False].sample(350, random_state=42)\n",
    "\n",
    "# Combine into one DataFrame\n",
    "sampled_batch_four = pd.concat([sample_ai, sample_non_ai]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#  Display the result\n",
    "print(f\"Number of articles in the batch four: {len(sampled_batch_four)}\")\n",
    "print(f\"Number of AI-related articles in the batch four: {len(sampled_batch_four[sampled_batch_four['mentions_ai'] == 1])}\")\n",
    "\n",
    "# drop the mentions_ai column\n",
    "sampled_batch_four = sampled_batch_four.drop(columns=['mentions_ai'])\n",
    "\n",
    "# write to csv file\n",
    "sampled_batch_four.to_csv(interim_path / \"articles_WSJ_batch_four.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523eef6",
   "metadata": {},
   "source": [
    "Annotating 25% of batch 4 (n=175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64857e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 25% at random, reproducible with seed\n",
    "df_fourth_batch_sample = df_fourth_batch.sample(frac=0.25, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# verify the sample size\n",
    "print(f\"Number of articles in the sample: {len(df_fourth_batch_sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate the articles using the annotator function\n",
    "df_fourth_batch_sample_annotated_author = af(df=df_fourth_batch_sample);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e77a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv file\n",
    "df_fourth_batch_sample_annotated_author.to_csv(interim_path /  \"articles_WSJ_batch_four_subsample_author.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
