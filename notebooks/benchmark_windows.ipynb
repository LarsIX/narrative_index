{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbffa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Determine the project root\n",
    "current_path = Path().resolve()\n",
    "project_root = current_path.parents[0]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93477e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the project root\n",
    "current_path = Path().resolve()\n",
    "project_root = current_path.parents[0]  \n",
    "\n",
    "# Define data path\n",
    "var_path = project_root / \"data\" / \"processed\" / \"variables\"\n",
    "art_path =  project_root / \"data\" / \"processed\" / \"articles\"\n",
    "\n",
    "# load labeled subset\n",
    "man_data = pd.read_csv(art_path / \"annotated_subsample_WSJ_final.csv\")\n",
    "dict_labels = pd.read_csv(art_path / \"naive_AI_labels_2023-2025.csv\")    \n",
    "\n",
    "\n",
    "# subset for 2024\n",
    "dict_labels[\"date\"] = pd.to_datetime(dict_labels[\"date\"])\n",
    "dict_labels = dict_labels[dict_labels[\"date\"].dt.year == 2024]\n",
    "dict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e9e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform hype level\n",
    "man_data[\"hype_level\"] = man_data[\"hype_level\"].replace([2,3],1).astype(int)\n",
    "\n",
    "# ensure datatype in label collumn\n",
    "dict_labels[\"hype_level\"] = dict_labels[\"about_ai\"].astype(int) \n",
    "\n",
    "# subset dicts\n",
    "dict_labels_sub = dict_labels[dict_labels[\"article_id\"].isin(man_data[\"article_id\"])]\n",
    "print(len(dict_labels_sub))\n",
    "\n",
    "# fraction of ai-related articles \n",
    "print(\n",
    "    f\"total: {man_data['hype_level'].sum()} | \"\n",
    "    f\"relative: {man_data['hype_level'].sum() / man_data['hype_level'].shape[0]:.3f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbf0d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = man_data[[\"article_id\", \"hype_level\"]].merge(\n",
    "    dict_labels_sub[[\"article_id\", \"hype_level\"]],\n",
    "    on=\"article_id\",\n",
    "    suffixes=(\"_man\", \"_dict\")\n",
    ")\n",
    "\n",
    "agree_total = (merged[\"hype_level_man\"] == merged[\"hype_level_dict\"]).sum()\n",
    "agree_total\n",
    "agree_frac = agree_total / 1018\n",
    "agree_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f204ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load FinBert-annotated binary data\n",
    "fin_data_23 = pd.read_csv(var_path / \"FinBERT_binary_prediction_2023.csv\")\n",
    "fin_data_24 = pd.read_csv(var_path / \"FinBERT_binary_prediction_2024.csv\")\n",
    "fin_data_25 = pd.read_csv(var_path / \"FinBERT_binary_prediction_2025.csv\")\n",
    "\n",
    "#concat finbert\n",
    "fin_data = pd.concat([fin_data_23,fin_data_24,fin_data_25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9421def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all three prediction outputs\n",
    "df23 = pd.read_csv(var_path / \"FinBERT_binary_prediction_2023.csv\")\n",
    "df24 = pd.read_csv(var_path / \"FinBERT_binary_prediction_2024.csv\")\n",
    "df25 = pd.read_csv(var_path / \"FinBERT_binary_prediction_2025.csv\")\n",
    "\n",
    "# Ensure article_id is a string and prefix with year\n",
    "df23[\"article_id\"] = \"2023\" + df23[\"article_id\"].astype(str)\n",
    "df24[\"article_id\"] = \"2024\" + df24[\"article_id\"].astype(str)\n",
    "df25[\"article_id\"] = \"2025\" + df25[\"article_id\"].astype(str)\n",
    "\n",
    "# Combine and check duplicates\n",
    "df_all = pd.concat([df23.assign(year=2023), df24.assign(year=2024), df25.assign(year=2025)])\n",
    "dupes = df_all[df_all.duplicated(\"article_id\", keep=False)]\n",
    "\n",
    "print(f\"Duplicate article_ids across years: {dupes.article_id.nunique()}\")\n",
    "print(dupes[[\"article_id\", \"year\", \"title\"]].sort_values(\"article_id\").head(10))#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87dbc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print(\"Basic Overview of df_all\")\n",
    "print(f\"Total rows: {len(df_all)}\")\n",
    "print(f\"Unique article_ids: {df_all['article_id'].nunique()}\")\n",
    "print(f\"Duplicate article_ids: {(df_all['article_id'].duplicated()).sum()}\")\n",
    "\n",
    "# Date range\n",
    "print(\"Date range:\")\n",
    "print(f\"  Min date: {df_all['date'].min()}\")\n",
    "print(f\"  Max date: {df_all['date'].max()}\")\n",
    "\n",
    "# Overall mean\n",
    "print(\"Mean predicted_label (overall):\", round(df_all[\"predicted_label\"].mean(), 3))\n",
    "\n",
    "# Mean by year\n",
    "print(\"Mean predicted_label by year:\")\n",
    "print(df_all.groupby(\"year\")[\"predicted_label\"].mean().round(3))\n",
    "\n",
    "# Count by year\n",
    "print(\"Article count by year:\")\n",
    "print(df_all[\"year\"].value_counts().sort_index())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ee1aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finbert inference of AI-relatedness\n",
    "print(f\"Total: {df_all['predicted_label'].sum()} |\"\n",
    "      f\"Relative: {df_all['predicted_label'].sum() / df_all.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cda404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load naively labeled date \n",
    "naive_df = pd.read_csv(art_path / \"naive_AI_labels_2023-2025.csv\")\n",
    "\n",
    "# Ensure date is parsed as datetime\n",
    "naive_df[\"date\"] = pd.to_datetime(naive_df[\"date\"])\n",
    "\n",
    "# Prefix year to article_id\n",
    "naive_df[\"article_id\"] = naive_df[\"date\"].dt.year.astype(str) + naive_df[\"article_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa18a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_fin_naive = df_all[[\"article_id\", \"predicted_label\"]].merge(\n",
    "    naive_df[[\"article_id\", \"about_ai\"]],\n",
    "    on=\"article_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "agreement = (merged_fin_naive[\"predicted_label\"].astype(int) == merged_fin_naive[\"about_ai\"].astype(int)).mean()\n",
    "print(\"Agreement rate on matched article_ids:\", round(agreement, 3))\n",
    "len(merged_fin_naive)\n",
    "naive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3731cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
