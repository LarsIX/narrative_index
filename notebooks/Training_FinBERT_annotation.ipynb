{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e32b65f",
   "metadata": {},
   "source": [
    "Notebook to train a custom FinBERT model on the binary AINI dataset to annotate WSJ articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094a2b7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatasetDict, Dataset\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, set_seed\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding, EarlyStoppingCallback,AutoConfig, get_scheduler,BertModel\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, set_seed\n",
    "from transformers import DataCollatorWithPadding, EarlyStoppingCallback,AutoConfig, get_scheduler,BertModel\n",
    "from IPython.display import display, Markdown \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    nltk.download(\"punkt\")\n",
    "    import en_core_web_sm\n",
    "    nlp = en_core_web_sm.load()\n",
    "\n",
    "# append model path\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent  \n",
    "sys.path.insert(0, str(project_root))  \n",
    "\n",
    "from src.modelling.CustomFinBERT import CustomFinBERT\n",
    "from src.modelling.ai_windows import extract_multiple_ai_snippets_with_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "268ab0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1018 total rows.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sub_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cleaned_corpus",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label_ai_related",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hype_level",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "modified",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hype_level_change",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "index_id",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "scanned_time",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "corpus",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "section",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "acff511c-2195-4fbd-910b-2008df6c69a1",
       "rows": [
        [
         "0",
         "28537",
         "Apple iPhone Sales Slump in China Amid Huawei’s Comeback",
         "Huawei’s sales were helped by the successful launch of its 5G-capable Mate 60 series",
         "April 23, 651 am. ET 2 min smartphone sales in. China dropped sharply in the first quarter, hurt of local rivals and the return of. Huawei. Technologies to the high-end segment, data from a closely followed research firm showed. Apple's iPhone sales fell 19 on year in the world's largest smartphone market, placing the company third overall behind. Vivo and. Honor,. Counterpoint. Research said. Tuesday. Sales by. No. 4 seller. Huawei, which had suffered from limited access to advanced chips a year ago, rose 70 on year, helped launch of its 5G-capable. Mate 60 series,. Counterpoint said. Oppo and. Xiaomi rounded out the top six places for the quarter. Counterpoint described the period as \"the most competitive quarter ever,\" with market shares of the top seller and the sixth-best seller differing 3.0 percentage points. Promotions and holiday demand pushed overall smartphone sales 1.5 higher on year in. China, marking a second straight quarter of growth,. Counterpoint said. For the full year, it forecast growth in the low single-digits, with more. Chinese brands integrating artificial intelligence features into their flagship devices. Apple's iPhone sales accounted for more than half of total revenue in the company's. October-to-December quarter, expanding nearly 6 on year to post 69.70 billion in sales. Investors have been concerned that growth in a critical segment for the. Cupertino,. California-based company is under pressure. Preliminary data from research firm. International. Data. Corporation showed last week that. Apple's global smartphone shipments had fallen 9.6 on year in the first quarter, toppling the iPhone maker from its perch as the world's largest phone maker. Trump's. New. Protectionist. Age. Trump and. His 'Little. Disturbance'. From. Tariffs. Canada,. Our. Friend,. Deserves. Better. Than. This. Apple. Says. Some. AI-Powered. Enhancements to. Siri to. Be. Delayed",
         "1",
         "1.0",
         "0.0",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "31336",
         "AI Is Helping Scammers Outsmart You—and Your Bank",
         "Your ‘spidey sense’ is no match for the new wave of scammers.",
         "ILLUSTRATION. MARK. HARRIS,. ISTOCK 7. June 22, 530 am. ET 8 minutes. Artificial intelligence is making scammers tougher to spot. Gone are the poorly worded messages that easily tipped off authorities as well as the grammar police. The bad guys are now better writers and more convincing conversationalists, who can hold a conversation without revealing they are a bot, say the bank and tech investigators who spend their days tracking the latest schemes. ChatGPT and other. AI tools can even enable scammers to create an imitation of your voice and identity. In recent years, criminals have used. AI-based software to impersonate senior executives and demand wire transfers. \"Your spidey senses are no longer going to prevent you from being victimized,\" said. Matt. O'Neill, a former. Secret. Service agent and co-founder of cybersecurity firm 5OH. Consulting. In these recent cases, the frauds are often similar to old scams. But. AI has enabled scammers to target much larger groups and use more personal information to convince you the scam is real. Fraud-prevention officials say these tactics are often harder to spot because they bypass traditional indicators of scams, such as malicious links and poor wording and grammar. Criminals today are faking driver's licenses and other identification in an attempt to open new bank accounts and adding computer-generated faces and graphics to pass identity-verification processes. All of these methods are hard to stave off, say the officials. JPMorgan. Chase has begun using large-language models to validate payments, which helps fight fraud. Carisma. Ramsey. Fields, vice president of external communications at. JPMorgan. Chase, said the bank has also stepped up its efforts to educate customers about scams. And while banks stop some fraud, the last line of defense will always be you. These security officials say to never financial or personal information unless you're certain about who's on the receiving end. If you do pay, use a credit card because it offers the most protection. \"Somebody who tells you to pay by crypto, cash, gold, wire transfer or a payment app is likely a scam,\" said. Lois. Greisman, an associate director of the. Federal. Trade. Commission. With. AI as an accomplice, fraudsters are reaping more money from victims of all ages. People reported losing a record 10 billion to scams in , up from 9 billion a year prior, according to the. FTC. Since the. FTC estimates only 5 of fraud victims report their losses, the actual number could be closer to 200 billion. Joey. Rosati, who owns a small cryptocurrency firm, never thought he could fall for a scam until a man he believed to be a police officer called him in. May. Finance entrepreneur. Joey. Rosati, who was almost scammed, was surprised how convincing and knowledgeable fraudsters can be. The man told. Rosati he had missed jury duty. The man seemed to know all about him, including his. Social. Security number and that he had just moved to a new house. Rosati followed the officer's instruction to come down to the station in. Hillsborough. County,. Fla. which didn't seem like something a scammer would suggest. On the drive over,. Rosati was asked to wire 4,500 to take care of the fine before he arrived. It was then that. Rosati realized it was a scam and hung up. \"I'm not uneducated, young, immature. I have my head on my shoulders,\". Rosati said. \"But they were perfect.\". Social-engineering attacks like the jury-duty scam have grown more sophisticated with. AI. Scammers use. AI tools to unearth details about targets from social media and data breaches, cybersecurity experts say. AI can help them adapt their schemes in real time messages that convincingly mimic trusted individuals, persuading targets to send money or divulge sensitive information. A job scammer played on the emotions of. David. Wenyu, who had been unemployed for six months. David. Wenyu's. LinkedIn profile displayed an \"open to work\" banner when he received an email in. May offering a job opportunity. It appeared to be from. SmartLight. Analytics, a legitimate company, and came six months after he had lost his job. He accepted the offer, even though he noticed the email address was slightly different from those on the company's website. The company issued him a check to purchase work-from-home equipment from a specific website. When they told him to buy the supplies before the money showed up in his account, he knew it was a scam. \"I was just emotionally too desperate, so. I ignored those red flags,\". Wenyu said. In an. April survey of 600 fraud-management officials at banks and financial institutions company. Biocatch, 70 said the criminals were more skilled at using. AI for financial crime than banks are at using it for prevention. Kimberly. Sutherland, vice president of fraud and identity strategy at. LexisNexis. Risk. Solutions, said there has been a noticeable rise in fraud attempts that appear to be. AI related in . Password risks, amplified. Criminals used to have to guess or steal passwords through phishing attacks or data breaches, often targeting high-value accounts one by one. Now, scammers can quickly cross-reference and test reused passwords across platforms. They can use. AI systems to write code that would automate various aspects of their ploys,. O'Neill said. If scammers obtain your email and a commonly used password from a tech company data breach,. AI tools can swiftly check if the same credentials unlock your bank, social media or shopping accounts. Financial institutions are taking new stepsand tapping. AI themselvesto shield your money and data. Banks monitor how you enter credentials, whether you tend to use your left or right hand when swiping on the app, and your device's. IP address to build a profile on you. If a login attempt doesn't match your typical behavior, it is flagged, and you may be prompted to provide more information before proceeding. They can tell when you're being coerced into filling out information, because of shifts in your typing cadence. If digits are copied and pasted, if the voice verification is too perfect, or if is too evenly spaced and grammatically correct, that is a red flag, said. Jim. Taylor, chief product officer at. RSA. Security, a firm with fraud-detection tech used ,. Citibank and others. Consumers paid scammers 1.4 billion in cryptocurrency in , up more than 250 from , according to. FTC data. What steps do you take to protect yourself and your money? Join the conversation below. As a result, security officials suggest that you turn on two-factor authentication, so you get a or email whenever someone tries logging into one of your accounts. If anything feels off during a potential money exchange, take a beat. Pressing pause on a potentially fraudulent situation is also important psychologically. Many scammers try to create a false urgency or confuse victims to manipulate them. If all the information about a transaction or account is coming from one person, that is a red flag. Get a second opinion from a trusted contact. \"If it's going to hurt if you lose it, validate it,\". O'Neill, the former. Secret. Service agent said. JPMorgan. Chase has begun using large-language models to validate payments, which helps fight fraud. An earlier version of this article incorrectly said the bank was using the models to fight identity fraud. Corrected on. June 27. To. Fix. Your. Food. Budget,. Trim the. Waste",
         "1",
         "2.0",
         "0.0",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "2",
         "29569",
         "Alibaba Cloud Trims Prices for International Customers",
         "Alibaba Cloud said the cuts were aligned with an “AI first strategy” to make core computing resources accessible to a range of customers",
         "Alibaba. Cloud said the cuts were aligned with an \"AI first strategy\" to make core computing resources accessible to a range of customers 2 min. The digital technology and intelligence arm of tech giant. Alibaba said the new pricing strategy covers five core public-cloud categories. Alibaba. BABA -10.62 decrease; red down pointing triangle. Group is lowering prices on core cloud products for international customers, seeking to tap enthusiasm for artificial intelligence computing and gain an edge in markets dominated of. Amazon,. Google and. Microsoft. The cloud-computing arm of. Chinese technology giant. Alibaba said. Monday that it has trimmed prices across five categories 23 for customers making use of its data centers outside mainland. China. The cuts an average 20 decrease in prices for various products in late. February for customers in. China. Alibaba. Cloud said the cuts were aligned with an \"AI first strategy\" to make core computing resources accessible to a range of customers. Alibaba's revenue from cloud services, considered one of the. Chinese e-commerce giant's most promising businesses, has slowed in recent quarters amid competition from rival cloud units of. Huawei. Technologies and. Tencent. Holdings 700 -1.23 decrease; red down pointing triangle in its key market of. China. Cloud sales rose about 3 from a year earlier in each of the last two. October-to-December quarters, slowing from 20 for the same period in . Alibaba had planned to spin off its cloud unit when the company announced a major restructuring plan last year. Chairman. Joe. Tsai then said in. November that the plan might not benefit shareholders, citing the effects of. U.S. export restrictions on advanced computing chips. Shares of. Alibaba closed 0.5 higher at 70.40. Hong. Kong dollars. US8.99, beating a 0.05 rise in the benchmark. Hang. Seng. Index. Trump and. His 'Little. Disturbance'. From. Tariffs. Women's. Soccer. League. Signs. Sponsorship. With 'Gen. Z. Whisperer'. Alex. Cooper",
         "1",
         "1.0",
         "0.0",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "18063",
         "Nvidia to Replace Intel in Dow Jones Industrial Average. Sherwin-Williams Also Joins.",
         "The swap reflects their reversal of fortunes within the tech industry and would have been unthinkable a few years ago",
         "The latest news analysis. MARKETS. FINANCE. Nvidia to. Replace. Intel in. Dow. Jones. Industrial. Average. Sherwin-Williams. Also. Joins. Nov. 1, 600 pm. ET 22 5 min. Big. Tech customers are investing in. AI systems that need. Nvidia's graphics processing units. Nvidia will replace. Intel in the. Dow. Jones. Industrial. Average next week, a swap that reflects their reversal of fortunes within the tech industry. Sherwin-Williams will replace. Dow. Inc. as well. SP. Dow. Jones. Indices, which manages the 30-stock benchmark, said the changes were made to ensure a more representative exposure to the semiconductors industry and the materials sector. They are effective prior to the open of trading on. Nov. 8. For. Intel and. Dow. Inc., the moves are largely symbolic. There should be little practical impact because few funds track the. Dow index compared with the larger. SP 500. For. Intel, being replaced have been unthinkable just three years ago. Now, it underscores one of numerous strategic missteps that have demoted. Intel from tech titan to a takeover target. The chip manufacturer largely missed the boat on artificial intelligence, which is developing into a driving force of the. U.S. economy. Intel shares have dropped more than 50 this year after it became clear that. Chief. Executive. Pat. Gelsinger's turnaround plan wasn't working. Intel paused its dividend, announced billions in cost cuts and said it would lay off 15,000 employees in a nightmare earnings report this summer that accelerated the selloff. The stock rose 7.8. Friday after the company reported a 16.6 billion quarterly loss but offered hints of optimism. Intel joined the blue-chip index on. Nov. 1, . Nvidia, meanwhile, has become the highflying face of the. AI boom. Big. Tech customers that are spending big on. AI systems can't get enough of. Nvidia's graphics processing units. Sales, and. Nvidia's stock price, have soared, sending the company to a more than 3 trillion market cap that places it neck-and-neck with. Apple for the title of most valuable. U.S. company. Shares have risen eightfold since the beginning of . Dow. Inc., meanwhile, had become a smaller company than the original chemical and materials conglomerate after spinning off into three separately traded companies in . Its shares are down slightly over the last five years, while its replacement,. Sherwin-Williams, is up about 85. Unlike the. SP 500 and the. Nasdaq. Composite, the blue-chip index is weighted by price, not . It is calculated prices of the 30 stocks and dividing by a factor that accounts for changes such as stock splits and index entrants. That means that companies with a higher price have a greater effect on index moves, regardless of their total market value. With a price of 23.20,. Intel was least influential stock in the benchmark, while. Dow. Inc. was. No. 28. At 135.40,. Nvidia would rank 22ndit executed a 10-for-1 stock split in. June that analysts said made its inclusion in the. Dow more likely. Sherwin-Williams closed. Friday at 357.97, which would give it the sixth-highest price in the index. The. Dow has lagged behind the. SP 500 and. Nasdaq in recent years because it is less oriented toward technology stocks. It is up 12 this year, while the other indexes have climbed more than 20. The. Dow's last shake-up came in. February, when online retail powerhouse. Amazon.com replaced. Walgreens. Boots. Alliance. A committee composed of representatives of. SP. Dow. Jones. Indices and. The. Wall. Street. Journal determines the composition of the index. The committee looks for companies with an \"excellent\" reputation, sustained growth and high level of interest from investors, according to index methodology. Changes are made to the index on an as-needed basis. A company must be part of the. SP 500 to be considered for membership in the. Dow. The. Dow industrials exclude companies in the utilities and transportation industries, which are represented in separate. Dow indexes. Charles. Dow, the first editor of the. Journal and co-founder of. Dow. Jones. Co., the publisher of the. Journal, created the. Dow average to help explain stock-market movements to his readers. An average of 12 stocks was published daily in the. Journal beginning in . An earlier stock average, consisting mostly of railroads, was printed in a predecessor publication in . The industrial average expanded to 20 names in and 30 companies in . Sherwin-Williams closed. Friday at 357.97, which would give it the sixth-highest price in the. Dow. Jones. Industrial. Average. An earlier version of this article incorrectly said. Sherwin-Williams closed. Friday at 70.94, which would make it among the five stocks with the lowest price. Corrected on. Nov. 1. A 6. Trillion. Trump. Tax. Increase? Nvidia's. Big. Show. Hits. Its. Marks but. Plays to. Tough. Crowd",
         "1",
         "0.0",
         "0.0",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "4",
         "29872",
         "The Man Whose Musings Fuel Elon Musk’s Nightmares",
         "A marketing professor in Canada has formed a bromance with the billionaire around what they fear is ailing the world",
         "The. Man. Whose. Musings. Fuel. Elon. Musk's. Nightmares. EMIL. LENDOFTHE. WALL. STREET. JOURNAL;. GETTY. IMAGES. May 11, 530 am. ET 440 7 min. Elon. Musk's bedtime routine is giving him nightmares. To hear him in public is to catalog a running list of his greatest fears end-of-the-world type stuff, killer. AI threats and, in recent years, the scourge of what he calls the. Woke. Mind. Virus. \"I listen to podcasts about the fall of civilizations to go to sleep,\". Musk said this past week during an appearance at the. Milken. Institute conference. \"So perhaps that might be part of the problem.\". One provocateur, in particular, has caught his attention of late. Gad. Saad, a marketing professor at. Concordia. University in. Montreal, and author of the book \"The. Parasitic. Mind. How. Infectious. Ideas. Are. Killing. Common. Sense.\". To read the book, which was first published in , is to see reflections of. Musk's most contentious comments during the past few years, as the billionaire has increasingly expressed concerns about diversity, equity and inclusion efforts and illegal immigration overtaking the. U.S. \"I read your insightful book on the parasitic woke mind virus,\". Musk tweeted praise to. Saad earlier this year. \"It gave me nightmares.\". Both men a common embrace of social media. On. X,. Saad has almost 900,000 followers and on. YouTube more than 300,000 subscribers. Both men have separately appeared on the popular. Joe. Rogan podcast several times. The. Canadian academic. Gad. Saad speaking at a. September summit in. Budapest. Along the way,. Musk and. Saad have developed something of a public bromance. This year alone,. Musk has interacted with. Saad's. X account more than 140 times. All of the attention has apparently helped boost interest in the book. Since going on sale, it has sold more than 120,000 copies across all formats, according to. Skyhorse. Publishing, which acquired the book's original publisher,. Regnery. Publishing. Paperback sales jumped 94 in the first four months of compared with a year earlier, while the digital version rose 254. \"We just did an unexpected rush reprint of 10,000 copies and. I anticipate another very soon,\". Tony. Lyons, president of. Skyhorse. Publishing, said in an email. The book is an extension of. Saad's career exploring how human evolution informs modern consumer behaviora controversial way of looking at the world that is sometimes called evolutionary psychology. Over the years, his academic writings have covered a range of disciplines, including, for example, how menstrual cycles influence food and appearance-related consumption; and how the use of waist-to-hip ratios in online escort ads illustrate what. Saad describes as \"near-universal\" preferences among men for certain female attributes. NEWSLETTER. SIGN-UP. A weekly digest of tech columns, big stories and personal tech advice, plus a news ticker and a touch of dark humor. Saad wrote that \"The. Parasitic. Mind\" was inspired, in part, in academia, where he described a herd mindset that chastised innovative thinkers. He described pushback he encountered, including his ideas being labeled as \"sexist nonsense\" and his efforts to use \"biologically-based theorizing\" to explain consumer behavior being dismissed as too reductionistic. \"The. West is currently suffering from such a devastating pandemic, a collective malady that destroys people's capacity to think rationally,\" the 59-year-old. Saad wrote at the beginning of his book. \"Unlike other pandemics where biological pathogens are to blame, the current culprit is composed of a collection of bad ideas, spawned on university campuses, that chip away at our edifices of reason, freedom, and individual dignity.\". His outspoken statementssuch as last year on the. Rogan podcast, in which he mocked the. Quebec. French accent as \"an affront to human dignity\"have drawn criticism back home in. Canada. \"There are several accents in. Quebec and this is what sets us apart around the world,\". Pascale. Dry,. Quebec's minister of higher education, responded in. French. \"This is what makes us special and we are very proud of it!\". Another inspiration for his book,. Saad writes, was his experience as a boy fleeing with other. Jews from his home in. Lebanon during that country's civil war. In the book, he detailed some of the horrors he experienced, including the kidnapping of his parents. \"The. Lebanese war taught me early about the ugliness of tribalism and religious dogma,\". Saad wrote. \"It likely informed my subsequent disdain for identity politics, as. I grew up in an ecosystem where the group to which you belonged mattered more than your individuality.\". Musk has said his concerns about. Woke. Mind. Virus, his way of labeling progressive liberal beliefs that he says are overly politically correct and stifling to public debate and free speech, helped fuel his desire to acquire the social-media company. Twitter turned. X in late . It is on that platform where. Musk, 52 years old, has aired many of his concerns. His increasingly public stance on contentious social issues has had an effect on his companies. Some advertisers have fled. X in the midst of concerns about the drama around his ownership, and data suggests car buyers who identify as. Democrats were turned off last fall from buying vehicles made by. Tesla, where he is chief executive, after his outbursts. For his part,. Musk says his politics are \"fairly moderate\"what he describes as his supporting safe cities, secure borders, a neutral judiciary and sensible spending. And, he adds, what he calls being \"pro environment.\". Still,. Musk is prone to painting risks at their most extreme and gravitating to others with similar world views. \"For many years now,. I have warned that the path that the. West is taking will result in civil war. It might take 5 years, 50 years, or 100 years but it is inevitable,\". Saad tweeted on the day of. Tesla's quarterly earnings call last month. What does. Elon. Musk's interest in the ideas of. Gad. Saad reveal to you? Join the conversation below. Before joining that call,. Musk was on. X, agreeing with. Saad in a thread of responses. \"War will come whether we want it or not,\". Musk posted. The full extent of their relationship couldn't be learned, though. Saad has suggested that the two trade emails and private messages. At one point this year,. Saad shared online that. Musk had slipped into his private messages a sassy question. How is that you're surviving in. Canada amidst the infestation of the woke mind viruses? \"I thought, 'Oh my god, what a cool life. I lead that. I can just receive out of the blue such a. DM from. Elon.'\". Behind. Musk's. Management. Philosophy. Trump and. His 'Little. Disturbance'. From. Tariffs. Why. Lululemon. Will. Avoid. Trump's. Tariffs and. Canada. Goose. Might. Not",
         "0",
         "0.0",
         "0.0",
         null,
         null,
         null,
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>sub_title</th>\n",
       "      <th>cleaned_corpus</th>\n",
       "      <th>label_ai_related</th>\n",
       "      <th>hype_level</th>\n",
       "      <th>modified</th>\n",
       "      <th>hype_level_change</th>\n",
       "      <th>index_id</th>\n",
       "      <th>scanned_time</th>\n",
       "      <th>corpus</th>\n",
       "      <th>section</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28537</td>\n",
       "      <td>Apple iPhone Sales Slump in China Amid Huawei’...</td>\n",
       "      <td>Huawei’s sales were helped by the successful l...</td>\n",
       "      <td>April 23, 651 am. ET 2 min smartphone sales in...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31336</td>\n",
       "      <td>AI Is Helping Scammers Outsmart You—and Your Bank</td>\n",
       "      <td>Your ‘spidey sense’ is no match for the new wa...</td>\n",
       "      <td>ILLUSTRATION. MARK. HARRIS,. ISTOCK 7. June 22...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29569</td>\n",
       "      <td>Alibaba Cloud Trims Prices for International C...</td>\n",
       "      <td>Alibaba Cloud said the cuts were aligned with ...</td>\n",
       "      <td>Alibaba. Cloud said the cuts were aligned with...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18063</td>\n",
       "      <td>Nvidia to Replace Intel in Dow Jones Industria...</td>\n",
       "      <td>The swap reflects their reversal of fortunes w...</td>\n",
       "      <td>The latest news analysis. MARKETS. FINANCE. Nv...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29872</td>\n",
       "      <td>The Man Whose Musings Fuel Elon Musk’s Nightmares</td>\n",
       "      <td>A marketing professor in Canada has formed a b...</td>\n",
       "      <td>The. Man. Whose. Musings. Fuel. Elon. Musk's. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  \\\n",
       "0       28537  Apple iPhone Sales Slump in China Amid Huawei’...   \n",
       "1       31336  AI Is Helping Scammers Outsmart You—and Your Bank   \n",
       "2       29569  Alibaba Cloud Trims Prices for International C...   \n",
       "3       18063  Nvidia to Replace Intel in Dow Jones Industria...   \n",
       "4       29872  The Man Whose Musings Fuel Elon Musk’s Nightmares   \n",
       "\n",
       "                                           sub_title  \\\n",
       "0  Huawei’s sales were helped by the successful l...   \n",
       "1  Your ‘spidey sense’ is no match for the new wa...   \n",
       "2  Alibaba Cloud said the cuts were aligned with ...   \n",
       "3  The swap reflects their reversal of fortunes w...   \n",
       "4  A marketing professor in Canada has formed a b...   \n",
       "\n",
       "                                      cleaned_corpus  label_ai_related  \\\n",
       "0  April 23, 651 am. ET 2 min smartphone sales in...                 1   \n",
       "1  ILLUSTRATION. MARK. HARRIS,. ISTOCK 7. June 22...                 1   \n",
       "2  Alibaba. Cloud said the cuts were aligned with...                 1   \n",
       "3  The latest news analysis. MARKETS. FINANCE. Nv...                 1   \n",
       "4  The. Man. Whose. Musings. Fuel. Elon. Musk's. ...                 0   \n",
       "\n",
       "   hype_level  modified  hype_level_change  index_id scanned_time corpus  \\\n",
       "0         1.0       0.0                NaN       NaN          NaN    NaN   \n",
       "1         2.0       0.0                NaN       NaN          NaN    NaN   \n",
       "2         1.0       0.0                NaN       NaN          NaN    NaN   \n",
       "3         0.0       0.0                NaN       NaN          NaN    NaN   \n",
       "4         0.0       0.0                NaN       NaN          NaN    NaN   \n",
       "\n",
       "  section date  \n",
       "0     NaN  NaN  \n",
       "1     NaN  NaN  \n",
       "2     NaN  NaN  \n",
       "3     NaN  NaN  \n",
       "4     NaN  NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.float64(0.0), np.float64(1.0), np.float64(2.0), np.float64(3.0)}\n",
      "Index(['article_id', 'title', 'sub_title', 'cleaned_corpus',\n",
      "       'label_ai_related', 'hype_level', 'modified', 'hype_level_change',\n",
      "       'index_id', 'scanned_time', 'corpus', 'section', 'date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Set project root, assumes being in /notebooks\n",
    "project_root = Path.cwd().parent\n",
    "\n",
    "# Path to annotated cvs\n",
    "articles_dir = project_root / \"data\" / \"processed\" / \"articles\" / \"annotated_subsample_WSJ_final.csv\"\n",
    "\n",
    "# read and merge annotated cvs\n",
    "df = pd.read_csv(articles_dir)\n",
    "\n",
    "# verify merge\n",
    "print(f\" {len(df)} total rows.\")\n",
    "display(df.head(5))\n",
    "print(set(df.hype_level.values))\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a453daa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# ensure integrity\n",
    "print(df.corpus.isna().sum())\n",
    "labeled_df = df.copy()\n",
    "labeled_df.loc[labeled_df[\"corpus\"].isna(), \"corpus\"] = df.loc[df[\"corpus\"].isna(), \"cleaned_corpus\"]\n",
    "\n",
    "# verify fix\n",
    "print(labeled_df.corpus.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "138719c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column\n",
    "labeled_df.rename(columns={\"hype_level\": \"label\"}, inplace=True)\n",
    "\n",
    "# binary label creation\n",
    "labeled_df[\"label\"] = labeled_df[\"label\"].apply(lambda x: 1 if x in [1.0, 2.0, 3.0] else 0)\n",
    "\n",
    "# merging cleaned corpus and title on corpus\n",
    "labeled_df[\"corpus\"] = \"Title: \" + labeled_df[\"title\"] + \"\\n\\n\" + labeled_df[\"corpus\"]\n",
    "\n",
    "#  Create class weights\n",
    "labels = labeled_df[\"label\"]\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(labels), y=labels)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "# Convert to HF Dataset\n",
    "hf_labeled_df = Dataset.from_pandas(labeled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7cea6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_truncate(batch, tokenizer):\n",
    "    encoded = tokenizer(batch[\"corpus\"], add_special_tokens=False)\n",
    "    input_ids = encoded[\"input_ids\"] # word tokens mapped to ids\n",
    "    attention_mask = encoded[\"attention_mask\"] # 0 if padded\n",
    "\n",
    "    truncated_input_ids = []\n",
    "    truncated_attention_mask = []\n",
    "\n",
    "    for ids, mask in zip(input_ids, attention_mask):\n",
    "        \n",
    "        # limit input length\n",
    "        if len(ids) > 510:\n",
    "            new_ids = ids[:128] + ids[-382:]\n",
    "            new_mask = mask[:128] + mask[-382:]\n",
    "        else:\n",
    "            new_ids = ids\n",
    "            new_mask = mask\n",
    "\n",
    "        # Add [CLS] and [SEP]\n",
    "        new_ids = [tokenizer.cls_token_id] + new_ids + [tokenizer.sep_token_id]\n",
    "        new_mask = [1] + new_mask + [1]\n",
    "\n",
    "        pad_len = 512 - len(new_ids)\n",
    "        new_ids += [tokenizer.pad_token_id] * pad_len\n",
    "        new_mask += [0] * pad_len\n",
    "\n",
    "        truncated_input_ids.append(new_ids)\n",
    "        truncated_attention_mask.append(new_mask)\n",
    "\n",
    "    return {\"input_ids\": truncated_input_ids, \"attention_mask\": truncated_attention_mask}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "523ec538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 712/712 [00:00<00:00, 1309.77 examples/s]\n",
      "Map: 100%|██████████| 712/712 [00:00<00:00, 6673.11 examples/s]\n",
      "Map: 100%|██████████| 204/204 [00:00<00:00, 1254.54 examples/s]\n",
      "Map: 100%|██████████| 204/204 [00:00<00:00, 5112.59 examples/s]\n",
      "Map: 100%|██████████| 102/102 [00:00<00:00, 1383.98 examples/s]\n",
      "Map: 100%|██████████| 102/102 [00:00<00:00, 3200.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Set seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "set_seed(seed)\n",
    "\n",
    "# Convert to pandas and extract AI-focused snippets\n",
    "df_labeled = hf_labeled_df.to_pandas()\n",
    "df_labeled = extract_multiple_ai_snippets_with_context(df_labeled, text_col='corpus', output_col='ai_window',context_window=2)\n",
    "\n",
    "# Stratified split: 70% train, 30% temp\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_labeled,\n",
    "    test_size=0.3,\n",
    "    stratify=labeled_df[\"label\"],\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "# Stratified split: 2/3 test, 1/3 eval from temp (→ 20% / 10%)\n",
    "eval_df, test_df  = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=1/3,\n",
    "    stratify=temp_df[\"label\"],\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"ai_window\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# Convert to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "eval_dataset = Dataset.from_pandas(eval_df.reset_index(drop=True))\n",
    "\n",
    "# fixing label for tensors\n",
    "def fix_labels(example):\n",
    "    return {\"labels\": int(example[\"label\"])}\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize)\n",
    "train_dataset = train_dataset.map(fix_labels)\n",
    "\n",
    "eval_dataset = eval_dataset.map(tokenize)\n",
    "eval_dataset = eval_dataset.map(fix_labels)\n",
    "\n",
    "test_dataset = test_dataset.map(tokenize)\n",
    "test_dataset = test_dataset.map(fix_labels)\n",
    "\n",
    "\n",
    "\n",
    "# prepare dictionary for trainer\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset,\n",
    "    \"eval\": eval_dataset\n",
    "})\n",
    "\n",
    "# create class weights\n",
    "labels = labeled_df[\"label\"]\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfc474ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Define model path and label mappings\n",
    "model_path = \"ProsusAI/finbert\"\n",
    "id2label = {0: \"No narrative\", 1: \"narrative\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Load and customize config\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1\n",
    ")\n",
    "\n",
    "# Load pretrained backbone\n",
    "backbone = BertModel.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_weights_tensor = class_weights_tensor.to(device)\n",
    "\n",
    "# Instantiate the model\n",
    "model = CustomFinBERT(backbone, class_weights_tensor, config).to(device)\n",
    "\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Format datasets\n",
    "columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_dataset.set_format(type=\"torch\", columns=columns)\n",
    "test_dataset.set_format(type=\"torch\", columns=columns)\n",
    "eval_dataset.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "581fd134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# narrativerparameters \n",
    "lr = 2e-5\n",
    "batch_size = 4\n",
    "num_epochs = 15\n",
    "\n",
    "# data collator for padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= project_root / \"models\" / \"FINBERT_Binary\",\n",
    "    learning_rate=lr,\n",
    "    logging_dir= project_root / \"models\" / \"FINBERT_Binary\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9757c3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 109,483,778 / 109,483,778 (100.00%)\n",
      "[✓] backbone.embeddings.word_embeddings.weight\n",
      "[✓] backbone.embeddings.position_embeddings.weight\n",
      "[✓] backbone.embeddings.token_type_embeddings.weight\n",
      "[✓] backbone.embeddings.LayerNorm.weight\n",
      "[✓] backbone.embeddings.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.0.attention.self.query.weight\n",
      "[✓] backbone.encoder.layer.0.attention.self.query.bias\n",
      "[✓] backbone.encoder.layer.0.attention.self.key.weight\n",
      "[✓] backbone.encoder.layer.0.attention.self.key.bias\n",
      "[✓] backbone.encoder.layer.0.attention.self.value.weight\n",
      "[✓] backbone.encoder.layer.0.attention.self.value.bias\n",
      "[✓] backbone.encoder.layer.0.attention.output.dense.weight\n",
      "[✓] backbone.encoder.layer.0.attention.output.dense.bias\n",
      "[✓] backbone.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.0.intermediate.dense.weight\n",
      "[✓] backbone.encoder.layer.0.intermediate.dense.bias\n",
      "[✓] backbone.encoder.layer.0.output.dense.weight\n",
      "[✓] backbone.encoder.layer.0.output.dense.bias\n",
      "[✓] backbone.encoder.layer.0.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.0.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.1.attention.self.query.weight\n",
      "[✓] backbone.encoder.layer.1.attention.self.query.bias\n",
      "[✓] backbone.encoder.layer.1.attention.self.key.weight\n",
      "[✓] backbone.encoder.layer.1.attention.self.key.bias\n",
      "[✓] backbone.encoder.layer.1.attention.self.value.weight\n",
      "[✓] backbone.encoder.layer.1.attention.self.value.bias\n",
      "[✓] backbone.encoder.layer.1.attention.output.dense.weight\n",
      "[✓] backbone.encoder.layer.1.attention.output.dense.bias\n",
      "[✓] backbone.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.1.intermediate.dense.weight\n",
      "[✓] backbone.encoder.layer.1.intermediate.dense.bias\n",
      "[✓] backbone.encoder.layer.1.output.dense.weight\n",
      "[✓] backbone.encoder.layer.1.output.dense.bias\n",
      "[✓] backbone.encoder.layer.1.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.1.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.2.attention.self.query.weight\n",
      "[✓] backbone.encoder.layer.2.attention.self.query.bias\n",
      "[✓] backbone.encoder.layer.2.attention.self.key.weight\n",
      "[✓] backbone.encoder.layer.2.attention.self.key.bias\n",
      "[✓] backbone.encoder.layer.2.attention.self.value.weight\n",
      "[✓] backbone.encoder.layer.2.attention.self.value.bias\n",
      "[✓] backbone.encoder.layer.2.attention.output.dense.weight\n",
      "[✓] backbone.encoder.layer.2.attention.output.dense.bias\n",
      "[✓] backbone.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.2.intermediate.dense.weight\n",
      "[✓] backbone.encoder.layer.2.intermediate.dense.bias\n",
      "[✓] backbone.encoder.layer.2.output.dense.weight\n",
      "[✓] backbone.encoder.layer.2.output.dense.bias\n",
      "[✓] backbone.encoder.layer.2.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.2.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.3.attention.self.query.weight\n",
      "[✓] backbone.encoder.layer.3.attention.self.query.bias\n",
      "[✓] backbone.encoder.layer.3.attention.self.key.weight\n",
      "[✓] backbone.encoder.layer.3.attention.self.key.bias\n",
      "[✓] backbone.encoder.layer.3.attention.self.value.weight\n",
      "[✓] backbone.encoder.layer.3.attention.self.value.bias\n",
      "[✓] backbone.encoder.layer.3.attention.output.dense.weight\n",
      "[✓] backbone.encoder.layer.3.attention.output.dense.bias\n",
      "[✓] backbone.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.3.intermediate.dense.weight\n",
      "[✓] backbone.encoder.layer.3.intermediate.dense.bias\n",
      "[✓] backbone.encoder.layer.3.output.dense.weight\n",
      "[✓] backbone.encoder.layer.3.output.dense.bias\n",
      "[✓] backbone.encoder.layer.3.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.3.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.4.attention.self.query.weight\n",
      "[✓] backbone.encoder.layer.4.attention.self.query.bias\n",
      "[✓] backbone.encoder.layer.4.attention.self.key.weight\n",
      "[✓] backbone.encoder.layer.4.attention.self.key.bias\n",
      "[✓] backbone.encoder.layer.4.attention.self.value.weight\n",
      "[✓] backbone.encoder.layer.4.attention.self.value.bias\n",
      "[✓] backbone.encoder.layer.4.attention.output.dense.weight\n",
      "[✓] backbone.encoder.layer.4.attention.output.dense.bias\n",
      "[✓] backbone.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.4.intermediate.dense.weight\n",
      "[✓] backbone.encoder.layer.4.intermediate.dense.bias\n",
      "[✓] backbone.encoder.layer.4.output.dense.weight\n",
      "[✓] backbone.encoder.layer.4.output.dense.bias\n",
      "[✓] backbone.encoder.layer.4.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.4.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.5.attention.self.query.weight\n",
      "[✓] backbone.encoder.layer.5.attention.self.query.bias\n",
      "[✓] backbone.encoder.layer.5.attention.self.key.weight\n",
      "[✓] backbone.encoder.layer.5.attention.self.key.bias\n",
      "[✓] backbone.encoder.layer.5.attention.self.value.weight\n",
      "[✓] backbone.encoder.layer.5.attention.self.value.bias\n",
      "[✓] backbone.encoder.layer.5.attention.output.dense.weight\n",
      "[✓] backbone.encoder.layer.5.attention.output.dense.bias\n",
      "[✓] backbone.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.5.intermediate.dense.weight\n",
      "[✓] backbone.encoder.layer.5.intermediate.dense.bias\n",
      "[✓] backbone.encoder.layer.5.output.dense.weight\n",
      "[✓] backbone.encoder.layer.5.output.dense.bias\n",
      "[✓] backbone.encoder.layer.5.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.5.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.6.attention.self.query.weight\n",
      "[✓] backbone.encoder.layer.6.attention.self.query.bias\n",
      "[✓] backbone.encoder.layer.6.attention.self.key.weight\n",
      "[✓] backbone.encoder.layer.6.attention.self.key.bias\n",
      "[✓] backbone.encoder.layer.6.attention.self.value.weight\n",
      "[✓] backbone.encoder.layer.6.attention.self.value.bias\n",
      "[✓] backbone.encoder.layer.6.attention.output.dense.weight\n",
      "[✓] backbone.encoder.layer.6.attention.output.dense.bias\n",
      "[✓] backbone.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.6.intermediate.dense.weight\n",
      "[✓] backbone.encoder.layer.6.intermediate.dense.bias\n",
      "[✓] backbone.encoder.layer.6.output.dense.weight\n",
      "[✓] backbone.encoder.layer.6.output.dense.bias\n",
      "[✓] backbone.encoder.layer.6.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.6.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.7.attention.self.query.weight\n",
      "[✓] backbone.encoder.layer.7.attention.self.query.bias\n",
      "[✓] backbone.encoder.layer.7.attention.self.key.weight\n",
      "[✓] backbone.encoder.layer.7.attention.self.key.bias\n",
      "[✓] backbone.encoder.layer.7.attention.self.value.weight\n",
      "[✓] backbone.encoder.layer.7.attention.self.value.bias\n",
      "[✓] backbone.encoder.layer.7.attention.output.dense.weight\n",
      "[✓] backbone.encoder.layer.7.attention.output.dense.bias\n",
      "[✓] backbone.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.7.intermediate.dense.weight\n",
      "[✓] backbone.encoder.layer.7.intermediate.dense.bias\n",
      "[✓] backbone.encoder.layer.7.output.dense.weight\n",
      "[✓] backbone.encoder.layer.7.output.dense.bias\n",
      "[✓] backbone.encoder.layer.7.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.7.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.8.attention.self.query.weight\n",
      "[✓] backbone.encoder.layer.8.attention.self.query.bias\n",
      "[✓] backbone.encoder.layer.8.attention.self.key.weight\n",
      "[✓] backbone.encoder.layer.8.attention.self.key.bias\n",
      "[✓] backbone.encoder.layer.8.attention.self.value.weight\n",
      "[✓] backbone.encoder.layer.8.attention.self.value.bias\n",
      "[✓] backbone.encoder.layer.8.attention.output.dense.weight\n",
      "[✓] backbone.encoder.layer.8.attention.output.dense.bias\n",
      "[✓] backbone.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.8.intermediate.dense.weight\n",
      "[✓] backbone.encoder.layer.8.intermediate.dense.bias\n",
      "[✓] backbone.encoder.layer.8.output.dense.weight\n",
      "[✓] backbone.encoder.layer.8.output.dense.bias\n",
      "[✓] backbone.encoder.layer.8.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.8.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.9.attention.self.query.weight\n",
      "[✓] backbone.encoder.layer.9.attention.self.query.bias\n",
      "[✓] backbone.encoder.layer.9.attention.self.key.weight\n",
      "[✓] backbone.encoder.layer.9.attention.self.key.bias\n",
      "[✓] backbone.encoder.layer.9.attention.self.value.weight\n",
      "[✓] backbone.encoder.layer.9.attention.self.value.bias\n",
      "[✓] backbone.encoder.layer.9.attention.output.dense.weight\n",
      "[✓] backbone.encoder.layer.9.attention.output.dense.bias\n",
      "[✓] backbone.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.9.intermediate.dense.weight\n",
      "[✓] backbone.encoder.layer.9.intermediate.dense.bias\n",
      "[✓] backbone.encoder.layer.9.output.dense.weight\n",
      "[✓] backbone.encoder.layer.9.output.dense.bias\n",
      "[✓] backbone.encoder.layer.9.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.9.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.10.attention.self.query.weight\n",
      "[✓] backbone.encoder.layer.10.attention.self.query.bias\n",
      "[✓] backbone.encoder.layer.10.attention.self.key.weight\n",
      "[✓] backbone.encoder.layer.10.attention.self.key.bias\n",
      "[✓] backbone.encoder.layer.10.attention.self.value.weight\n",
      "[✓] backbone.encoder.layer.10.attention.self.value.bias\n",
      "[✓] backbone.encoder.layer.10.attention.output.dense.weight\n",
      "[✓] backbone.encoder.layer.10.attention.output.dense.bias\n",
      "[✓] backbone.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.10.intermediate.dense.weight\n",
      "[✓] backbone.encoder.layer.10.intermediate.dense.bias\n",
      "[✓] backbone.encoder.layer.10.output.dense.weight\n",
      "[✓] backbone.encoder.layer.10.output.dense.bias\n",
      "[✓] backbone.encoder.layer.10.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.10.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.11.attention.self.query.weight\n",
      "[✓] backbone.encoder.layer.11.attention.self.query.bias\n",
      "[✓] backbone.encoder.layer.11.attention.self.key.weight\n",
      "[✓] backbone.encoder.layer.11.attention.self.key.bias\n",
      "[✓] backbone.encoder.layer.11.attention.self.value.weight\n",
      "[✓] backbone.encoder.layer.11.attention.self.value.bias\n",
      "[✓] backbone.encoder.layer.11.attention.output.dense.weight\n",
      "[✓] backbone.encoder.layer.11.attention.output.dense.bias\n",
      "[✓] backbone.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "[✓] backbone.encoder.layer.11.intermediate.dense.weight\n",
      "[✓] backbone.encoder.layer.11.intermediate.dense.bias\n",
      "[✓] backbone.encoder.layer.11.output.dense.weight\n",
      "[✓] backbone.encoder.layer.11.output.dense.bias\n",
      "[✓] backbone.encoder.layer.11.output.LayerNorm.weight\n",
      "[✓] backbone.encoder.layer.11.output.LayerNorm.bias\n",
      "[✓] backbone.pooler.dense.weight\n",
      "[✓] backbone.pooler.dense.bias\n",
      "[✓] classifier.weight\n",
      "[✓] classifier.bias\n"
     ]
    }
   ],
   "source": [
    "# Freeze all parameters in the FinBERT encoder\n",
    "for name, param in model.backbone.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the classification head and optional with ( name or \"encoder.layer.11\" in ) the last encoder layer (layer.11)\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" or \"encoder.layer.11\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "# Print a summary of how many parameters are trainable\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params:,} / {total_params:,} ({trainable_params / total_params:.2%})\")\n",
    "\n",
    "# Optionally, list all parameter names that are currently trainable\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"[✓] {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13b44d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
    "        \"precision_macro\": precision_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"recall_macro\": recall_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"f1_weighted\": f1_score(labels, preds, average=\"weighted\"),\n",
    "        \"precision_weighted\": precision_score(labels, preds, average=\"weighted\", zero_division=0),\n",
    "        \"recall_weighted\": recall_score(labels, preds, average=\"weighted\", zero_division=0),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1371a5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in training data: {0, 1}\n"
     ]
    }
   ],
   "source": [
    "# Sanity check on label values\n",
    "unique_labels = set(train_dataset[\"labels\"].tolist())\n",
    "print(\"Unique labels in training data:\", unique_labels)\n",
    "assert all(label in [0, 1] for label in unique_labels), \"Found unexpected label values!\"\n",
    "\n",
    "# Ensure GPU is used   \n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d477102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_12148\\1632590623.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='890' max='2670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 890/2670 09:31 < 19:06, 1.55 it/s, Epoch 5/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>Precision Weighted</th>\n",
       "      <th>Recall Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.343200</td>\n",
       "      <td>0.477074</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.899160</td>\n",
       "      <td>0.895261</td>\n",
       "      <td>0.906052</td>\n",
       "      <td>0.902620</td>\n",
       "      <td>0.906084</td>\n",
       "      <td>0.901961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.331900</td>\n",
       "      <td>0.524069</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.899160</td>\n",
       "      <td>0.895261</td>\n",
       "      <td>0.906052</td>\n",
       "      <td>0.902620</td>\n",
       "      <td>0.906084</td>\n",
       "      <td>0.901961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.264800</td>\n",
       "      <td>0.550139</td>\n",
       "      <td>0.897059</td>\n",
       "      <td>0.893937</td>\n",
       "      <td>0.890282</td>\n",
       "      <td>0.899880</td>\n",
       "      <td>0.897683</td>\n",
       "      <td>0.900447</td>\n",
       "      <td>0.897059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.194400</td>\n",
       "      <td>0.574569</td>\n",
       "      <td>0.897059</td>\n",
       "      <td>0.893937</td>\n",
       "      <td>0.890282</td>\n",
       "      <td>0.899880</td>\n",
       "      <td>0.897683</td>\n",
       "      <td>0.900447</td>\n",
       "      <td>0.897059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.155800</td>\n",
       "      <td>0.651273</td>\n",
       "      <td>0.892157</td>\n",
       "      <td>0.889076</td>\n",
       "      <td>0.885296</td>\n",
       "      <td>0.895815</td>\n",
       "      <td>0.892882</td>\n",
       "      <td>0.896379</td>\n",
       "      <td>0.892157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=890, training_loss=0.2580179171615772, metrics={'train_runtime': 572.6048, 'train_samples_per_second': 18.652, 'train_steps_per_second': 4.663, 'total_flos': 0.0, 'train_loss': 0.2580179171615772, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train them model\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset, # 70% training set\n",
    "    eval_dataset=eval_dataset, # 20% training set\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True),\n",
    "    compute_metrics=compute_metrics, # metrics as defined above\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbffc92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "No narrative       0.95      0.89      0.92       123\n",
      "   narrative       0.84      0.93      0.88        81\n",
      "\n",
      "    accuracy                           0.90       204\n",
      "   macro avg       0.90      0.91      0.90       204\n",
      "weighted avg       0.91      0.90      0.90       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define save path\n",
    "project_root = Path.cwd().parent\n",
    "save_dir = project_root / \"models\" / \"FinBERT_Binary\" \n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model, tokenizer and configd\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "config.save_pretrained(save_dir)  \n",
    "\n",
    "# Extract log history from training\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Convert to DataFrame\n",
    "log_df = pd.DataFrame(log_history)\n",
    "\n",
    "# Filter out entries without epoch information\n",
    "log_df = log_df[log_df['epoch'].notna()].copy()\n",
    "\n",
    "# Save full training logs\n",
    "log_df.to_csv(f\"{save_dir}/full_training_metrics.csv\", index=False)\n",
    "#print(log_df)\n",
    "\n",
    "# Save optimizer and scheduler state\n",
    "torch.save(trainer.optimizer.state_dict(), f\"{save_dir}/optimizer.pt\")\n",
    "torch.save(trainer.lr_scheduler.state_dict(), f\"{save_dir}/scheduler.pt\")\n",
    "torch.save(class_weights_tensor, save_dir / \"class_weights.pt\")\n",
    "\n",
    "# Save config details\n",
    "with open(f\"{save_dir}/config_summary.txt\", \"w\") as f:\n",
    "    f.write(f\"Model: {save_dir}\\n\")\n",
    "    f.write(f\"Learning Rate: {training_args.learning_rate}\\n\")\n",
    "    f.write(f\"Epochs: {training_args.num_train_epochs}\\n\")\n",
    "    f.write(f\"Batch Size: {training_args.per_device_train_batch_size}\\n\")\n",
    "    f.write(\"Tokenizer: ProsusAI/finbert\\n\")\n",
    "    f.write(\"Special preprocessing: large_ai_snippets_with_context=2\\n\")\n",
    "    f.write(\"Additional finetuning: encoder.layer.11 \\n\")\n",
    "    f.write(\"Label mapping: {0: 'No narrative', 1: 'narrative'}\\n\")\n",
    "\n",
    "# Run prediction on eval set (not test set)\n",
    "eval_output = trainer.predict(eval_dataset)\n",
    "\n",
    "# Extract predictions and true labels\n",
    "eval_preds = np.argmax(eval_output.predictions, axis=1)\n",
    "eval_labels = eval_output.label_ids\n",
    "\n",
    "# Create DataFrame with predictions\n",
    "eval_results_df = pd.DataFrame({\n",
    "    \"text\": eval_dataset[\"ai_window\"],\n",
    "    \"true_label\": eval_labels,\n",
    "    \"predicted_label\": eval_preds\n",
    "})\n",
    "\n",
    "# Map label IDs to names\n",
    "id2label =  {0: 'No narrative', 1: 'narrative'}\n",
    "eval_results_df[\"true_label_name\"] = eval_results_df[\"true_label\"].map(id2label)\n",
    "eval_results_df[\"predicted_label_name\"] = eval_results_df[\"predicted_label\"].map(id2label)\n",
    "\n",
    "# Save predictions\n",
    "eval_results_df.to_csv(f\"{save_dir}/eval_predictions.csv\", index=False)\n",
    "\n",
    "# Save evaluation classification report\n",
    "report = classification_report(eval_labels, eval_preds, target_names=list(id2label.values()))\n",
    "with open(f\"{save_dir}/eval_classification_report.txt\", \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daab5af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('c:\\\\Users\\\\PC\\\\Desktop\\\\Masterarbeit\\\\AI_narrative_index\\\\models\\\\FinBERT_Binary\\\\tokenizer_config.json',\n",
       " 'c:\\\\Users\\\\PC\\\\Desktop\\\\Masterarbeit\\\\AI_narrative_index\\\\models\\\\FinBERT_Binary\\\\special_tokens_map.json',\n",
       " 'c:\\\\Users\\\\PC\\\\Desktop\\\\Masterarbeit\\\\AI_narrative_index\\\\models\\\\FinBERT_Binary\\\\vocab.txt',\n",
       " 'c:\\\\Users\\\\PC\\\\Desktop\\\\Masterarbeit\\\\AI_narrative_index\\\\models\\\\FinBERT_Binary\\\\added_tokens.json',\n",
       " 'c:\\\\Users\\\\PC\\\\Desktop\\\\Masterarbeit\\\\AI_narrative_index\\\\models\\\\FinBERT_Binary\\\\tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run prediction\n",
    "output = trainer.predict(eval_dataset)\n",
    "\n",
    "# Extract predicted and true labels\n",
    "preds = np.argmax(output.predictions, axis=1)\n",
    "true = output.label_ids\n",
    "\n",
    "# Define label mappings\n",
    "id2label = {0: \"No narrative\", 1: \"narrative\"}\n",
    "\n",
    "# Create full DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    \"text\": eval_dataset[\"ai_window\"],\n",
    "    \"true_label\": true,\n",
    "    \"predicted_label\": preds,\n",
    "    \"true_label_name\": [id2label[i] for i in true],\n",
    "    \"predicted_label_name\": [id2label[i] for i in preds]\n",
    "})\n",
    "\n",
    "# Filter misclassified samples\n",
    "misclassified_df = results_df[results_df[\"true_label\"] != results_df[\"predicted_label\"]]\n",
    "\n",
    "# Save to CSV\n",
    "misclassified_df.to_csv(\"misclassified_eval_samples.csv\", index=False)\n",
    "\n",
    "# Adjust pandas display options for full text visibility\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "# Save model state dict manually\n",
    "torch.save(model.state_dict(), save_dir / \"pytorch_model.bin\")\n",
    "\n",
    "# Save config and tokenizer\n",
    "config.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5331871",
   "metadata": {},
   "source": [
    "Just for FINAL evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb044780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      loss  grad_norm  learning_rate  epoch  step  eval_loss  eval_accuracy  \\\n",
      "0   0.3432   0.189552       0.000019    1.0   178        NaN            NaN   \n",
      "1      NaN        NaN            NaN    1.0   178   0.477074       0.901961   \n",
      "2   0.3319   0.187772       0.000017    2.0   356        NaN            NaN   \n",
      "3      NaN        NaN            NaN    2.0   356   0.524069       0.901961   \n",
      "4   0.2648   0.092782       0.000016    3.0   534        NaN            NaN   \n",
      "5      NaN        NaN            NaN    3.0   534   0.550139       0.897059   \n",
      "6   0.1944   0.103910       0.000015    4.0   712        NaN            NaN   \n",
      "7      NaN        NaN            NaN    4.0   712   0.574569       0.897059   \n",
      "8   0.1558   0.034760       0.000013    5.0   890        NaN            NaN   \n",
      "9      NaN        NaN            NaN    5.0   890   0.651273       0.892157   \n",
      "10     NaN        NaN            NaN    5.0   890        NaN            NaN   \n",
      "\n",
      "    eval_f1_macro  eval_precision_macro  eval_recall_macro  ...  \\\n",
      "0             NaN                   NaN                NaN  ...   \n",
      "1        0.899160              0.895261           0.906052  ...   \n",
      "2             NaN                   NaN                NaN  ...   \n",
      "3        0.899160              0.895261           0.906052  ...   \n",
      "4             NaN                   NaN                NaN  ...   \n",
      "5        0.893937              0.890282           0.899880  ...   \n",
      "6             NaN                   NaN                NaN  ...   \n",
      "7        0.893937              0.890282           0.899880  ...   \n",
      "8             NaN                   NaN                NaN  ...   \n",
      "9        0.889076              0.885296           0.895815  ...   \n",
      "10            NaN                   NaN                NaN  ...   \n",
      "\n",
      "    eval_precision_weighted  eval_recall_weighted  eval_runtime  \\\n",
      "0                       NaN                   NaN           NaN   \n",
      "1                  0.906084              0.901961       10.1348   \n",
      "2                       NaN                   NaN           NaN   \n",
      "3                  0.906084              0.901961       10.1107   \n",
      "4                       NaN                   NaN           NaN   \n",
      "5                  0.900447              0.897059       10.0220   \n",
      "6                       NaN                   NaN           NaN   \n",
      "7                  0.900447              0.897059       10.0251   \n",
      "8                       NaN                   NaN           NaN   \n",
      "9                  0.896379              0.892157       10.0543   \n",
      "10                      NaN                   NaN           NaN   \n",
      "\n",
      "    eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
      "0                       NaN                    NaN            NaN   \n",
      "1                    20.129                  5.032            NaN   \n",
      "2                       NaN                    NaN            NaN   \n",
      "3                    20.177                  5.044            NaN   \n",
      "4                       NaN                    NaN            NaN   \n",
      "5                    20.355                  5.089            NaN   \n",
      "6                       NaN                    NaN            NaN   \n",
      "7                    20.349                  5.087            NaN   \n",
      "8                       NaN                    NaN            NaN   \n",
      "9                    20.290                  5.072            NaN   \n",
      "10                      NaN                    NaN       572.6048   \n",
      "\n",
      "    train_samples_per_second  train_steps_per_second  total_flos  train_loss  \n",
      "0                        NaN                     NaN         NaN         NaN  \n",
      "1                        NaN                     NaN         NaN         NaN  \n",
      "2                        NaN                     NaN         NaN         NaN  \n",
      "3                        NaN                     NaN         NaN         NaN  \n",
      "4                        NaN                     NaN         NaN         NaN  \n",
      "5                        NaN                     NaN         NaN         NaN  \n",
      "6                        NaN                     NaN         NaN         NaN  \n",
      "7                        NaN                     NaN         NaN         NaN  \n",
      "8                        NaN                     NaN         NaN         NaN  \n",
      "9                        NaN                     NaN         NaN         NaN  \n",
      "10                    18.652                   4.663         0.0    0.258018  \n",
      "\n",
      "[11 rows x 21 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract log history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Convert to DataFrame\n",
    "log_df = pd.DataFrame(log_history)\n",
    "\n",
    "# 3. Filter out irrelevant entries (like those without epoch info)\n",
    "log_df = log_df[log_df['epoch'].notna()].copy()\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "# Save full logs with all metrics\n",
    "log_df.to_csv(f\"{save_dir}/full_training_metrics.csv\", index=False)\n",
    "\n",
    "# Print or display metrics\n",
    "print(log_df)\n",
    "\n",
    "# Save optimizer and learning rate scheduler state\n",
    "torch.save(trainer.optimizer.state_dict(), f\"{save_dir}/optimizer.pt\")\n",
    "torch.save(trainer.lr_scheduler.state_dict(), f\"{save_dir}/scheduler.pt\")\n",
    "\n",
    "# write model details to txt\n",
    "with open(f\"{save_dir}/config_summary.txt\", \"w\") as f:\n",
    "    f.write(f\"Model: {save_dir}\\n\")\n",
    "    f.write(f\"Learning Rate: {training_args.learning_rate}\\n\")\n",
    "    f.write(f\"Epochs: {training_args.num_train_epochs}\\n\")\n",
    "    f.write(f\"Batch Size: {training_args.per_device_train_batch_size}\\n\")\n",
    "    f.write(\"Tokenizer: ProsusAI/finbert\\n\")\n",
    "    f.write(\"Special preprocessing: large_ai_snippets_with_context=2\\n\")\n",
    "    f.write(\"Additional finetuning:  \")\n",
    "    f.write(\"Label mapping: {0: 'No narrative', 1: 'narrative'}\\n\")\n",
    "\n",
    "# Run prediction on test set\n",
    "test_output = trainer.predict(test_dataset)\n",
    "\n",
    "# Extract predictions and true labels\n",
    "test_preds = np.argmax(test_output.predictions, axis=1)\n",
    "test_labels = test_output.label_ids\n",
    "\n",
    "# Save raw predictions and true labels to CSV\n",
    "test_results_df = pd.DataFrame({\n",
    "    \"text\": test_dataset[\"ai_window\"],\n",
    "    \"true_label\": test_labels,\n",
    "    \"predicted_label\": test_preds\n",
    "})\n",
    "\n",
    "# map to label names\n",
    "id2label = {0: \"No narrative\", 1: \"narrative\"}\n",
    "test_results_df[\"true_label_name\"] = test_results_df[\"true_label\"].map(id2label)\n",
    "test_results_df[\"predicted_label_name\"] = test_results_df[\"predicted_label\"].map(id2label)\n",
    "\n",
    "# Save to CSV\n",
    "test_results_df.to_csv(f\"{save_dir}/test_predictions.csv\", index=False)\n",
    "\n",
    "# Save classification report (precision, recall, f1)\n",
    "report = classification_report(test_labels, test_preds, target_names=list(id2label.values()))\n",
    "with open(f\"{save_dir}/test_classification_report.txt\", \"w\") as f:\n",
    "    f.write(report)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da5303d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "No narrative       0.98      0.89      0.93        61\n",
      "   narrative       0.85      0.98      0.91        41\n",
      "\n",
      "    accuracy                           0.92       102\n",
      "   macro avg       0.92      0.93      0.92       102\n",
      "weighted avg       0.93      0.92      0.92       102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
