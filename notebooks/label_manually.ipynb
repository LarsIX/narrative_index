{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2243976",
   "metadata": {},
   "source": [
    "Notebook to manually annotate WSJ articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99fbf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from IPython.display import display\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Determine the project root\n",
    "current_path = Path().resolve()\n",
    "project_root = current_path.parents[0]  \n",
    "\n",
    "# Define paths to annotation and cleaning modules\n",
    "annotation_path = project_root / \"src\" / \"annotation\"\n",
    "flagging_path = project_root / \"src\" / \"preprocessing\"\n",
    "cleaning_path = project_root / \"src\" / \"cleaning\"\n",
    "\n",
    "# Add to sys.path if not already present\n",
    "for path in [annotation_path, cleaning_path, flagging_path]:\n",
    "    path_str = str(path)\n",
    "    if path_str not in sys.path:\n",
    "        sys.path.append(path_str)\n",
    "\n",
    "#  Imports modules\n",
    "from label_articles import annotate_articles_with_hype as af, find_training_examples\n",
    "from simple_ai_filter import flag_ai_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load samples\n",
    "batch_1 = pd.read_csv(project_root / \"data\" / \"interim\" / \"articles_WSJ_batch_one.csv\")\n",
    "batch_2 = pd.read_csv(project_root / \"data\" / \"interim\" /  \"articles_WSJ_batch_two.csv\")\n",
    "batch_3 = pd.read_csv(project_root / \"data\" / \"interim\" / \"articles_WSJ_batch_three.csv\")\n",
    "batch_4 = pd.read_csv(project_root / \"data\" / \"interim\" / \"articles_WSJ_batch_four.csv\")\n",
    "batch_list = [batch_1,batch_2,batch_3,batch_4]\n",
    "\n",
    "# verify data\n",
    "i = 1\n",
    "for batch in batch_list:\n",
    "    print(f\"batch number {i} has {len(batch)} articles with {batch.corpus.isna().sum()} empty corpora\")\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be07c3",
   "metadata": {},
   "source": [
    "Annotate articles using 0-1 scale for valence based index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c65f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate first batch\n",
    "first_Batch_articles_WSJ_author = af(df=batch_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb03729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the annotated sample to a csv file \n",
    "df_path = project_root / \"data\" / \"interim\" / \"annotaeted_batches_valence\" /  \"articles_WSJ_batch_one_author.csv\"\n",
    "first_Batch_articles_WSJ_author.to_csv(df_path, index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ffe741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate second batch\n",
    "df_second_batch_annotated_author = af(df=batch_2)\n",
    "\n",
    "# save the second annotated sample to a csv file, ensure being in \\notebooks\n",
    "output_path = project_root / \"data\" / \"interim\" / \"annotaeted_batches_valence\" / \"articles_WSJ_batch_two_author.csv\"\n",
    "df_second_batch_annotated_author.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ac759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate second batch\n",
    "df_second_batch_annotated_author = af(df=batch_2)\n",
    "\n",
    "# save the second annotated sample to a csv file, ensure being in \\notebooks\n",
    "output_path = project_root / \"data\" / \"interim\" / \"annotaeted_batches_valence\" /  \"articles_WSJ_batch_two_author.csv\"\n",
    "df_second_batch_annotated_author.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef28314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate second batch\n",
    "df_second_batch_annotated_author = af(df=batch_2)\n",
    "\n",
    "# save the second annotated sample to a csv file, ensure being in \\notebooks\n",
    "output_path = project_root / \"data\" / \"interim\" / \"annotaeted_batches_valence\" /  \"articles_WSJ_batch_two_author.csv\"\n",
    "df_second_batch_annotated_author.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42049940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate third batch\n",
    "df_second_batch_annotated_author = af(df=batch_3)\n",
    "\n",
    "# save the third annotated sample to a csv file, ensure being in \\notebooks\n",
    "output_path = project_root / \"data\" / \"interim\" / \"annotaeted_batches_valence\" / \"articles_WSJ_batch_three_author.csv\"\n",
    "df_second_batch_annotated_author.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 25% at random from batch 4, reproducible with seed\n",
    "df_fourth_batch_sample = batch_4.sample(frac=0.25, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# verify the sample size\n",
    "print(f\"Number of articles in the sample: {len(df_fourth_batch_sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92fb9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate the articles using the annotator function\n",
    "df_fourth_batch_sample_annotated_author = af(df=df_fourth_batch_sample);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv file\n",
    "df_fourth_batch_sample_annotated_author.to_csv(project_root / \"data\" / \"annotaeted_batches_valence\" /  \"articles_WSJ_batch_four_subsample_author.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226281f8",
   "metadata": {},
   "source": [
    "Annotate articles for few-shot prompting & GPT-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b77a404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['article_id', 'image_src', 'scanned_time', 'title', 'sub_title',\n",
      "       'corpus', 'index_id', 'id', 'date', 'link', 'section',\n",
      "       'cleaned_corpus'],\n",
      "      dtype='object')\n",
      "22904\n"
     ]
    }
   ],
   "source": [
    "# Define query\n",
    "query = \"SELECT * FROM article\"\n",
    "\n",
    "# Paths\n",
    "db_path_23 = project_root / \"data\" / \"processed\" / \"articles\" / \"articlesWSJ_clean_2023.db\"\n",
    "db_path_24 = project_root / \"data\" / \"processed\" / \"articles\" / \"articlesWSJ_clean_2024.db\"\n",
    "db_path_25 = project_root / \"data\" / \"processed\" / \"articles\" / \"articlesWSJ_clean_2025.db\"\n",
    "\n",
    "# Load data\n",
    "with sqlite3.connect(db_path_23) as conn:\n",
    "    data_23 = pd.read_sql(query, conn)\n",
    "\n",
    "with sqlite3.connect(db_path_24) as conn:\n",
    "    data_24 = pd.read_sql(query, conn)\n",
    "\n",
    "with sqlite3.connect(db_path_25) as conn:\n",
    "    data_25 = pd.read_sql(query, conn)\n",
    "\n",
    "\n",
    "# concat\n",
    "data = pd.concat([data_23,data_24,data_25])\n",
    "\n",
    "# verify\n",
    "print(data.columns)\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adba39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting AI keyword context windows:   0%|          | 21/22904 [00:04<1:00:58,  6.25it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Extracting AI keyword context windows: 100%|██████████| 22904/22904 [1:10:00<00:00,  5.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "---\n",
       "### Article 1/22904 — ID: `36`  \n",
       "#### Title: Best Buy-Owned Phone Service Faces Angry Customers After 3G Network Shutdown\n",
       "---\n",
       "\n",
       "#### Text Snippet:\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# suffle with seed\n",
    "subset = data.sample(frac=1, random_state=48).reset_index(drop=True)\n",
    "\n",
    "# annotate first batch\n",
    "few_shot_data = find_training_examples(df=data,context_window=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
