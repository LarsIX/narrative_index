{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c10c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# detection of the project root, ensure being in / notebooks\n",
    "current_path = Path().resolve()\n",
    "project_root = current_path.parents[1] \n",
    "annotation_path = project_root / \"AI_narrative_index\" / \"annotation\"\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.append(str(annotation_path))\n",
    "\n",
    "# Now import the required functions from the module\n",
    "from comparing_annotations import resolve_label_disagreements_AI, resolve_hype_disagreements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4864d0fa",
   "metadata": {},
   "source": [
    "Review first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936f5d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv files for the first batch\n",
    "first_batch_author = pd.read_csv(\"articles_WSJ_batch_one_author.csv\")\n",
    "first_batch_annotator = pd.read_csv(\"articles_WSJ_batch_one_annotator.csv\", encoding='cp1252') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate distribution of hype level and label_ai_related in annotator's annotation\n",
    "print(f\"distribution of hype level: {first_batch_annotator['hype_level'].value_counts()}\")\n",
    "print(f\"distribution of label_ai_related: {first_batch_annotator['label_ai_related'].value_counts()}\")\n",
    "print(f\"Number of articles with AI-related annotation: {first_batch_annotator['label_ai_related'].sum()}\")\n",
    "\n",
    "# investigate distribution of hype level and label_ai_related in aauthor's annotation\n",
    "print(f\"distribution of hype level author: {first_batch_author['hype_level'].value_counts()}\")\n",
    "print(f\"distribution of label_ai_related author: {first_batch_author['label_ai_related'].value_counts()}\")\n",
    "print(f\"Number of articles with AI-related annotation author: {first_batch_author['label_ai_related'].sum()}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef4df58",
   "metadata": {},
   "source": [
    "Inspect the dataframes, ensure compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e485e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the nan values to 0 in the author's dataframe\n",
    "first_batch_author['hype_level'] = first_batch_author['hype_level'].fillna(0) # change the nan values to 0 in the review dataframe\n",
    "\n",
    "# float hype_levels\n",
    "first_batch_annotator['hype_level'] = first_batch_annotator['hype_level'].astype(float) \n",
    "first_batch_author['hype_level'] = first_batch_author['hype_level'].astype(float) \n",
    "\n",
    "# check if datatype of the label column is float\n",
    "print(first_batch_annotator['hype_level'].dtype) \n",
    "print(first_batch_author['hype_level'].dtype)\n",
    "\n",
    "# check total values of hype levels in the review dataframe\n",
    "print(first_batch_annotator['hype_level'].sum()) # \n",
    "print(first_batch_author['hype_level'].sum()) #\n",
    "\n",
    "# print unique values of the hype level column in the review dataframe\n",
    "print(first_batch_annotator['hype_level'].unique()) \n",
    "print(first_batch_author['hype_level'].unique()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba879fd0",
   "metadata": {},
   "source": [
    "As a suggestion, the annotator labeled some articles as hype = 3, but the descision was made to set a max of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de0d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hype level to 2 if hype level is 3 in the review dataframe\n",
    "first_batch_annotator.loc[first_batch_annotator['hype_level'] == 3, 'hype_level'] = 2 \n",
    "\n",
    "# verify the change\n",
    "print(first_batch_annotator['hype_level'].unique()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b234d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align dataframes on article_id \n",
    "merged = first_batch_annotator.merge(first_batch_author, on=\"article_id\", suffixes=('_annotator', '_author'))\n",
    "\n",
    "# Count differences in classifications\n",
    "print(f'Total differences in hype classification: {((merged[\"hype_level_annotator\"] != merged[\"hype_level_author\"]).sum()) / len(merged[\"hype_level_annotator\"]) }')\n",
    "print(f'Total differences in ai_related classification: {((merged[\"label_ai_related_annotator\"] != merged[\"label_ai_related_author\"]).sum())/len(merged)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the resolve_label_disagreements function to resolve the AI label disagreements between the two dataframes\n",
    "df_final_first_batch = resolve_label_disagreements(first_batch_author, first_batch_annotator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the resolve_hype_disagreements function to resolve the hype disagreements between the two dataframes\n",
    "df_final_first_batch = resolve_hype_disagreements(first_batch_author, df_final_first_batch)\n",
    "\n",
    "# write the final dataframe to a csv file\n",
    "df_final_first_batch.to_csv(\"articles_WSJ_batch_one_final.csv\", index=False) # write the final dataframe to a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b801b",
   "metadata": {},
   "source": [
    "Review second batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e0d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the second batches from csv files\n",
    "second_batch_annotator = pd.read_csv(\"articles_WSJ_batch_two_annotator.csv\")\n",
    "second_batch_author = pd.read_csv(\"articles_WSJ_batch_two_author.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bb9c98",
   "metadata": {},
   "source": [
    "Inspect the dataframes, ensure compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e870373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the columns of the annotated second batch\n",
    "print(f\"Columns in the annotated second batch: {second_batch_annotator.columns}\")\n",
    "\n",
    "# inspect the columns of the annotated second batch\n",
    "print(f\"Number of articles in the annotated second batch: {len(second_batch_annotator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5086d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# align column names with authors annotation\n",
    "second_batch_annotator = second_batch_annotator.rename(columns={\"AI_RELEVANT\": \"label_ai_related\", \"HYPE_LEVEL\": \"hype_level\"})\n",
    "\n",
    "# change the nan values to 0 in the author's dataframe\n",
    "second_batch_author['hype_level'] = second_batch_author['hype_level'].fillna(0) # change the nan values to 0 in the review dataframe\n",
    "\n",
    "# float hype_levels\n",
    "second_batch_annotator['hype_level'] = second_batch_annotator['hype_level'].astype(float) \n",
    "second_batch_author['hype_level'] = second_batch_author['hype_level'].astype(float)\n",
    "\n",
    "# check if dtype of the label column is float\n",
    "print(second_batch_annotator['hype_level'].dtype)\n",
    "print(second_batch_author['hype_level'].dtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d03c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the values of the hype level column in the annotators second batch\n",
    "print(f\"Values of the hype level column in the annotated second batch: {second_batch_annotator['hype_level'].unique()}\")\n",
    "\n",
    "# print the values of the label_ai_related column in the annotators second batch\n",
    "print(f\"Values of the label_ai_related column in the annotated second batch: {second_batch_annotator['label_ai_related'].unique()}\")\n",
    "\n",
    "# print the values of the hype level column in the author's second batch\n",
    "print(f\"Values of the hype level column in the author's second batch: {second_batch_author['hype_level'].unique()}\")\n",
    "\n",
    "# print the values of the label_ai_related column in the author's second batch\n",
    "print(f\"Values of the label_ai_related column in the author's second batch: {second_batch_author['label_ai_related'].unique()}\")\n",
    "\n",
    "# number of articles with AI-related annotation in the second batch\n",
    "print(f\"Number of articles with AI-related annotation in the second batch: {second_batch_annotator['label_ai_related'].sum()}\")\n",
    "\n",
    "# number of articles with AI-related annotation in the second batch author\n",
    "print(f\"Number of articles with AI-related annotation in the second batch author: {second_batch_author['label_ai_related'].sum()}\")\n",
    "\n",
    "# total hype levels in the second batch\n",
    "print(f\"Total hype levels in the second batch: {second_batch_annotator['hype_level'].sum()}\")\n",
    "\n",
    "# total hype levels in the second batch author\n",
    "print(f\"Total hype levels in the second batch author: {second_batch_author['hype_level'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align dataframes on article_id \n",
    "merged = second_batch_annotator.merge(second_batch_author, on=\"article_id\", suffixes=('_annotator', '_author'))\n",
    "\n",
    "# Count differences in classifications\n",
    "print(f'Total differences in hype classification: {((merged[\"hype_level_annotator\"] != merged[\"hype_level_author\"]).sum()) / len(merged[\"hype_level_annotator\"]) }')\n",
    "print(f'Total differences in ai_related classification: {((merged[\"label_ai_related_annotator\"] != merged[\"label_ai_related_author\"]).sum())/len(merged)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba7c093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the label disagreements between the two dataframes using the resolve_label_disagreements function\n",
    "df_ai_level_merge  = resolve_label_disagreements_AI(second_batch_author, second_batch_annotator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c58b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check df_ai_level_merge\n",
    "print(f\"Number of changes in the merged dataframe: {df_ai_level_merge['modified'].sum()}\")\n",
    "print(f\"Number of articles with ai-related annotation: {df_ai_level_merge['label_ai_related'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2615e6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the nan values to 0 in the author's dataframe\n",
    "second_batch_author['hype_level'] = second_batch_author['hype_level'].fillna(0) # change the nan values to 0 in the review dataframe\n",
    "\n",
    "# float hype_levels\n",
    "df_ai_level_merge['hype_level'] = df_ai_level_merge['hype_level'].astype(float) # convert the hype level column to int\n",
    "second_batch_author['hype_level'] = second_batch_author['hype_level'].astype(float) # convert the hype level column to int\n",
    "\n",
    "# check if type of the label column is float\n",
    "print(df_ai_level_merge['hype_level'].dtype) # check the type of the label column\t\n",
    "print(second_batch_author['hype_level'].dtype) # check the type of the label column\n",
    "\n",
    "# check total values of hype levels in the review dataframe\n",
    "print(df_ai_level_merge['hype_level'].sum()) # \n",
    "print(second_batch_author['hype_level'].sum()) #\n",
    "\n",
    "# print unique values of the hype level column in the review dataframe\n",
    "print(df_ai_level_merge['hype_level'].unique()) # check the unique values of the hype level column in the review dataframe\n",
    "print(second_batch_author['hype_level'].unique()) # check the unique values of the hype level column in the author dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dbbaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the resolve_hype_disagreements function to resolve the hype disagreements between the two dataframes\n",
    "df_final_second_batch = resolve_hype_disagreements(second_batch_author, df_ai_level_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0668beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the final dataframe to a csv file\n",
    "df_final_second_batch.to_csv(r\"second_batch_WSJ_final.csv\", index=False) # write the final dataframe to a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1620191",
   "metadata": {},
   "source": [
    "Review third batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3928d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the third batches from csv files\n",
    "third_batch_annotator = pd.read_csv(r\"articles_WSJ_batch_three_annotator.csv\")\n",
    "third_batch_author = pd.read_csv(r\"articles_WSJ_batch_three_author.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the columns of the annotated third batch\n",
    "print(f\"Columns in the annotated third batch: {third_batch_annotator.columns}\")\n",
    "\n",
    "# inspect the columns of the annotated third batch\n",
    "print(f\"Number of articles in the annotated third batch: {len(third_batch_annotator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0884caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# align column names with authors annotation\n",
    "third_batch_annotator = third_batch_annotator.rename(columns={\"AI_Relevant\": \"label_ai_related\", \"Hype_Level\": \"hype_level\"})\n",
    "\n",
    "# change the nan values to 0 in the author's dataframe\n",
    "third_batch_author['hype_level'] = third_batch_author['hype_level'].fillna(0) # change the nan values to 0 in the review dataframe\n",
    "\n",
    "# float hype_levels\n",
    "third_batch_annotator['hype_level'] = third_batch_annotator['hype_level'].astype(float) # convert the hype level column to int\n",
    "third_batch_author['hype_level'] = third_batch_author['hype_level'].astype(float) # convert the hype level column to int\n",
    "\n",
    "# check if type of the label column is float\n",
    "print(third_batch_annotator['hype_level'].dtype) # check the type of the label column\t\n",
    "print(third_batch_author['hype_level'].dtype) # check the type of the label column\n",
    "\n",
    "# compare the hype levels in the two dataframes\n",
    "print(f'The total hype levels in the annotator\\'s dataframe: {third_batch_annotator[\"hype_level\"].sum()}')\n",
    "print(f'The total hype levels in the author\\'s dataframe: {third_batch_author[\"hype_level\"].sum()}')\n",
    "\n",
    "# compare the ai_reated levels in the two dataframes\n",
    "print(f'The total ai_reated levels in the annotator\\'s dataframe: {third_batch_annotator[\"label_ai_related\"].sum()}')\n",
    "print(f'The total ai_reated levels in the author\\'s dataframe: {third_batch_author[\"label_ai_related\"].sum()}')\n",
    "\n",
    "# Align dataframes on article_id \n",
    "merged = third_batch_annotator.merge(third_batch_author, on=\"article_id\", suffixes=('_annotator', '_author'))\n",
    "\n",
    "# Count differences in classifications\n",
    "print(f'Total differences in hype classification: {(merged[\"hype_level_annotator\"] != merged[\"hype_level_author\"]).sum()}')\n",
    "print(f'Total differences in ai_related classification: {(merged[\"label_ai_related_annotator\"] != merged[\"label_ai_related_author\"]).sum()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9868f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align dataframes on article_id \n",
    "merged = third_batch_annotator.merge(third_batch_author, on=\"article_id\", suffixes=('_annotator', '_author'))\n",
    "\n",
    "# Count differences in classifications\n",
    "print(f'Total differences in hype classification: {((merged[\"hype_level_annotator\"] != merged[\"hype_level_author\"]).sum()) / len(merged[\"hype_level_annotator\"]) }')\n",
    "print(f'Total differences in ai_related classification: {((merged[\"label_ai_related_annotator\"] != merged[\"label_ai_related_author\"]).sum())/len(merged)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82052e7d",
   "metadata": {},
   "source": [
    "Resolve disagreements between the author and the annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da527651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the label disagreements between the two dataframes using the resolve_label_disagreements function\n",
    "third_df_ai_level_merge = resolve_label_disagreements_AI(third_batch_author, third_batch_annotator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c2143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the resolve_hype_disagreements function to resolve the hype disagreements between the two dataframes\n",
    "df_final_third_batch = resolve_hype_disagreements(third_batch_author, third_df_ai_level_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca58bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the annotation process\n",
    "print(f\"Number of articles in the third sample: {len(df_final_third_batch)}\")\n",
    "print(f\"columns in the third sample: {df_final_third_batch.columns}\")\n",
    "print(f\"Number of articles with ai-related annotation: {df_final_third_batch['label_ai_related'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfde923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the final dataframe to a csv file\n",
    "df_final_third_batch.to_csv(\"articles_WSJ_batch_three_final.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e8a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the csv\n",
    "df_final_third_batch = pd.read_csv(\"articles_WSJ_batch_three_final.csv\")\n",
    "print(f\"Number of articles in the third sample: {len(df_final_third_batch)}\")\n",
    "print(f\"columns in the third sample: {df_final_third_batch.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ce4d6d",
   "metadata": {},
   "source": [
    "Review fraction of batch four annotated by author and annotator (sampled at random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e1c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv file of the fourth batch\n",
    "fourth_batch_author = pd.read_csv(\"articles_WSJ_batch_four_subsample_author.csv\")\n",
    "fourth_batch_annotator = pd.read_csv(\"articles_WSJ_batch_four_annotator.csv\", encoding='cp1252') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3357f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify data frames\n",
    "print(f\"The fourth batch annotated by the author contains {len(fourth_batch_author)} rows\")\n",
    "print(f\"The fourth batch annotated by the annotator contains {len(fourth_batch_annotator)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46147a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract rows from annotators df also in authors subset\n",
    "subset_annotator = fourth_batch_annotator[fourth_batch_annotator[\"article_id\"].isin(fourth_batch_author[\"article_id\"])]\n",
    "\n",
    "# verify subset\n",
    "print(len(subset_annotator) == 175)\n",
    "print(subset_annotator.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5713cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower column names\n",
    "subset_annotator.rename(columns=str.lower, inplace=True)\n",
    "\n",
    "# change ai_relevant name\n",
    "subset_annotator.rename(columns={\"ai_relevant\": \"label_ai_related\"}, inplace=True)\n",
    "\n",
    "# verify manipulations\n",
    "print(subset_annotator.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e3181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect values\n",
    "print(subset_annotator[[\"label_ai_related\",\"hype_level\"]])\n",
    "print(subset_annotator[[\"label_ai_related\",\"hype_level\"]].dtypes)\n",
    "print(fourth_batch_author[[\"label_ai_related\",\"hype_level\"]])\n",
    "print(fourth_batch_author[[\"label_ai_related\",\"hype_level\"]].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226d1201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set missing values to 0 in authors df\n",
    "fourth_batch_author[\"hype_level\"].fillna(0, inplace=True)\n",
    "\n",
    "# verify the change\n",
    "print(fourth_batch_author[\"hype_level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd6aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align dataframes on article_id \n",
    "merged = subset_annotator.merge(fourth_batch_author, on=\"article_id\", suffixes=('_annotator', '_author'))\n",
    "\n",
    "# Count differences in classifications\n",
    "print(f'Total differences in hype classification: {((merged[\"hype_level_annotator\"] != merged[\"hype_level_author\"]).sum()) / len(merged[\"hype_level_annotator\"]) }')\n",
    "print(f'Total differences in ai_related classification: {((merged[\"label_ai_related_annotator\"] != merged[\"label_ai_related_author\"]).sum())/len(merged)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77d7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the label disagreements between the two dataframes using the resolve_label_disagreements function\n",
    "fourth_sub_df_ai_level_merge = resolve_label_disagreements_AI(fourth_batch_author, subset_annotator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb791568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the resolve_hype_disagreements function to resolve the hype disagreements between the two dataframes\n",
    "df_final_fourth_batch_sub = resolve_hype_disagreements(fourth_batch_author, fourth_sub_df_ai_level_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9533ad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sampled & resolved articles as csv\n",
    "df_final_fourth_batch_sub.to_csv(\"articles_WSJ_batch_four_sample_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f740bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify csv\n",
    "df_final_fourth_batch_sub = pd.read_csv(\"articles_WSJ_batch_four_sample_final.csv\")\n",
    "print(df_final_fourth_batch_sub.head())\n",
    "print(f\"There are {len(df_final_fourth_batch_sub)} articles in the annotated subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3bc140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fourth_batch_annotator\n",
    "df_final_fourth_batch = pd.read_csv(\"articles_WSJ_batch_four_annotator.csv\",encoding='cp1252')\n",
    "print(fourth_batch_annotator.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e71b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index to article_id for both DataFrames\n",
    "df_final_fourth_batch.set_index(\"article_id\", inplace=True)\n",
    "df_final_fourth_batch_sub.set_index(\"article_id\", inplace=True)\n",
    "\n",
    "# Update all columns in-place where IDs match\n",
    "df_final_fourth_batch.update(df_final_fourth_batch_sub)\n",
    "\n",
    "# Reset index back to column \n",
    "df_final_fourth_batch.reset_index(inplace=True)\n",
    "\n",
    "# lower column names\n",
    "df_final_fourth_batch.rename(columns=str.lower, inplace=True)\n",
    "\n",
    "# change ai_relevant name\n",
    "df_final_fourth_batch.rename(columns={\"ai_relevant\": \"label_ai_related\"}, inplace=True)\n",
    "\n",
    "# verify manipulations\n",
    "print(len(df_final_fourth_batch))\n",
    "print(df_final_fourth_batch.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4760d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write final batch to csv\n",
    "df_final_fourth_batch.to_csv(\"articles_WSJ_batch_four_final.csv\", index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (berts)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
