{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f1d750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import itertools\n",
    "import datetime as dt\n",
    "import sqlite3\n",
    "\n",
    "# connect to folder for custom functions\n",
    "root = Path.cwd().parent\n",
    "vis_path = root / \"src\" / \"visualizations\" \n",
    "mod_path = root / \"src\" / \"modelling\" \n",
    "sys.path.append(str(vis_path))\n",
    "sys.path.append(str(mod_path))\n",
    "\n",
    "from read_articles import read\n",
    "from plot_functions import plot_aini_series_subplots\n",
    "from compute_extrema import compute_aini_extrema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af9a91e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare paths\n",
    "var_path = root / \"data\" / \"processed\" / \"variables\"\n",
    "art_path = root / \"data\" / \"processed\" / \"articles\"\n",
    "table_path = root / \"reports\" / \"tables\"\n",
    "\n",
    "# load aini data\n",
    "aini_custom = pd.read_csv(var_path / \"binary_AINI_variables.csv\")\n",
    "aini_w0 = pd.read_csv(var_path / \"w0_AINI_variables.csv\")\n",
    "aini_w1 = pd.read_csv(var_path / \"w1_AINI_variables.csv\")\n",
    "aini_w2 =  pd.read_csv(var_path / \"w2_AINI_variables.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fed7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes for visualizaions\n",
    "normalized_AINI = pd.DataFrame()\n",
    "\n",
    "# ensure sorting\n",
    "aini_w0 = aini_w0.sort_values(\"date\")\n",
    "aini_w1 = aini_w1.sort_values(\"date\")\n",
    "aini_w2 = aini_w2.sort_values(\"date\")\n",
    "aini_custom = aini_custom.sort_values(\"date\")\n",
    "\n",
    "# compute extrema\n",
    "merged, tidy, pivot, extrema = compute_aini_extrema(aini_w0,aini_w1,aini_w2,aini_custom)\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbb5c70",
   "metadata": {},
   "source": [
    "Explore differences in min, max, mean and std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59a9384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_columns(df, exclude=[\"date\"]):\n",
    "    \"\"\"Return mean, std, min, max for each numeric column in df (except exclude).\"\"\"\n",
    "    results = []\n",
    "    for col in df.columns:\n",
    "        if col not in exclude:\n",
    "            series = df[col]\n",
    "            results.append({\n",
    "                \"variable\": col,\n",
    "                \"mean\": series.mean(),\n",
    "                \"std\": series.std(),\n",
    "                \"min\": series.min(),\n",
    "                \"max\": series.max()\n",
    "            })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# usage\n",
    "stats_individual = summarize_columns(merged)\n",
    "print(stats_individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cb23354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "extrema.to_csv(table_path / \"aini_extrema.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddf50dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to LaTeX with booktabs, tabular (single-page), wrapped in adjustbox\n",
    "latex_table = extrema.to_latex(\n",
    "    index=False,\n",
    "    escape=True,\n",
    "    column_format=\"l\" + \"c\" * (len(extrema.columns) - 1),\n",
    "    bold_rows=False\n",
    ")\n",
    "\n",
    "# Add booktabs spacing\n",
    "latex_table = latex_table.replace(\"\\\\toprule\", \"\\\\toprule\\n\\\\addlinespace\")\n",
    "latex_table = latex_table.replace(\"\\\\midrule\", \"\\\\midrule\\n\\\\addlinespace\")\n",
    "latex_table = latex_table.replace(\"\\\\bottomrule\", \"\\\\addlinespace\\n\\\\bottomrule\")\n",
    "\n",
    "# Wrap in table + adjustbox\n",
    "latex_wrapped = (\n",
    "    \"\\\\begin{table}[!htbp]\\n\"\n",
    "    \"\\\\centering\\n\"\n",
    "    \"\\\\begin{adjustbox}{width=\\\\textwidth}\\n\"\n",
    "    + latex_table +\n",
    "    \"\\\\end{adjustbox}\\n\"\n",
    "    \"\\\\caption{AINI extrema}\\n\"\n",
    "    \"\\\\label{tab:aini_extrema}\\n\"\n",
    "    \"\\\\end{table}\\n\"\n",
    ")\n",
    "\n",
    "# Save to file\n",
    "output_path = table_path / \"aini_extrema.tex\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(latex_wrapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06324e2",
   "metadata": {},
   "source": [
    "Calculate weekly extrema (by calendar week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7ce0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# derive calendar week (ISO year + week number)\n",
    "tidy[\"week\"] = tidy[\"date\"].dt.to_period(\"W\").apply(lambda r: r.start_time)\n",
    "\n",
    "# Count how often each min/max week occurs\n",
    "counts_by_week = (\n",
    "    tidy.groupby([\"type\", \"week\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "        .sort_values([\"type\", \"count\"], ascending=[True, False])\n",
    ")\n",
    "\n",
    "# subset n > 0\n",
    "extrema_weekly = counts_by_week[counts_by_week[\"count\"] > 0]\n",
    "\n",
    "# collect variables for each week\n",
    "week_dict = (\n",
    "    tidy.groupby([\"week\"])[\"variable\"]\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# attach variables to each week\n",
    "extrema_weekly[\"measure\"] = extrema_weekly[\"week\"].map(week_dict)\n",
    "\n",
    "# bring into convenient format\n",
    "extrema_weekly_clean = extrema_weekly.copy()\n",
    "\n",
    "# week start (Monday)\n",
    "week_start = extrema_weekly_clean[\"week\"]\n",
    "# week end (Sunday) = start + 6 days\n",
    "week_end = week_start + pd.Timedelta(days=6)\n",
    "\n",
    "# format as \"dd.mm.yyyy - dd.mm.yyyy\"\n",
    "extrema_weekly_clean[\"week\"] = (\n",
    "    week_start.dt.strftime(\"%d.%m.%Y\") + \" - \" + week_end.dt.strftime(\"%d.%m.%Y\")\n",
    ")\n",
    "\n",
    "extrema_weekly_clean[\"type\"] = extrema_weekly_clean[\"type\"].replace({\"min\": \"minimum\", \"max\": \"maximum\"})\n",
    "extrema_weekly_clean.rename(columns={\"count\": \"n measures\"}, inplace=True)\n",
    "\n",
    "# save\n",
    "extrema_weekly_clean.to_csv(table_path / \"aini_weekly_extrema.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to LaTeX with booktabs, tabular (single-page), wrapped in adjustbox\n",
    "latex_table = extrema_weekly_clean.to_latex(\n",
    "    index=False,\n",
    "    escape=True,\n",
    "    column_format=\"l\" + \"c\" * (len(extrema_weekly_clean.columns) - 1),\n",
    "    bold_rows=False\n",
    ")\n",
    "\n",
    "# Add booktabs spacing\n",
    "latex_table = latex_table.replace(\"\\\\toprule\", \"\\\\toprule\\n\\\\addlinespace\")\n",
    "latex_table = latex_table.replace(\"\\\\midrule\", \"\\\\midrule\\n\\\\addlinespace\")\n",
    "latex_table = latex_table.replace(\"\\\\bottomrule\", \"\\\\addlinespace\\n\\\\bottomrule\")\n",
    "\n",
    "# Wrap in table + adjustbox\n",
    "latex_wrapped = (\n",
    "    \"\\\\begin{table}[!htbp]\\n\"\n",
    "    \"\\\\centering\\n\"\n",
    "    \"\\\\begin{adjustbox}{width=\\\\textwidth}\\n\"\n",
    "    + latex_table +\n",
    "    \"\\\\end{adjustbox}\\n\"\n",
    "    \"\\\\caption{AINI extrema}\\n\"\n",
    "    \"\\\\label{tab:aini_extrema}\\n\"\n",
    "    \"\\\\end{table}\\n\"\n",
    ")\n",
    "\n",
    "# Save to file\n",
    "output_path = table_path / \"aini_extrema_weekly.tex\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(latex_wrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac42bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extrema.sort_values(\"n measures\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f659810",
   "metadata": {},
   "outputs": [],
   "source": [
    "extrema_weekly_clean.sort_values(\"n measures\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc239ea5",
   "metadata": {},
   "source": [
    "Note: Min1 & Min2 in week with most Minima; \n",
    "Max1 is in week with most maxima, Max2 in second week with most maxima. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf51d518",
   "metadata": {},
   "source": [
    "load data with AINI predictions & compare with maxima and minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33491ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load & merge w1 data custom finbert data\n",
    "c_df23 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2023_on_binary.csv\")\n",
    "c_df24 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2024_on_binary.csv\")\n",
    "c_df25 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2025_on_binary.csv\")\n",
    "c_df = pd.concat([c_df23,c_df24,c_df25]) \n",
    "\n",
    "# load & merge w0 data\n",
    "w0_df23 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2023_windsize_0.csv\")\n",
    "w0_df24 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2024_windsize_0.csv\")\n",
    "w0_df25 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2025_windsize_0.csv\")\n",
    "w0_df = pd.concat([w0_df23,w0_df24,w0_df25]) \n",
    "\n",
    "# load & merge w1 data\n",
    "w1_df23 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2023_windsize_1.csv\")\n",
    "w1_df24 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2024_windsize_1.csv\")\n",
    "w1_df25 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2025_windsize_1.csv\")\n",
    "w1_df = pd.concat([w1_df23,w1_df24,w1_df25]) \n",
    "\n",
    "# merge on normalized_aini_wo to identify relevant articles\n",
    "w2_df23 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2023_windsize_2.csv\")\n",
    "w2_df24 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2024_windsize_2.csv\")\n",
    "w2_df25 = pd.read_csv(var_path / \"FinBERT_AINI_prediction_2025_windsize_2.csv\")\n",
    "w2_df = pd.concat([w2_df23,w2_df24,w2_df25]) \n",
    "\n",
    "# create df list \n",
    "aini_dfs = [c_df,w0_df,w1_df,w2_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c605b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify integrity\n",
    "for df in aini_dfs:\n",
    "    print(\n",
    "        f\"First entry: {df.date.min()} \"\n",
    "        f\"\\n last entry: {df.date.max()}, \"\n",
    "        f\"\\n n entries = {len(df)}, \"\n",
    "        f\"\\n n non-unique ids: {df['article_id'].duplicated().sum()}\",\n",
    "        f\"Columns: {df.columns}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995dc208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset for relevant columns\n",
    "rel_col = [\"article_id\",\"sentiment_label\",\"sentiment_score\",\"hype_score\"]\n",
    "clean_df = []\n",
    "\n",
    "for df in aini_dfs:\n",
    "    df = df[rel_col].copy()\n",
    "    clean_df.append(df)\n",
    "    \n",
    "c_df_sub,w0_df_sub,w1_df_sub,w2_df_sub = clean_df\n",
    "c_df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of DB paths\n",
    "db_files = [\n",
    "    art_path / \"articlesWSJ_clean_2023.db\",\n",
    "    art_path / \"articlesWSJ_clean_2024.db\",\n",
    "    art_path / \"articlesWSJ_clean_2025.db\",\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for db in db_files:\n",
    "    with sqlite3.connect(db) as conn:\n",
    "\n",
    "        # read data base into data frame\n",
    "        df = pd.read_sql(\"SELECT * FROM article\", conn)\n",
    "        dfs.append(df)\n",
    "\n",
    "# combine into one DataFrame\n",
    "all_articles = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# verify integrity\n",
    "print(\n",
    "    f\"First entry: {all_articles.date.min()} \"\n",
    "    f\"\\n last entry: {all_articles.date.max()}, \"\n",
    "    f\"\\n n entries = {len(all_articles)}, \"\n",
    "    f\"\\n n non-unique ids: {all_articles['article_id'].duplicated().sum()}\",\n",
    "    f\"Columns: {all_articles.columns}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52882832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# harmonize key dtype\n",
    "all_articles[\"article_id\"] = all_articles[\"article_id\"].astype(str)\n",
    "c_df_sub[\"article_id\"]  = c_df_sub[\"article_id\"].astype(str)\n",
    "w0_df_sub[\"article_id\"] = w0_df_sub[\"article_id\"].astype(str)\n",
    "w1_df_sub[\"article_id\"] = w1_df_sub[\"article_id\"].astype(str)\n",
    "w2_df_sub[\"article_id\"] = w2_df_sub[\"article_id\"].astype(str)\n",
    "\n",
    "# start from base\n",
    "complete_df = all_articles.copy()\n",
    "\n",
    "# merge step by step with suffixes\n",
    "complete_df = complete_df.merge(c_df_sub,  on=\"article_id\", how=\"left\", suffixes=(\"\", \"_c\"))\n",
    "complete_df = complete_df.merge(w0_df_sub, on=\"article_id\", how=\"left\", suffixes=(\"\", \"_w0\"))\n",
    "complete_df = complete_df.merge(w1_df_sub, on=\"article_id\", how=\"left\", suffixes=(\"\", \"_w1\"))\n",
    "complete_df = complete_df.merge(w2_df_sub, on=\"article_id\", how=\"left\", suffixes=(\"\", \"_w2\"))\n",
    "\n",
    "\n",
    "# verify integrity\n",
    "print(\n",
    "    f\"First entry: {complete_df.date.min()} \"\n",
    "    f\"\\n last entry: {complete_df.date.max()}, \"\n",
    "    f\"\\n n entries = {len(complete_df)}, \"\n",
    "    f\"\\n n non-unique ids: {complete_df['article_id'].duplicated().sum()}\",\n",
    "    f\"Columns: {w0_df.columns}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c486a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure format in date col \n",
    "complete_df[\"dates\"] = pd.to_datetime(complete_df[\"date\"])\n",
    "\n",
    "# define minima\n",
    "min_1 = pd.Timestamp(\"2025-02-06\") # n=8\n",
    "min_2 = pd.Timestamp(\"2025-02-04\") # n=2\n",
    "\n",
    "# define maxima\n",
    "max_1 = pd.Timestamp(\"2025-06-16\") # n=6\n",
    "max_2 = pd.Timestamp(\"2025-04-01\")  # n=4\n",
    "\n",
    "# subset original data extrema, minima\n",
    "articles_min1 = complete_df[complete_df[\"dates\"] == min_1] \n",
    "articles_min2 = complete_df[complete_df[\"dates\"] == min_2]\n",
    "\n",
    "# subset original data extrema, maxmima\n",
    "articles_max1 = complete_df[complete_df[\"dates\"] == max_1] \n",
    "articles_max2 = complete_df[complete_df[\"dates\"] == max_2] \n",
    "\n",
    "# investigate structure\n",
    "articles_max1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e7529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate 06.02.2025; min 1\n",
    "read(articles_min1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48daf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate 16.06.2025; max 1 according to ['normalized_AINI_w1', 'normalized_AINI_w2', 'EMA_02_w1', 'EMA_02_w2', 'EMA_08_w1', 'EMA_08_w2']\n",
    "articles_max1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd978fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate 01.04.2023; max 2 according to normalized_AINI_custom, simple_AINI_custom, EMA_02_custom, EMA_08_custom\n",
    "articles_max2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dad634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure datetime type\n",
    "complete_df[\"date\"] = pd.to_datetime(complete_df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# subsets per year\n",
    "df_2023 = complete_df[complete_df[\"date\"].dt.year == 2023]\n",
    "df_2024 = complete_df[complete_df[\"date\"].dt.year == 2024]\n",
    "df_2025 = complete_df[complete_df[\"date\"].dt.year == 2025]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ded2cc",
   "metadata": {},
   "source": [
    "Investigate AINI by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a3b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_aini_series_subplots(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb6b137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
